{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b2d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-3s2tj7580VL_qgpMS0oUq6IL2qngg3l\n",
      "From (redirected): https://drive.google.com/uc?id=1-3s2tj7580VL_qgpMS0oUq6IL2qngg3l&confirm=t&uuid=ae457754-3040-4f99-8892-f64be97c1bd9\n",
      "To: /Users/mike/OFFLINEX4_rad031DA.tmpPBWEBHOST437663.62.json\n",
      "100%|██████████████████████████████████████| 8.50G/8.50G [12:38<00:00, 11.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OFFLINEX4_rad031DA.tmpPBWEBHOST437663.62.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_id = \"1-3s2tj7580VL_qgpMS0oUq6IL2qngg3l\"\n",
    "destination = \"OFFLINEX4_rad031DA.tmpPBWEBHOST437663.62.json\"\n",
    "\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", destination, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bc230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ff403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = \"OFFLINEX4_rad031DA.tmpPBWEBHOST437663.62.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076af184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      "\"Author\": \"PatBase\",\n",
      "\n",
      "\"Format\": \"PatBase52\",\n",
      "\n",
      "\"Source\": \"4 and cc=us\",\n",
      "\n",
      "\"Created\": 1738751264,\n",
      "\n",
      "\"Records\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20210552492\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"KNEE ARTHROPLASTY VALIDATION AND GAP BALANCING INSTRUMENTATION\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"To address technical problems facing knee arthroplasty resection validation, the present subject matter provides a tracked knee arthroplasty instrument for objective measurement of resection depth. By performing a precise comparison between the location of the tracked knee arthroplasty instrument and a reference location, the knee arthroplasty instrument measures and validates each tibial and femoral resection. To address technical problems facing validation of joint laxity following knee arthroplasty, the tracked knee arthroplasty instrument is shaped to validate the flexion gap and extension gap. When the tracked knee arthroplasty instrument is inserted between the resected tibial plateau and femoral head, the instrument shape validates whether the desired flexion gap and extension gap have been achieved.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATION</p><p>[0001] This application is a continuation application of U.S. Provisional Patent Application No. 63/126,395, filed on Dec. 16, 2020, and entitled \\\"Knee Arthroplasty Validation and Gap Balancing Instrumentation,\\\" the application of which is hereby incorporated by reference in its entirety.</p></relapp><shortsum><p>FIELD</p><p>[0002] The present application relates to surgical knee replacement.</p><p>BACKGROUND</p><p>[0003] A knee replacement procedure (e.g., knee arthroplasty) is used to repair or replace damaged bone or damaged tissue in a patient knee joint. A knee arthroplasty includes repairing or replacing damaged or diseased articular surfaces of the tibia or femur. The arthroplasty procedure may include cutting (e.g., resecting) one or more articular surfaces of the tibia and femur and replacing a portion of each articular surface with a prosthesis (e.g., articular surface implant). A total knee arthroplasty (TKA) may be used to repair all articular surfaces of the tibia and femur, whereas a partial knee arthroplasty (PKA) may be used to repair a portion of the articular surfaces of the knee, such as the medial, lateral, or patellofemoral compartment. The TKA and PKA procedures require precise resections of the tibia and femur. The cut depth for each resection is specific to the patient and each prosthesis. A surgeon may validate a resection depth manually by inserting a trial prosthesis and exercising the knee through various motions. However, this resection validation is subjective and subject to errors. What is needed is an improved knee arthroplasty resection validation.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0004] FIG. <b>1</b> is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0005] FIGS. <b>2</b>A-<b>2</b>B are perspective views of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0006] FIG. <b>3</b> is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0007] FIG. <b>4</b> is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0008] FIG. <b>5</b> is a tibial resection diagram, in accordance with some embodiments.</p><p>[0009] FIG. <b>6</b> is a tibial resection slope graph, in accordance with some embodiments.</p><p>[0010] FIG. <b>7</b> is a diagram of a knee arthroplasty graphical user interface (GUI), in accordance with some embodiments.</p><p>[0011] FIGS. <b>8</b>A-<b>8</b>D are diagrams of an augment cut validation, in accordance with some embodiments.</p><p>[0012] FIGS. <b>9</b>A-<b>9</b>C are diagrams of an augment cut validation device, in accordance with some embodiments.</p><p>[0013] FIG. <b>10</b> is a diagram of an augment cut validation tracker device, in accordance with some embodiments.</p><p>[0014] FIG. <b>11</b> illustrates a flow chart showing a knee arthroplasty technique, in accordance with some embodiments.</p><p>[0015] FIG. <b>12</b> illustrates an example of a block diagram of a machine upon which any one or more of the techniques (e.g., methodologies) discussed herein may perform in accordance with some embodiments.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0016] The present disclosure describes technical solutions to various technical problems facing knee arthroplasty procedures. To address technical problems facing knee arthroplasty resection validation, the present subject matter provides a tracked knee arthroplasty instrument for objective measurement of resection depth. By performing a precise comparison between the location of the tracked knee arthroplasty instrument and a reference location, the knee arthroplasty instrument measures and validates each tibial and femoral resection. To address technical problems facing validation of joint laxity following knee arthroplasty, the tracked knee arthroplasty instrument is shaped to validate the flexion gap and extension gap. When the tracked knee arthroplasty instrument is inserted between the resected tibial plateau and femoral head, the instrument shape validates whether the desired flexion gap and extension gap have been achieved.</p><p>[0017] In an example PKA surgical procedure, a tibia is resected, the tracked knee arthroplasty instrument is used to validate the resection and check flexion gap and extension gap, the femur is resected, and the tracked knee arthroplasty instrument is again used to validate the resection and check gaps. The use of the tracked knee arthroplasty instrument to validate resections and check gaps ensures that bone gaps and soil tissue allow for sufficient space for an implant and sufficient space in the postoperative elongated leg. In addition to validating resections and checking gaps, the use of the tracked knee arthroplasty instrument provides information regarding limb alignment and tactile feel of the resected surfaces.</p><p>[0018] The tracked knee arthroplasty instrument may be used with a robotic surgical device. In an example, a robotic surgical device may perform a tibial or femoral resection, and the tracked knee arthroplasty instrument may be used by a surgeon or by the robotic surgical device to validate resections and check gaps. In an example, the robotic surgical device may position resection surgical tools to prepare for the resection, a surgeon may perform a tibial or femoral resection, and the tracked knee arthroplasty instrument may be used by a surgeon or by the robotic surgical device to validate resections and check gaps. The robotic surgical system and tracked knee arthroplasty instrument may use a combination of one or more coordinate systems or tracked positioning systems. In an example, the tracked knee arthroplasty instrument is tracked using an optical tracking system, the robotic surgical device uses a robotic device coordinate system, and a surgical plan management system translates the tracked knee arthroplasty instrument position and robotic surgical device position into a common coordinate system viewable by the surgeon.</p><p>[0019] In the drawings, which are not necessarily drawn to scale, like numerals may describe similar components in different views. Like numerals having different letter suffixes may represent different instances of similar components. The drawings illustrate generally, by way of example, but not by way of limitation, various embodiments discussed in the present document.</p><p>[0020] FIG. <b>1</b> is a perspective view of a tracked knee arthroplasty system <b>100</b>, in accordance with some embodiments. System <b>100</b> includes an arthroplasty validation instrument <b>110</b>, where instrument <b>110</b> includes one or more articular contact surfaces that may be placed in contact with a resected tibial surface. In an example, a horizontal resection validation surface on the bottom surface (not shown) of instrument <b>110</b> may be placed on the tibial plateau horizontal resection <b>150</b>. Similarly, a vertical resection validation surface on the distant surface (not shown) of instrument <b>110</b> may be placed on the vertical resection <b>155</b> (e.g., tibial sagittal resection).</p><p>[0021] The thickness (e.g., height) of instrument <b>110</b> separates the top surface from the horizontal resection validation surface on the bottom surface (not shown) of instrument <b>110</b>. This gap validation thickness may be used to validate the gap between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>. FIG. <b>1</b> shows the gap validation thickness being used to validate the extension gap while the knee is in flexion, though the gap validation thickness may also be used to validate the flexion gap while the knee is in extension.</p><p>[0022] Instrument <b>110</b> may be attached to a manual manipulation device <b>120</b>. The manipulation device <b>120</b> may include grooves, a grip, or other surface to improve the ability of a surgeon to manipulate the instrument <b>110</b>. The instrument <b>110</b> or the manipulation device <b>120</b> may include an orientation mechanism (e.g., detent, keying surface) to ensure the instrument <b>110</b> and manipulation device <b>120</b> are attached in a reliable and precise configuration. In an example, the instrument <b>110</b> includes a threaded aperture and the manipulation device <b>120</b> includes a threaded socket, and a threaded screw <b>130</b> is attached through the instrument <b>110</b> into the manipulation device <b>120</b>.</p><p>[0023] The manipulation device <b>120</b> may be attached to a location tracking device <b>140</b>, such as an optical tracker. The tracking device <b>140</b> may be used by an optical tracking system to determine the precise location of the instrument <b>110</b>. In an example, once the instrument <b>110</b> is positioned against the horizontal resection <b>150</b> and against the vertical resection <b>155</b>, the tracking device <b>140</b> may be used to validate the horizontal resection <b>150</b> and the vertical resection <b>155</b>. The validation of the horizontal resection <b>150</b> may include determining a resection cut depth, a varus or valgus angle, a resection slope, or other horizontal resection geometry. The validation of the vertical resection <b>155</b> may include determining a resection rotation, a resection medial-lateral offset, or other vertical resection geometry.</p><p>[0024] In another example, the position of the instrument <b>110</b> may be tracked to ensure the instrument <b>110</b> is inserted to a sufficient depth between the tibial plateau horizontal resection <b>150</b> and the native femoral head <b>160</b> or a distal femoral resection, where the gap validation thickness (e.g., height) of instrument <b>110</b> is used to validate the gap between the tibial plateau horizontal resection <b>150</b> and the native femoral head <b>160</b> or a distal femoral resection. The optical system may determine the position of the tracking device <b>140</b> relative to another tracked position, such as relative to an optical tracker fixedly attached to the patient tibia, relative to a registration pointer attached to a robotic arm, or relative to another tracked position.</p><p>[0025] FIGS. <b>2</b>A-<b>2</b>B are perspective views of a tracked knee arthroplasty system <b>200</b>, in accordance with some embodiments. System <b>200</b> includes an arthroplasty validation instrument <b>210</b> attached to a manual manipulation device <b>220</b>, such as using a threaded screw <b>130</b> threaded through instrument <b>210</b> into manipulation device <b>220</b>. Instrument <b>210</b> may include a proximate portion <b>225</b> that is proximate to the manipulation device <b>220</b>, and may include a distal portion <b>215</b> that is distal from the manipulation device <b>220</b>.</p><p>[0026] As shown in FIG. <b>2</b>A, the proximate portion <b>225</b> may be thicker than the distal portion <b>215</b>. The use of different thicknesses may be used to validate different gap sizes, such as validating a posterior gap on a posterior portion of a tibial plateau resection and a larger anterior gap on an anterior portion of the tibial plateau resection. Instrument <b>210</b> may include a transition region <b>235</b> between the proximate portion <b>225</b> and the distal portion <b>215</b>. The transition region <b>235</b> may facilitate insertion of the instrument <b>210</b> between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>, such as by providing a linear sigmoid, or other smooth transition between the proximate portion <b>225</b> and the distal portion <b>215</b>.</p><p>[0027] As shown in FIG. <b>2</b>B, the proximate portion <b>225</b> may be wider than the distal portion <b>215</b>. The wider proximate portion <b>225</b> may be used to provide a mechanical stop, such as by providing a stop against an anterior tibial surface when inserted between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>.</p><p>[0028] FIG. <b>3</b> is a perspective view of a tracked knee arthroplasty system <b>300</b>, in accordance with some embodiments, System <b>300</b> includes a proximate portion <b>325</b> of an arthroplasty validation instrument, which may be inserted into a patient incision <b>350</b>. The proximate portion <b>325</b> may be attached to a manual manipulation device <b>220</b>. The manipulation device <b>320</b> may include a pointed tip portion <b>340</b> that is received within a tip receptacle within proximate portion <b>325</b>. Once the pointed tip portion <b>340</b> is seated correctly within the tip receptacle, the manipulation device <b>320</b> may be secured to the proximate portion <b>325</b> using a threaded screw <b>330</b>.</p><p>[0029] FIG. <b>4</b> is a perspective view of a tracked knee arthroplasty system <b>400</b>, in accordance with some embodiments. System <b>400</b> includes an arthroplasty validation instrument <b>410</b>, where instrument <b>410</b> includes one or more articular contact surfaces that may be placed in contact with a resected tibial surface. A horizontal resection validation surface on the bottom surface (not shown) of instrument <b>410</b> may be placed on the tibial plateau horizontal resection <b>450</b>. Similarly, a vertical resection validation surface on the distant surface (not shown) of instrument <b>410</b> may be placed on the vertical resection <b>455</b> (e.g., tibial sagittal resection). Instrument <b>410</b> may be attached to a manual manipulation device <b>420</b>, such as using a threaded screw <b>430</b> threaded through instrument <b>410</b> into the manipulation device <b>420</b>, Manipulation device <b>420</b> may be connected to an optical tracker or other tracking device (not shown).</p><p>[0030] The thickness of instrument <b>410</b> separates the top surface from the horizontal resection validation surface on the bottom surface (not shown) of instrument <b>410</b>. This gap validation thickness may be used to validate the gap between the tibial plateau horizontal resection <b>450</b> and the resected femoral head <b>460</b>. FIG. <b>4</b> shows the gap validation thickness being used to validate the extension gap while the knee is in flexion, though the gap validation thickness may also be used to validate the flexion gap while the knee is in extension.</p><p>[0031] Instrument <b>410</b> includes an anterior stop <b>440</b>. When fully inserted, the anterior stop <b>440</b> rests against the tibial anterior cortex <b>470</b>. The anterior stop <b>440</b> may be used to minimize or prevent instrument <b>410</b> from migrating during drilling, pinning, impaction, or other surgical procedures. When used with a tracking device, the anterior stop <b>440</b> may be used to provide key cortex location information or other tracking information, which may be used to make more precise recuts in imageless cases. This tracking information may reduce or prevent the need for discrete (e.g., dedicated) digitization or registration pointer checks.</p><p>[0032] Instrument <b>410</b> may include one or more structural features to provide additional validation information. The length of instrument <b>410</b> may be used to locate the tibial posterior cortex while validating the tibial plateau resection plane. In an example, instrument <b>410</b> may include distal tibial hooks, distal tibial stops, or other mechanical features (not shown) extending beyond the end of instrument <b>410</b> to locate the posterior cortex. This determined location of the posterior cortex may assist in finding additional reference locations for anatomic landmarking, such as to define the tibial internal and external rotation coordinate system at the plane of the tibial resection. The combination of distal tibial hook and the anterior stop <b>440</b> may be used to provide information about the geometry of the tibia, which may be used to size the tibia. In an example, instrument <b>410</b> may include medial or lateral tibial side hooks or other mechanical features (not shown) extending to either side of instrument <b>410</b>. The side hooks may be used to map the size and geometry of the medial cortex or lateral cortex. This cortex information may be used for femoral sizing, such as selecting standard or narrow femoral head implants. In an example, instrument <b>410</b> may include a distal trochlea stylus (not shown), which might be used to locate or map the femoral trochlea (e.g., intercondylar fossa of femur). The trochlea stylus may provide anterior reference information, which may be used to improve femoral sizing or notching information within a resection. Information from the anterior stop <b>440</b> or one or more tibial hooks may be used to validate resections or update anatomic information. In an example, anatomic information may be gathered through preoperative digitization of the bone, and the preoperatively gathered information may be updated using intraoperative information gathered from the anterior stop <b>440</b> or one or more tibial hooks. This updated information may be used to refresh or improve surgical plans intraoperatively while reducing or minimizing additional intraoperative surgical procedure steps.</p><p>[0033] FIG. <b>5</b> is a tibial resection diagram <b>500</b>, in accordance with some embodiments. A surgeon may use an arthroplasty validation instrument to determine that the depth or slope of the primary cut <b>510</b> (e.g., initial tibial resection) is insufficient, and that a secondary cut <b>520</b> (e.g., secondary resection) may be needed. To change the slope of a tibial resection, the secondary cut <b>520</b> must begin at a lower point on the tibial anterior cortex to ensure a full resection. The starting points of the primary cut <b>510</b> and the secondary cut <b>520</b> may be separated by a cut bias <b>530</b>. To minimize the number of additional tibial resections, the cut bias <b>530</b> may be selected to be the smallest bias that is sufficiently large to perform the secondary cut <b>520</b>. This may be particularly useful when performing a secondary cut <b>520</b> where there is insufficient information available about the location of the tibial anterior cortex, such as in imageless arthroplasty procedures. The bias selection may be improved by determining information about the location of the anterior cortex, such as using the anterior stop <b>440</b> to provide cortex location information.</p><p>[0034] FIG. <b>6</b> is a tibial resection slope graph <b>600</b>, in accordance with some embodiments. Graph <b>600</b> depicts an example primary cut <b>610</b> and a secondary cut <b>620</b>. In a conventional TKA surgery, the rotation point for the posterior slope is set at the anterior aspect of the tibia, so, the surgeon does not need to worry about increased resection depth for an increased slope recut. For a PKA surgery, the posterior slope is set based on the middle of the tibial plateau, so a secondary cut to change the slope will always include an increase in the resection depth <b>630</b> (e.g., secondary cut bias) to ensure a full resection.</p><p>[0035] The slope and depth of the secondary cut <b>620</b> may be adjustable to provide a desired slope while remaining consistent with other surgical parameters. In an example, a PKA surgical plan may have an associated maximum allowed parallel recut <b>640</b>, which may correspond with a worst-case slope and depth change <b>650</b>. Table 1 shows various combinations of tibial resection depth and slope. In particular, Table 1 shows a minimum increase in depth required to provide a full resection, and shows the maximum increase in resection depth that will result in a resection within 3 mm distal to the primary cut on the anterior/posterior side (e.g., maximum allowed parallel recut).</p><p>[0036] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Tibial Resection Depth and Slope</TD></TR><TR><TD>Change in</TD><TD>Minimum</TD><TD>Minimum change in</TD><TD>Maximum increase in</TD></TR><TR><TD>Slope (with</TD><TD>change in</TD><TD>resection depth</TD><TD>resection depth (for</TD></TR><TR><TD>respect to</TD><TD>resection</TD><TD>required for</TD><TD>minimum resection</TD></TR><TR><TD>first cut)</TD><TD>depth</TD><TD>full resection</TD><TD>depth change)</TD></TR><TR></TR><TR><TD>0 degrees </TD><TD>0.0</TD><TD> 0 mm</TD><TD><sup></sup>3 mm</TD></TR><TR><TD>2 degrees </TD><TD>0.72</TD><TD> 1.1 mm</TD><TD>3.7 mm</TD></TR><TR><TD>4 degrees </TD><TD>1.43</TD><TD>2.17 mm</TD><TD>4.4 mm</TD></TR><TR><TD>5 degrees </TD><TD>1.79</TD><TD>2.71 mm</TD><TD>4.8 mm</TD></TR><TR><TD>6 degrees </TD><TD>2.15</TD><TD>3.26 mm</TD><TD>5.1 mm</TD></TR><TR><TD>8 degrees </TD><TD>2.88</TD><TD>4.36 mm</TD><TD>5.9 mm</TD></TR><TR><TD>10 degrees </TD><TD>3.61</TD><TD> 5.5 mm</TD><TD>6.6 mm</TD></TR><TR></TR></Table></p><p>[0037] FIG. <b>7</b> is a diagram of a knee arthroplasty graphical user interface (GUI) <b>700</b>, in accordance with some embodiments. GUI <b>700</b> may be used to display information about planned or measured arthroplasty resection depths or angles. GUI <b>700</b> may include an anterior view <b>710</b> of the femoral head <b>720</b> and the proximal tibia <b>730</b>. Similarly, GUI <b>700</b> may include a medial view <b>715</b> of the femoral head <b>725</b> and the proximal tibia <b>735</b>. The anterior view <b>710</b> may have an associated anterior view control <b>740</b>, and the medial view <b>715</b> may have an associated medial view control <b>745</b>, which may be used to rotate the view of the femur and tibia displayed within GUI <b>700</b>. The anterior view <b>710</b> may have an associated anterior tibial control <b>750</b>, and the medial view <b>715</b> may have an associated medial tibial control <b>755</b>, which may be used to change the flexion angle or modify tibial slope or resection. GUI <b>700</b> may also provide information about distal resection depth <b>760</b>, proximal resection depth <b>765</b>, proximal resection slope angles <b>770</b>, posterior slope angles <b>775</b>, hip-knee-ankle (HKA) axis angles <b>780</b>, plan laxity measurements <b>785</b>, and a flexion angle <b>790</b>.</p><p>[0038] The display of information, bone views, or other portions within GUI <b>700</b> may be modified to indicate whether one or more steps in the knee arthroplasty surgical procedure have been completed. For example, the proximal resection depth <b>765</b> may be presented in a first color to indicate a sufficient resection depth, and the proximal tibia <b>730</b> and proximal resection angle <b>770</b> may be presented in a second color to indicate additional surgical procedure steps are needed to provide the planned resection slope. In another example, the proximal resection depth <b>765</b> may be presented in a first color to indicate the depth is based on a depth validated by an arthroplasty validation instrument, and the proximal tibia <b>730</b> and proximal resection angle <b>770</b> may be presented in a second color to indicate the displayed resection slope angle is using outdated information.</p><p>[0039] FIGS. <b>8</b>A-<b>8</b>D are diagrams of an augment cut validation <b>800</b>, in accordance with some embodiments. FIG. <b>8</b>A shows a patient tibia with a partial implant <b>810</b>, such as may be used in a PKA surgical procedure. FIG. <b>8</b>B shows a horizontal revision surgery tibia cut <b>820</b> and a deeper augment cut <b>830</b>. A surgeon may use the revision surgery when a portion of the knee has bad bone quality, where the surgeon can remove the bad bone quality region with an augment implant to provide a stable surface for the femoral implant. FIG. <b>8</b>C shows the revision surgery with an augment implant <b>840</b> and a revision implant <b>850</b>. While FIG. <b>8</b>C shows a revision surgery with a correct augment implant cut depth, FIG. <b>8</b>D shows a revision surgery with an insufficient augment implant cut depth, resulting in a gap <b>860</b>. To determine whether the augment implant cut depth is sufficient, an augment cut validation device may be used, such as shown in FIG. <b>9</b>A.</p><p>[0040] FIGS. <b>9</b>A-<b>9</b>C are diagrams of an augment cut validation device <b>900</b>, in accordance with some embodiments. The augment cut validation device <b>900</b> may be used to determine whether a revision surgery augment resection and horizontal resection are cut to a correct depth. As shown in FIG. <b>9</b>A, augment cut validation device <b>900</b> may include a tracker mount <b>910</b> and a base <b>920</b>. One or more slide-in augment spacers <b>930</b>, <b>935</b> may be attached to base <b>920</b>. In an example, each augment spacer <b>930</b>, <b>935</b> may have a flange <b>940</b> that slides within base channel <b>925</b> and one or more detents <b>945</b> to secure the augment spacer <b>930</b>, <b>935</b> in a fixed position relative to the augment cut validation device <b>900</b>. As shown in FIG. <b>9</b>B, an augment cut validation device <b>900</b> may have an extended base for validating a surface on a larger bone. As shown in FIG. <b>9</b>C, variously sized augment spacers <b>950</b> may be used. In various examples, the augment spacers <b>950</b> may include incremental sizes, such as 5 mm, 10 mm, 15 mm, or other sizes. In an example, two different sized augment spacers <b>950</b> may be used to validate a first cut dept of a resected surface of a horizontal resection and a deeper cut depth of a resected surface of an augment resection.</p><p>[0041] FIG. <b>10</b> is a diagram of an augment cut validation tracker device <b>1000</b>, in accordance with some embodiments. The augment cut validation tracker device <b>1000</b> includes a tracker mount <b>1010</b> that attaches to a tracker attachment <b>1020</b>, which is fixedly attached to an optical tracker <b>1040</b>. The augment cut validation tracker device <b>1000</b> includes one or more augment spacers <b>1030</b> that may be used to validate a revision surgery augment resection and horizontal resection. In an example, surgeon may position the augment cut validation tracker device <b>1000</b> such that the augment spacers <b>1030</b> are in contact with an augment resection and horizontal resection of a patient tibia <b>1050</b>, and the optical tracker <b>1040</b> may be used to determine the depth of the augment resection and horizontal resection by comparing a measured location of the optical tracker <b>1040</b> against a known location of the tibia <b>1050</b>. Similarly, the augment cut validation tracker device <b>1000</b> may be used to compare the augment resection depth to the horizontal resection depth, such as by determining that a vertical axis of the optical tracker <b>1040</b> is offset from the vertical axis of the tibia <b>1050</b>.</p><p>[0042] FIG. <b>11</b> illustrates a flow chart showing a knee arthroplasty technique <b>1100</b>, in accordance with some embodiments. Technique <b>1100</b> may include outputting <b>1110</b> control instructions to cause a robotic surgical device to assist in a resection of a patient tibia or femur. The resection may include a tibial plateau resection, which may include a resected horizontal surface and a resected vertical surface. The resection may include an augment resection, which may include a resected augment surface and a resected revision implant surface.</p><p>[0043] Technique <b>1100</b> includes positioning <b>1120</b> a knee arthroplasty validation device to contact the horizontal resection and to contact the vertical resection. Positioning of the knee arthroplasty validation device may include outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device. The knee arthroplasty validation device may include a horizontal resection validation surface, a vertical resection validation surface, one or more augment spacers, and an optical tracker fixedly attached to the knee arthroplasty validation device. The vertical resection validation surface may be orthogonal to the horizontal resection validation surface, and a substantially planar gap validation surface. The gap validation surface may be substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by a gap validation thickness. The gap validation thickness may be used to validate a flexion gap and an extension gap.</p><p>[0044] Technique <b>1100</b> includes validating <b>1130</b>, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker. Technique <b>1100</b> may include validating <b>1135</b>, using processing circuitry of the robotic surgical device, the vertical resection based on a tracked validation position of the optical tracker. Technique <b>1100</b> may include validating <b>1145</b>, using processing circuitry of the robotic surgical device, an augment resection based on a tracked validation position of the optical tracker.</p><p>[0045] Technique <b>1100</b> may include validating <b>1140</b> a flexion gap or an extension gap. Validating <b>1140</b> the flexion gap may include comparing the gap validation thickness of the knee arthroplasty validation device against the flexion gap formed by the patient tibia and a corresponding patient femur in flexion. Validating <b>1140</b> the extension gap may include comparing the gap validation thickness of the knee arthroplasty validation device against the extension gap formed by the patient tibia and the corresponding patient femur in extension.</p><p>[0046] Technique <b>1100</b> may include instructing <b>1150</b> the robotic surgical device to assist in a distal femoral resection of corresponding patient femur. Technique <b>1100</b> may include disposing <b>1160</b> the knee arthroplasty validation device against the distal femoral resection and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the knee arthroplasty validation device.</p><p>[0047] Technique <b>1100</b> may include a surgeon positioning <b>1170</b> the knee arthroplasty validation device and receiving a validation initiation input from the surgeon. The validation input may initiate the validation of the horizontal resection and the vertical resection.</p><p>[0048] Technique <b>1100</b> may include comparing <b>1180</b> the validation position of the knee arthroplasty validation device against a tracked tibial position. The tracked tibial position may be based on an optical tibial tracker fixedly attached to the patient tibia. The tracked tibial position may be based on a registration position of a registration pointer, where the registration pointer is fixedly attached to a robotic arm of the robotic surgical device.</p><p>[0049] FIG. <b>12</b> illustrates an example of a block diagram of a machine <b>1200</b> upon which any one or more of the techniques (e.g., methodologies) discussed herein may perform in accordance with some embodiments. In alternative embodiments, the machine <b>1200</b> may operate as a standalone device or may be connected (e.g., networked) to other machines. In a networked deployment, the machine <b>1200</b> may operate in the capacity of a server machine, a client machine, or both in server-client network environments. The machine <b>1200</b> may be a personal computer (PC), a tablet PC, a personal digital assistant (PDA), a mobile telephone, a web appliance, a network router, switch or bridge, or any machine capable of executing instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term \\\"machine\\\" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein, such as cloud computing, software as a service (SaaS), other computer cluster configurations.</p><p>[0050] Examples, as described herein, may include, or may operate on, logic or a number of components, modules, or like mechanisms. Such mechanisms are tangible entities (e.g., hardware) capable of performing specified operations when operating. In an example, the hardware may be specifically configured to carry out a specific operation (e.g., hardwired). In an example, the hardware may include configurable execution units (e.g., transistors, circuits, etc.) and a computer readable medium containing instructions, where the instructions configure the execution units to carry out a specific operation when in operation. The configuring may occur under the direction of the execution units or a loading mechanism. Accordingly, the execution units are communicatively coupled to the computer readable medium when the device is operating. For example, under operation, the execution units may be configured by a first set of instructions to implement a first set of features at one point in time and reconfigured by a second set of instructions to implement a second set of features.</p><p>[0051] Machine (e.g., computer system) <b>1200</b> may include a hardware processor <b>1202</b> (e.g., a central processing unit (CPU), a graphics processing unit (GPU), a hardware processor core, or any combination thereof), a main memory <b>1204</b> and a static memory <b>1206</b>, some or all of which may communicate with each other via an interlink (e.g., bus) <b>1208</b>. The machine <b>1200</b> may further include a display unit <b>1210</b>, an alphanumeric input device <b>1212</b> (e.g., a keyboard), and a user interface (IA) navigation device <b>1214</b> (e.g., a mouse). In an example, the display unit <b>1210</b>, alphanumeric input device <b>1212</b> and UI navigation device <b>1214</b> may be a touch screen display. The display unit <b>1210</b> may include goggles, glasses, an augmented reality (AR) display, a virtual reality (VR) display, or another display component. For example, the display unit may be worn on a head of a user and may provide a heads-up-display to the user. The alphanumeric input device <b>1212</b> may include a virtual keyboard (e.g., a keyboard displayed virtually in a VR or AR setting.</p><p>[0052] The machine <b>1200</b> may additionally include a storage device (e.g., drive unit) <b>1216</b>, a signal generation device <b>1218</b> (e.g., a speaker), a network interface device <b>1220</b>, and one or more sensors <b>1221</b>, such as a global positioning system (GPS) sensor, compass, accelerometer, or other sensor. The machine <b>1200</b> may include an output controller <b>1228</b>, such as a serial (e.g., universal serial bus (USB), parallel, or other wired or wireless (e.g., infrared (IR), near field communication (NFC), etc.) connection to communicate or control one or more peripheral devices.</p><p>[0053] The storage device <b>1216</b> may include a machine readable medium <b>1222</b> that is non-transitory on which is stored one or more sets of data structures or instructions <b>1224</b> (e.g., software) embodying or utilized by any one or more of the techniques or functions described herein. The instructions <b>1224</b> may also reside, completely or at least partially, within the main memory <b>1204</b>, within static memory <b>1206</b>, or within the hardware processor <b>1202</b> during execution thereof by the machine <b>1200</b>. In an example, one or any combination of the hardware processor <b>1202</b>, the main memory <b>1204</b>, the static memory <b>1206</b>, or the storage device <b>1216</b> may constitute machine readable media.</p><p>[0054] While the machine readable medium <b>1222</b> is illustrated as a single medium, the term \\\"machine readable medium\\\" may include a single medium or multiple media (e.g., a centralized or distributed database, or associated caches and servers) configured to store the one or more instructions <b>1224</b>.</p><p>[0055] The term \\\"machine readable medium\\\" may include any medium that is capable of storing, encoding, or carrying instructions for execution by the machine <b>1200</b> and that cause the machine <b>1200</b> to perform any one or more of the techniques of the present disclosure, or that is capable of storing, encoding or carrying data structures used by or associated with such instructions. Non-limiting machine readable medium examples may include solid-state memories, and optical and magnetic media. Specific examples of machine readable media may include: non-volatile memory, such as semiconductor memory devices (e.g., Electrically Programmable Read-Only Memory (EPROM), Electrically Erasable Programmable Read-Only Memory (EEPROM)) and flash memory devices; magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.</p><p>[0056] The instructions <b>1224</b> may further be transmitted or received over a communications network <b>1226</b> using a transmission medium via the network interface device <b>1220</b> utilizing any one of a number of transfer protocols (e.g., frame relay, internet protocol (IP), transmission control protocol (TCP), user datagram protocol (UDP), hypertext transfer protocol (HTTP), etc.). Example communication networks may include a local area network (LAN), a wide area network (WAN), a packet data network (e.g., the Internet), mobile telephone networks (e.g., cellular networks), Plain Old Telephone (POTS) networks, and wireless data networks (e.g., Institute of Electrical and Electronics Engineers (IEEE) 802.11 family of standards known as Wi-Fi(R), as the personal area network family of standards known as Bluetooth(R) that are promulgated by the Bluetooth Special Interest Group, peer-to-peer (P2P) networks, among others. In an example, the network interface device <b>1220</b> may include one or more physical jacks (e.g., Ethernet, coaxial, or phone jacks) or one or more antennas to connect to the communications network <b>1226</b>. In an example, the network interface device <b>1220</b> may include a plurality of antennas to wirelessly communicate using at least one of single-input multiple-output (SIMO), multiple-input multiple-output (MIMO), or multiple-input single-output (MISO) techniques. The term \\\"transmission medium\\\" shall be taken to include any intangible medium that is capable of storing, encoding, or carrying instructions for execution by the machine <b>1200</b>, and includes digital or analog communications signals or other intangible medium to facilitate communication of such software.</p><XXEMP><p>[0057] Each of the following non-limiting examples may stand on its own, or may be combined in various permutations or combinations with one or more of the other examples: <li><li>Example 1 is a knee arthroplasty validation method for intraoperative validation of cut surfaces, the method comprising: outputting control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; positioning a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; validating, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and triggering an update of a display to indicate completion of the validation.</li><li>In Example 2, the subject matter of Example 1 includes, validating, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</li><li>In Example 3, the subject matter of Example 2 includes, validating, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness,</li><li>In Example 4, the subject matter of Example 3 includes, validating, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</li><li>In Example 5, the subject matter of Example 4 includes, instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; disposing the PKA validation device against the distal femoral resection; and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</li><li>In Example 6, the subject matter of Examples 2-5 includes, validating, using processing circuity of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</li><li>In Example 7, the subject matter of Examples 1-6 includes, wherein the positioning of the knee arthroplasty validation device includes outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</li><li>In Example 8, the subject matter of Examples 1-7 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the method further including receiving a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</li><li>In Example 9, the subject matter of Examples 1-8 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</li><li>In Example 10, the subject matter of Example 9 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</li><li>In Example 11, the subject matter of Examples 9-10 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</li><li>Example 12 is a knee arthroplasty validation system for intraoperative validation of cut surfaces, the system comprising: a robotic surgical device including processing circuitry, the robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; a knee arthroplasty validation device positioned to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; and an optical tracker fixedly attached to the knee arthroplasty validation device; wherein processing circuitry of the robotic surgical device validates the horizontal resection validation surface based on a tracked validation position of the optical tracker and triggers an update of a display to indicate completion of the validation.</li><li>In Example 13, the subject matter of Example 12 includes, the processing circuitry of the robotic surgical device further to validate a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of a patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</li><li>In Example 14, the subject matter of Example 13 includes, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</li><li>In Example 15, the subject matter of Example 14 includes, wherein the substantially planar gap validation surface validates a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</li><li>In Example 16, the subject matter of Example 15 includes, wherein in response to receipt of control instructions, the robotic surgical device is further to: assist in a distal femoral resection of patient femur corresponding to the patient tibia; and validate the distal femoral resection based on a tracked femoral position of the PKA validation device disposed against the distal femoral resection.</li><li>In Example 17, the subject matter of Examples 12-16 includes, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</li><li>In Example 18, the subject matter of Examples 12-17 includes, wherein in response to receipt of control instructions, the control instructions further cause the robotic surgical device to position the knee arthroplasty validation device.</li><li>In Example 19, the subject matter of Examples 12-18 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the processing circuitry of the robotic surgical device further to receive a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</li><li>In Example 20, the subject matter of Examples 12-19 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</li><li>In Example 21, the subject matter of Example 20 includes, an optical bone position tracker fixedly attached to the patient tibia or the patient femur, wherein the tracked bone position is based on the optical bone position tracker.</li><li>In Example 22, the subject matter of Examples 20-21 includes, a registration pointer fixedly attached to a robotic arm of the robotic surgical device, wherein the tracked bone position is based on a registration position of a registration pointer.</li><li>Example 23 is at least one non-transitory machine-readable storage medium, comprising a plurality of instructions that, responsive to being executed with processor circuitry of a computer-controlled device, cause the computer-controlled device to: output control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; position a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; validate, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and trigger an update of a display to indicate completion of the validation.</li><li>In Example 24, the subject matter of Example 23 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</li><li>in Example 25, the subject matter of Example 24 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</li><li>In Example 26, the subject matter of Example 25 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</li><li>In Example 27, the subject matter of Example 26 includes, instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; disposing the PKA validation device against the distal femoral resection; and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</li><li>In Example 28, the subject matter of Examples 24-27 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</li><li>In Example 29, the subject matter of Examples 23-28 includes, wherein the positioning of the knee arthroplasty validation device includes outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</li><li>In Example 30, the subject matter of Examples 23-29 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the instructions further causing the computer-controlled device to receive a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</li><li>In Example 31, the subject matter of Examples 23-30 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</li><li>In Example 32, the subject matter of Example 31 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</li><li>In Example 33, the subject matter of Examples 31-32 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</li><li>Example 34 is a knee arthroplasty validation apparatus for intraoperative validation of cut surfaces, the apparatus comprising: means for outputting control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; means for positioning a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; means for validating, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and means for triggering an update of a display to indicate completion of the validation.</li><li>In Example 35, the subject matter of Example 34 includes, means for validating, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</li><li>In Example 36, the subject matter of Example 35 includes, means for validating, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</li><li>In Example 37, the subject matter of Example 36 includes, means for validating, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</li><li>In Example 38, the subject matter of Example 37 includes, means for instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; means for disposing the PKA validation device against the distal femoral resection; and means for instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</li><li>In Example 39, the subject matter of Examples 35-38 includes, means for validating, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut, validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</li><li>In Example 40, the subject matter of Examples 34-39 includes, wherein the means for positioning of the knee arthroplasty validation device includes means for outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</li><li>In Example 41, the subject matter of Examples 34-40 includes, wherein the means for positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the apparatus further including means for receiving a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</li><li>In Example 42, the subject matter of Examples 34-41 includes, wherein the validation of the horizontal resection validation surface further includes means for comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</li><li>In Example 43, the subject matter of Example 42 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</li><li>In Example 44, the subject matter of Examples 42-43 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</li><li>Example 45 is at least one machine-readable medium including instructions that, when executed by processing circuitry, cause the processing circuitry to perform operations to implement of any of Examples 1-44.</li><li>Example 46 is an apparatus comprising means to implement of any of Examples 1-44.</li><li>Example 47 is a system to implement of any of Examples 1-44.</li><li>Example 48 is a method to implement of any of Examples 1-44.</li></li></p><p>[0106] Method examples described herein may be machine or computer-implemented at least in part. Some examples may include a computer-readable medium or machine-readable medium encoded with instructions operable to configure an electronic device to perform methods as described in the above examples. An implementation of such methods may include code, such as microcode, assembly language code, a higher-level language code, or the like. Such code may include computer readable instructions for performing various methods. The code may form portions of computer program products. Further, in an example, the code may be tangibly stored on one or more volatile, non-transitory, or non-volatile tangible computer-readable media, such as during execution or at other times. Examples of these tangible computer-readable media may include, but are not limited to, hard disks, removable magnetic disks, removable optical disks (e.g., compact disks and digital video disks), magnetic cassettes, memory cards or sticks, random access memories (RAMs), read only memories (ROMs), and the like.</p></XXEMP>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A knee arthroplasty validation system for intraoperative validation of cut surfaces, the system comprising: <BR />a robotic surgical device including processing circuitry, the robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection and a vertical resection;<BR />a solid knee arthroplasty validation device positioned to contact the horizontal resection and the vertical resection, the solid knee arthroplasty validation device including a horizontal resection validation surface, a top validation surface opposite from the horizontal resection validation surface, a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection, a distal portion to validate a posterior tibial plateau resection gap, a proximate portion to validate an anterior tibial plateau resection gap, and an anterior stop configured to rest against a tibial anterior cortex of the patient tibia; and<BR />an optical tracker fixedly attached to the proximate portion of the solid knee arthroplasty validation device;<BR />wherein the knee arthroplasty validation device includes a gap validation thickness between the top validation surface and the horizontal resection validation surface, the gap validation thickness being different in the proximate portion from the distal portion;<BR />wherein the processing circuitry of the robotic surgical device validates the vertical resection validation surface, the posterior tibial plateau resection gap, and the anterior tibial plateau resection gap based on a tracked validation position of the optical tracker and triggers an update on a display to indicate completion of the validation.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The system of claim 1, wherein the tibiofemoral joint resection includes a tibial plateau resection of a patient tibia, the tibial plateau resection including the horizontal resection and the vertical resection; and <BR />wherein the solid knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including the vertical resection validation surface.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>3. The system of claim 2, the processing circuitry of the robotic surgical device further to validate, using the processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; <BR />wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"3\\\"><p>4. The system of claim 3, wherein the substantially planar gap validation surface validates a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"4\\\"><p>5. The system of claim 4, wherein in response to receipt of control instructions, the robotic surgical device is further to: <BR />assist in a distal femoral resection of patient femur corresponding to the patient tibia; and<BR />validate the distal femoral resection based on a tracked femoral position of the PKA validation device disposed against the distal femoral resection.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>6. The system of claim 2, wherein the processing circuitry further validates, using the gap validation thickness, a flexion gap formed between the tibial plateau resection and a corresponding resected surface of the patient femur when a patient knee is in flexion.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"6\\\"><p>7. The system of claim 6, wherein the processing circuitry further validates, using the gap validation thickness, an extension gap formed between the tibial plateau resection and the corresponding resected surface of the patient femur when the patient knee is in extension.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The system of claim 1, wherein the processing circuitry further validates the horizontal resection by determining a horizontal resection depth and a horizontal resection slope based on the tracked validation position.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>9. The system of claim 1, wherein the processing circuitry further determines a location of a tibial posterior cortex of the patient tibia using a length of the arthroplasty validation device.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"6\\\"><p>10. The system of claim 6, wherein the solid knee arthroplasty validation device includes: <BR />a proximate portion, the optical tracker fixedly attached to the proximate portion; and<BR />a distal portion opposite from the proximate portion, the gap validation thickness associated with the distal portion.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B17/02\",\n",
      "\n",
      "\"A61B17/16\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/30\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B17/1675\",\n",
      "\n",
      "\"A61B2034/2055\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/30\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"ORTHOSOFT ULC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20201216,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US12213686\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20250204,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US12213686\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20411216,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=91933436\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20210552492\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"KNEE ARTHROPLASTY VALIDATION AND GAP BALANCING INSTRUMENTATION\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"To address technical problems facing knee arthroplasty resection validation, the present subject matter provides a tracked knee arthroplasty instrument for objective measurement of resection depth. By performing a precise comparison between the location of the tracked knee arthroplasty instrument and a reference location, the knee arthroplasty instrument measures and validates each tibial and femoral resection. To address technical problems facing validation of joint laxity following knee arthroplasty, the tracked knee arthroplasty instrument is shaped to validate the flexion gap and extension gap. When the tracked knee arthroplasty instrument is inserted between the resected tibial plateau and femoral head, the instrument shape validates whether the desired flexion gap and extension gap have been achieved.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATION</p><p>[0001] This application is a continuation application of U.S. Provisional Patent Application No. 63/126,395, filed on Dec. 16, 2020, and entitled \\\"Knee Arthroplasty Validation and Gap Balancing Instrumentation,\\\" the application of which is hereby incorporated by reference in its entirety,</p></relapp><shortsum><p>FIELD</p><p>[0002] The present application relates to surgical knee replacement.</p><p>BACKGROUND</p><p>[0003] A knee replacement procedure (e.g., knee arthroplasty) is used to repair or replace damaged bone or damaged tissue in a patient knee joint. A knee arthroplasty includes repairing or replacing damaged or diseased articular surfaces of the tibia or femur. The arthroplasty procedure may include cutting (e,g., resecting) one or more articular surfaces of the tibia and femur and replacing a portion of each articular surface with a prosthesis (e.g., articular surface implant). A total knee arthroplasty (TKA) may be used to repair all articular surfaces of the tibia and femur, whereas a partial knee arthroplasty (PKA) may be used to repair a portion of the articular surfaces of the knee, such as the medial, lateral, or patellofemoral compartment. The TKA and PKA procedures require precise resections of the tibia and femur. The cut depth for each resection is specific to the patient and each prosthesis. A surgeon may validate a resection depth manually by inserting a trial prosthesis and exercising the knee through various motions. However, this resection validation is subjective and subject to errors. What is needed is an improved knee arthroplasty resection validation.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0004] FIG. 1 is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0005] FIGS. 2A-2B are perspective views of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0006] FIG. 3 is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0007] FIG. 4 is a perspective view of a tracked knee arthroplasty system, in accordance with some embodiments.</p><p>[0008] FIG. 5 is a tibial resection diagram, in accordance with some embodiments.</p><p>[0009] FIG. 6 is a tibial resection slope graph, in accordance with some embodiments.</p><p>[0010] FIG. 7 is a diagram of a knee arthroplasty graphical user interface (GUI), in accordance with some embodiments.</p><p>[0011] FIGS. 8A-8D are diagrams of an augment cut validation, in accordance with some embodiments.</p><p>[0012] FIGS. 9A-9C are diagrams of an augment cut validation device, in accordance with some embodiments.</p><p>[0013] FIG. 10 is a diagram of an augment cut validation tracker device, in accordance with some embodiments.</p><p>[0014] FIG. 11 illustrates a flow chart showing a knee arthroplasty technique, in accordance with some embodiments.</p><p>[0015] FIG. 12 illustrates an example of a block diagram of a machine upon which any one or more of the techniques (e.g., methodologies) discussed herein may perform in accordance with some embodiments.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0016] The present disclosure describes technical solutions to various technical problems facing knee arthroplasty procedures. To address technical problems facing knee arthroplasty resection validation, the present subject matter provides a tracked knee arthroplasty instrument for objective measurement of resection depth. By performing a precise comparison between the location of the tracked knee arthroplasty instrument and a reference location, the knee arthroplasty instrument measures and validates each tibial and femoral resection. To address technical problems facing validation of joint laxity following knee arthroplasty, the tracked knee arthroplasty instrument is shaped to validate the flexion gap and extension gap. When the tracked knee arthroplasty instrument is inserted between the resected tibial plateau and femoral head, the instrument shape validates whether the desired flexion gap and extension gap have been achieved.</p><p>[0017] In an example PKA surgical procedure, a tibia is resected, the tracked knee arthroplasty instrument is used to validate the resection and check flexion gap and extension gap, the femur is resected, and the tracked knee arthroplasty instrument is again used to validate the resection and check gaps. The use of the tracked knee arthroplasty instrument to validate resections and check gaps ensures that bone gaps and soil tissue allow for sufficient space for an implant and sufficient space in the postoperative elongated leg. In addition to validating resections and checking gaps, the use of the tracked knee arthroplasty instrument provides information regarding limb alignment and tactile feel of the resected surfaces.</p><p>[0018] The tracked knee arthroplasty instrument may be used with a robotic surgical device. In an example, a robotic surgical device may perform a tibial or femoral resection, and the tracked knee arthroplasty instrument may be used by a surgeon or by the robotic surgical device to validate resections and check gaps. In an example, the robotic surgical device may position resection surgical tools to prepare for the resection, a surgeon may perform a tibial or femoral resection, and the tracked knee arthroplasty instrument may be used by a surgeon or by the robotic surgical device to validate resections and check gaps. The robotic surgical system and tracked knee arthroplasty instrument may use a combination of one or more coordinate systems or tracked positioning systems. In an example, the tracked knee arthroplasty instrument is tracked using an optical tracking system, the robotic surgical device uses a robotic device coordinate system, and a surgical plan management system translates the tracked knee arthroplasty instrument position and robotic surgical device position into a common coordinate system viewable by the surgeon.</p><p>[0019] In the drawings, which are not necessarily drawn to scale, like numerals may describe similar components in different views. Like numerals having different letter suffixes may represent different instances of similar components. The drawings illustrate generally, by way of example, but not by way of limitation, various embodiments discussed in the present document.</p><p>[0020] FIG. 1 is a perspective view of a tracked knee arthroplasty system <b>100</b>, in accordance with some embodiments. System <b>100</b> includes an arthroplasty validation instrument <b>110</b>, where instrument <b>110</b> includes one or more articular contact surfaces that may be placed in contact with a resected tibial surface. In an example, a horizontal resection validation surface on the bottom surface (not shown) of instrument <b>110</b> may be placed on the tibial plateau horizontal resection <b>150</b>. Similarly, a vertical resection validation surface on the distant surface (not shown) of instrument <b>110</b> may be placed on the vertical resection <b>155</b> (e.g., tibial sagittal resection).</p><p>[0021] The thickness (e.g., height) of instrument <b>110</b> separates the top surface from the horizontal resection validation surface on the bottom surface (not shown) of instrument <b>110</b>. This gap validation thickness may be used to validate the gap between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>. FIG. 1 shows the gap validation thickness being used to validate the extension gap while the knee is in flexion, though the gap validation thickness may also be used to validate the flexion gap while the knee is in extension.</p><p>[0022] Instrument <b>110</b> may be attached to a manual manipulation device <b>120</b>. The manipulation device <b>120</b> may include grooves, a grip, or other surface to improve the ability of a surgeon to manipulate the instrument <b>110</b>. The instrument <b>110</b> or the manipulation device <b>120</b> may include an orientation mechanism (e.g., detent, keying surface) to ensure the instrument <b>110</b> and manipulation device <b>120</b> are attached in a reliable and precise configuration. In an example, the instrument <b>110</b> includes a threaded aperture and the manipulation device <b>120</b> includes a threaded socket, and a threaded screw <b>130</b> is attached through the instrument <b>110</b> into the manipulation device <b>120</b>.</p><p>[0023] The manipulation device <b>120</b> may be attached to a location tracking device <b>140</b>, such as an optical tracker. The tracking device <b>140</b> may be used by an optical tracking system to determine the precise location of the instrument <b>110</b>. In an example, once the instrument <b>110</b> is positioned against the horizontal resection <b>150</b> and against the vertical resection <b>155</b>, the tracking device <b>140</b> may be used to validate the horizontal resection <b>150</b> and the vertical resection <b>155</b>. The validation of the horizontal resection <b>150</b> may include determining a resection cut depth, a varus or valgus angle, a resection slope, or other horizontal resection geometry. The validation of the vertical resection <b>155</b> may include determining a resection rotation, a resection medial-lateral offset, or other vertical resection geometry.</p><p>[0024] In another example, the position of the instrument <b>110</b> may be tracked to ensure the instrument <b>110</b> is inserted to a sufficient depth between the tibial plateau horizontal resection <b>150</b> and the native femoral head <b>160</b> or a distal femoral resection, where the gap validation thickness (e.g., height) of instrument <b>110</b> is used to validate the gap between the tibial plateau horizontal resection <b>150</b> and the native femoral head <b>160</b> or a distal femoral resection. The optical system may determine the position of the tracking device <b>140</b> relative to another tracked position, such as relative to an optical tracker fixedly attached to the patient tibia, relative to a registration pointer attached to a robotic arm, or relative to another tracked position.</p><p>[0025] FIGS. 2A-2B are perspective views of a tracked knee arthroplasty system <b>200</b>, in accordance with some embodiments. System <b>200</b> includes an arthroplasty validation instrument <b>210</b> attached to a manual manipulation device <b>220</b>, such as using a threaded screw <b>130</b> threaded. through instrument <b>210</b> into manipulation device <b>220</b>. Instrument <b>210</b> may include a proximate portion <b>225</b> that is proximate to the manipulation device <b>220</b>, and may include a distal portion <b>215</b> that is distal from the manipulation device <b>220</b>.</p><p>[0026] As shown in FIG. 2A, the proximate portion <b>225</b> may be thicker than the distal portion <b>215</b>. The use of different thicknesses may be used to validate different gap sizes, such as validating a posterior gap on a posterior portion of a tibial plateau resection and a larger anterior gap on an anterior portion of the tibial plateau resection. Instrument <b>210</b> may include a transition region <b>235</b> between the proximate portion <b>225</b> and the distal portion <b>215</b>. The transition region <b>235</b> may facilitate insertion of the instrument <b>210</b> between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>, such as by providing a linear sigmoid, or other smooth transition between the proximate portion <b>225</b> and the distal portion <b>215</b>.</p><p>[0027] As shown in FIG. 2B, the proximate portion <b>225</b> may be wider than the distal portion <b>215</b>. The wider proximate portion <b>225</b> may be used to provide a mechanical stop, such as by providing a stop against an anterior tibial surface when inserted between the tibial plateau horizontal resection <b>150</b> and the femoral head <b>160</b>.</p><p>[0028] FIG. 3 is a perspective view of a tracked knee arthroplasty system <b>300</b>, in accordance with some embodiments, System <b>300</b> includes a proximate portion <b>325</b> of an arthroplasty validation instrument, which may be inserted into a patient incision <b>350</b>. The proximate portion <b>325</b> may be attached to a manual manipulation device <b>220</b>. The manipulation device <b>320</b> may include a pointed tip portion <b>340</b> that is received within a tip receptacle within proximate portion <b>325</b>. Once the pointed tip portion <b>340</b> is seated correctly within the tip receptacle, the manipulation device <b>320</b> may be secured to the proximate portion <b>325</b> using a threaded screw <b>330</b>.</p><p>[0029] FIG. 4 is a perspective view of a tracked knee arthroplasty system <b>400</b>, in accordance with some embodiments. System <b>400</b> includes an arthroplasty validation instrument <b>410</b>, where instrument <b>410</b> includes one or more articular contact surfaces that may be placed in contact with a resected tibial surface. A horizontal resection validation surface on the bottom surface (not shown) of instrument <b>410</b> may be placed on the tibial plateau horizontal resection <b>450</b>. Similarly, a vertical resection validation surface on the distant surface (not shown) of instrument <b>410</b> may be placed on the vertical resection <b>455</b> (e.g., tibial sagittal resection). Instrument <b>410</b> may be attached to a manual manipulation device <b>420</b>, such as using a threaded screw <b>430</b> threaded through instrument <b>410</b> into the manipulation device <b>420</b>, Manipulation device <b>420</b> may be connected to an optical tracker or other tracking device (not shown).</p><p>[0030] The thickness of instrument <b>410</b> separates the top surface from the horizontal resection validation surface on the bottom surface (not shown) of instrument <b>410</b>. This gap validation thickness may be used to validate the gap between the tibial plateau horizontal resection <b>450</b> and the resected femoral head <b>460</b>. FIG. 4 shows the gap validation thickness being used to validate the extension gap while the knee is in flexion, though the gap validation thickness may also be used to validate the flexion gap while the knee is in extension.</p><p>[0031] Instrument <b>410</b> includes an anterior stop <b>440</b>. When fully inserted, the anterior stop <b>440</b> rests against the tibial anterior cortex <b>470</b>. The anterior stop <b>440</b> may be used to minimize or prevent instrument <b>410</b> from migrating during drilling, pinning, impaction, or other surgical procedures. When used with a tracking device, the anterior stop <b>440</b> may be used to provide key cortex location information or other tracking information, which may be used to make more precise recuts in imageless cases. This tracking information may reduce or prevent the need for discrete (e.g., dedicated) digitization or registration pointer checks.</p><p>[0032] Instrument <b>410</b> may include one or more structural features to provide additional validation information. The length of instrument <b>410</b> may be used to locate the tibial posterior cortex while validating the tibial plateau resection plane. In an example, instrument <b>410</b> may include distal tibial hooks, distal tibial stops, or other mechanical features (not shown) extending beyond the end of instrument <b>410</b> to locate the posterior cortex. This determined location of the posterior cortex may assist in finding additional reference locations for anatomic landmarking, such as to define the tibial internal and external rotation coordinate system at the plane of the tibial resection. The combination of distal tibial hook and the anterior stop <b>440</b> may be used to provide information about the geometry of the tibia, which may be used to size the tibia. In an example, instrument <b>410</b> may include medial or lateral tibial side hooks or other mechanical features (not shown) extending to either side of instrument <b>410</b>. The side hooks may be used to map the size and geometry of the medial cortex or lateral cortex. This cortex information may be used for femoral sizing, such as selecting standard or narrow femoral head implants. In an example, instrument <b>410</b> may include a distal trochlea stylus (not shown), which might be used to locate or map the femoral trochlea (e.g., intercondylar fossa of femur). The trochlea stylus may provide anterior reference information, which may be used to improve femoral sizing or notching information within a resection. Information from the anterior stop <b>440</b> or one or more tibial hooks may be used to validate resections or update anatomic information. In an example, anatomic information may be gathered through preoperative digitization of the bone, and the preoperatively gathered information may be updated using intraoperative information gathered from the anterior stop <b>440</b> or one or more tibial hooks. This updated information may be used to refresh or improve surgical plans intraoperatively while reducing or minimizing additional intraoperative surgical procedure steps.</p><p>[0033] FIG, <b>5</b> is a tibial resection diagram <b>500</b>, in accordance with some embodiments. A surgeon may use an arthroplasty validation instrument to determine that the depth or slope of the primary cut <b>510</b> (e.g., initial tibial resection) is insufficient, and that a secondary cut <b>520</b> (e.g., secondary resection) may be needed. To change the slope of a tibial resection, the secondary cut <b>520</b> must begin at a lower point on the tibial anterior cortex to ensure a full resection. The starting points of the primary cut <b>510</b> and the secondary cut <b>520</b> may be separated by a cut bias <b>530</b>. To minimize the number of additional tibial resections, the cut bias <b>530</b> may be selected to be the smallest bias that is sufficiently large to perform the secondary cut <b>520</b>. This may be particularly useful when performing a secondary cut <b>520</b> where there is insufficient information available about the location of the tibial anterior cortex, such as in imageless arthroplasty procedures. The bias selection may be improved by determining information about the location of the anterior cortex, such as using the anterior stop <b>440</b> to provide cortex location information.</p><p>[0034] FIG. 6 is a tibial resection slope graph <b>600</b>, in accordance with some embodiments. Graph <b>600</b> depicts an example primary cut <b>610</b> and a secondary cut <b>620</b>. In a conventional TKA surgery, the rotation point for the posterior slope is set at the anterior aspect of the tibia, so, the surgeon does not need to worry about increased resection depth for an increased slope recut. For a PKA surgery, the posterior slope is set based on the middle of the tibial plateau, so a secondary cut to change the slope will always include an increase in the resection depth <b>630</b> (e.g., secondary cut bias) to ensure a full resection.</p><p>[0035] The slope and depth of the secondary cut <b>620</b> may be adjustable to provide a desired slope while remaining consistent with other surgical parameters. In an example, a PKA surgical plan may have an associated maximum allowed parallel recut <b>640</b>, which may correspond with a worst-case slope and depth change <b>650</b>. Table 1 shows various combinations of tibial resection depth and slope. In particular, Table 1 shows a minimum increase in depth required to provide a full resection, and shows the maximum increase in resection depth that will result in a resection within 3 mm distal to the primary cut on the anterior/posterior side (e.g., maximum allowed parallel recut).</p><p><Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Tibial Resection Depth and Slope</TD></TR><TR><TD>Change in</TD><TD>Minimum</TD><TD>Minimum change in</TD><TD>Maximum increase in</TD></TR><TR><TD>Slope (with</TD><TD>change in</TD><TD>resection depth</TD><TD>resection depth (for</TD></TR><TR><TD>respect to</TD><TD>resection</TD><TD>required for</TD><TD>minimum resection</TD></TR><TR><TD>first cut)</TD><TD>depth</TD><TD>full resection</TD><TD>depth change)</TD></TR><TR></TR><TR><TD>0 degrees </TD><TD>0.0</TD><TD> 0 mm</TD><TD><sup></sup>3 mm</TD></TR><TR><TD>2 degrees </TD><TD>0.72</TD><TD> 1.1 mm</TD><TD>3.7 mm</TD></TR><TR><TD>4 degrees </TD><TD>1.43</TD><TD>2.17 mm</TD><TD>4.4 mm</TD></TR><TR><TD>5 degrees </TD><TD>1.79</TD><TD>2.71 mm</TD><TD>4.8 mm</TD></TR><TR><TD>6 degrees </TD><TD>2.15</TD><TD>3.26 mm</TD><TD>5.1 mm</TD></TR><TR><TD>8 degrees </TD><TD>2.88</TD><TD>4.36 mm</TD><TD>5.9 mm</TD></TR><TR><TD>10 degrees </TD><TD>3.61</TD><TD> 5.5 mm</TD><TD>6.6 mm</TD></TR><TR></TR></Table></p><p>[0036] FIG. 7 is a diagram of a knee arthroplasty graphical user interface (GUI) <b>700</b>, in accordance with some embodiments. GUI <b>700</b> may be used to display information about planned or measured arthroplasty resection depths or angles. GUI <b>700</b> may include an anterior view <b>710</b> of the femoral head <b>720</b> and the proximal tibia <b>730</b>. Similarly, GUI <b>700</b> may include a medial view <b>715</b> of the femoral head <b>725</b> and the proximal tibia <b>735</b>. The anterior view <b>710</b> may have an associated anterior view control <b>740</b>, and the medial view <b>715</b> may have an associated medial view control <b>745</b>, which may be used to rotate the view of the femur and tibia displayed within GUI <b>700</b>. The anterior view <b>710</b> may have an associated anterior tibial control <b>750</b>, and the medial view <b>715</b> may have an associated medial tibial control <b>755</b>, which may be used to change the flexion angle or modify tibial slope or resection. GUI <b>700</b> may also provide information about distal resection depth <b>760</b>, proximal resection depth <b>765</b>, proximal resection slope angles <b>770</b>, posterior slope angles <b>775</b>, hip-knee-ankle (HKA) axis angles <b>780</b>, plan laxity measurements <b>785</b>, and a flexion angle <b>790</b>.</p><p>[0037] The display of information, bone views, or other portions within GUI <b>700</b> may be modified to indicate whether one or more steps in the knee arthroplasty surgical procedure have been completed. For example, the proximal resection depth <b>765</b> may be presented in a first color to indicate a sufficient resection depth, and the proximal tibia <b>730</b> and proximal resection angle <b>770</b> may be presented in a second color to indicate additional surgical procedure steps are needed to provide the planned resection slope. In another example, the proximal resection depth <b>765</b> may be presented in a first color to indicate the depth is based on a depth validated by an arthroplasty validation instrument, and the proximal tibia <b>730</b> and proximal resection angle <b>770</b> may be presented in a second color to indicate the displayed resection slope angle is using outdated information.</p><p>[0038] FIGS. 8A-8D are diagrams of an augment cut validation <b>800</b>, in accordance with sonic embodiments. FIG. 8A shows a patient tibia with a partial implant <b>810</b>, such as may be used in a PKA surgical procedure. FIG. 8B shows a horizontal revision surgery tibia cut <b>820</b> and a deeper augment cut <b>830</b>. A surgeon may use the revision surgery when a portion of the knee has bad bone quality, where the surgeon can remove the bad bone quality region with an augment implant to provide a stable surface for the femoral implant. FIG. 8C shows the revision surgery with an augment implant <b>840</b> and a revision implant <b>850</b>. While FIG. 8C shows a revision surgery with a correct augment implant cut depth, FIG. 8D shows a revision surgery with an insufficient augment implant cut depth, resulting in a gap <b>860</b>. To determine whether the augment implant cut depth is sufficient, an augment cut validation device may be used, such as shown in FIG. 9A.</p><p>[0039] Ms, <b>9</b>A-<b>9</b>C are diagrams of an augment cut validation device <b>900</b>, in accordance with some embodiments. The augment cut validation device <b>900</b> may be used to determine whether a revision surgery augment resection and horizontal resection are cut to a correct depth. As shown in FIG. 9A, augment cut validation device <b>900</b> may include a tracker mount <b>910</b> and a base <b>920</b>. One or more slide-in augment spacers <b>930</b>, <b>935</b> may be attached to base <b>920</b>. In an example, each augment spacer <b>930</b>, <b>935</b> may have a flange <b>940</b> that slides within base channel <b>925</b> and one or more and tents <b>945</b> to secure the augment spacer <b>930</b>, <b>935</b> in a fixed position relative to the augment cut validation device <b>900</b>. As shown in FIG. 9B, an augment cut validation device <b>900</b> may have an extended base for validating a surface on a larger bone. As shown in FIG. 9C, variously sized augment spacers <b>950</b> may be used. In various examples, the augment spacers <b>950</b> may include incremental sizes, such as 5 mm, 10 mm, 15 mm, or other sizes. In an example, two different sized augment spacers <b>950</b> may be used to validate a first cut dept of a resected surface of a horizontal resection and a deeper cut depth of a resected surface of an augment resection.</p><p>[0040] FIG. 10 is a diagram of an augment cut validation tracker device <b>1000</b>, in accordance with some embodiments. The augment cut validation tracker device <b>1000</b> includes a tracker mount <b>1010</b> that attaches to a tracker attachment <b>1020</b>, which is fixedly attached to an optical tracker <b>1040</b>. The augment cut validation tracker device <b>1000</b> includes one or more augment spacers <b>1030</b> that may be used to validate a revision surgery augment resection and horizontal resection. In an example, surgeon may position the augment cut validation tracker device <b>1000</b> such that the augment spacers <b>1030</b> are in contact with an augment resection and horizontal resection of a patient tibia <b>1050</b>, and the optical tracker <b>1040</b> may be used to determine the depth of the augment resection and horizontal resection by comparing a measured location of the optical tracker <b>1040</b> against a known location of the tibia <b>1050</b>. Similarly, the augment cut validation tracker device <b>1000</b> may be used to compare the augment resection depth to the horizontal resection depth, such as by determining that a vertical axis of the optical tracker <b>1040</b> is offset from the vertical axis of the tibia <b>1050</b>.</p><p>[0041] FIG. 11 illustrates a flow chart showing a knee arthroplasty technique <b>1100</b>, in accordance with some embodiments. Technique <b>1100</b> may include outputting <b>1110</b> control instructions to cause a robotic surgical device to assist in a resection of a patient tibia or femur. The resection may include a tibial plateau resection, which may include a resected horizontal surface and a resected vertical surface. The resection may include an augment resection, which may include a resected augment surface and a resected revision implant surface.</p><p>[0042] Technique <b>1100</b> includes positioning <b>1120</b> a knee arthroplasty validation device to contact the horizontal resection and to contact the vertical resection. Positioning of the knee arthroplasty validation device may include outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device. The knee arthroplasty validation device may include a horizontal resection validation surface, a vertical resection validation surface, one or more augment spacers, and an optical tracker fixedly attached to the knee arthroplasty validation device. The vertical resection validation surface may be orthogonal to the horizontal resection validation surface, and a substantially planar gap validation surface. The gap validation surface may be substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by a gap validation thickness. The gap validation thickness may be used to validate a flexion gap and an extension gap.</p><p>[0043] Technique <b>1100</b> includes validating <b>1130</b>, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker. Technique <b>1100</b> may include validating <b>1135</b>, using processing circuitry of the robotic surgical device, the vertical resection based on a tracked validation position of the optical tracker. Technique <b>1100</b> may include validating <b>1145</b>, using processing circuitry of the robotic surgical device, an augment resection based on a tracked validation position of the optical tracker.</p><p>[0044] Technique <b>1100</b> may include validating <b>1140</b> a flexion gap or an extension gap. Validating <b>1140</b> the flexion gap may include comparing the gap validation thickness of the knee arthroplasty validation device against the flexion gap formed by the patient tibia and a corresponding patient femur in flexion. Validating <b>1140</b> the extension gap may include comparing the gap validation thickness of the knee arthroplasty validation device against the extension gap formed by the patient tibia and the corresponding patient femur in extension,</p><p>[0045] Technique <b>1100</b> may include instructing <b>1150</b> the robotic surgical device to assist in a distal femoral resection of corresponding patient femur. Technique <b>1100</b> may include disposing <b>1160</b> the knee arthroplasty validation device against the distal femoral resection and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the knee arthroplasty validation device.</p><p>[0046] Technique <b>1100</b> may include a surgeon positioning <b>1170</b> the knee arthroplasty validation device and receiving a validation initiation input from the surgeon. The validation input may initiate the validation of the horizontal resection and the vertical resection.</p><p>[0047] Technique <b>1100</b> may include comparing <b>1180</b> the validation position of the knee arthroplasty validation device against a tracked tibial position. The tracked tibial position may be based on an optical tibial tracker fixedly attached to the patient tibia. The tracked tibial position may be based on a registration position of a registration pointer, where the registration pointer is fixedly attached to a robotic arm of the robotic surgical device.</p><p>[0048] FIG. 12 illustrates an example of a block diagram of a machine <b>1200</b> upon which any one or more of the techniques (e.g., methodologies) discussed herein may perform in accordance with some embodiments. In alternative embodiments, the machine <b>1200</b> may operate as a standalone device or may be connected (e.g., networked) to other machines. In a networked deployment, the machine <b>1200</b> may operate in the capacity of a server machine, a client machine, or both in server-client network environments. The machine <b>1200</b> may be a personal computer (PC), a tablet PC, a personal digital assistant (PDA), a mobile telephone, a web appliance, a network router, switch or bridge, or any machine capable of executing instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term \\\"machine\\\" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein, such as cloud computing, software as a service (SaaS), other computer cluster configurations.</p><p>[0049] Examples, as described herein, may include, or may operate on, logic or a number of components, modules, or like mechanisms. Such mechanisms are tangible entities (e.g., hardware) capable of performing specified operations when operating. In an example, the hardware may be specifically configured to carry out a specific operation (e.g., hardwired). In an example, the hardware may include configurable execution units (e.g., transistors, circuits, etc.) and a computer readable medium containing instructions, where the instructions configure the execution units to carry out a specific operation when in operation. The configuring may occur under the direction of the execution units or a loading mechanism. Accordingly, the execution units are communicatively coupled to the computer readable medium when the device is operating, For example, under operation, the execution units may be configured by a first set of instructions to implement a first set of features at one point in time and reconfigured by a second set of instructions to implement a second set of features.</p><p>[0050] Machine (e.g., computer system) <b>1200</b> may include a hardware processor <b>1202</b> (e.g., a central processing unit (CPU), a graphics processing unit (GPU), a hardware processor core, or any combination thereof), a main memory <b>1204</b> and a static memory <b>1206</b>, some or all of which may communicate with each other via an interlink (e.g., bus) <b>1208</b>. The machine <b>1200</b> may further include a display unit <b>1210</b>, an alphanumeric input device <b>1212</b> (es., a keyboard), and a user interface (IA) navigation device <b>1214</b> (e.g., a mouse). In an example, the display unit <b>1210</b>, alphanumeric input device <b>1212</b> and U<b>1</b> navigation device <b>1214</b> may be a touch screen display. The display unit <b>1210</b> may include goggles, glasses, an augmented reality (AR) display, a virtual reality (VR) display, or another display component. For example, the display unit may be worn on a head of a user and may provide a heads-up-display to the user. The alphanumeric input device <b>1212</b> may include a virtual keyboard (e.g., a keyboard displayed virtually in a VR or AR setting.</p><p>[0051] The machine <b>1200</b> may additionally include a storage device (e.g., drive unit) <b>1216</b>, a signal generation device <b>1218</b> (e.g., a speaker), a network interface device <b>1220</b>, and one or more sensors <b>1221</b>, such as a global positioning system (GPS) sensor, compass, accelerometer, or other sensor. The machine <b>1200</b> may include an output controller <b>1228</b>, such as a serial (e.g., universal serial bus (USB), parallel, or other wired or wireless (e.g., infrared (IR), near field communication (NFC), etc.) connection to communicate or control one or more peripheral devices.</p><p>[0052] The storage device <b>1216</b> may include a machine readable medium <b>1222</b> that is non-transitory on which is stored one or more sets of data structures or instructions <b>1224</b> (e.g., software) embodying or utilized by any one or more of the techniques or functions described herein. The instructions <b>1224</b> may also reside, completely or at least partially, within the main memory <b>1204</b>, within static memory <b>1206</b>, or within the hardware processor <b>1202</b> during execution thereof by the machine <b>1200</b>. In an example, one or any combination of the hardware processor <b>1202</b>, the main memory <b>1204</b>, the static memory <b>1206</b>, or the storage device <b>1216</b> may constitute machine readable media.</p><p>[0053] While the machine readable medium <b>1222</b> is illustrated as a single medium, the term \\\"machine readable medium\\\" may include a single medium or multiple media (e.g., a centralized or distributed database, or associated caches and servers) configured to store the one or more instructions <b>1224</b>.</p><p>[0054] The term \\\"machine readable medium\\\" may include any medium that is capable of storing, encoding, or carrying instructions for execution by the machine <b>1200</b> and that cause the machine <b>1200</b> to perform any one or more of the techniques of the present disclosure, or that is capable of storing, encoding or carrying data structures used by or associated with such instructions. Non-limiting machine readable medium examples may include solid-state memories, and optical and magnetic media. Specific examples of machine readable media may include: non-volatile memory, such as semiconductor memory devices (e.g., Electrically Programmable Read-Only Memory (EPROM), Electrically Erasable Programmable Read-Only Memory (EEPROM)) and flash memory devices; magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.</p><p>[0055] The instructions <b>1224</b> may further be transmitted or received over a communications network <b>1226</b> using a transmission medium via the network interface device <b>1220</b> utilizing any one of a number of transfer protocols (e.g., frame relay, internet protocol (IP), transmission control protocol (TCP), user datagram protocol (UDP), hypertext transfer protocol (HTTP), etc.). Example communication networks may include a local area network (LAN), a wide area network (WAN), a packet data network (e.g., the Internet), mobile telephone networks (e.g., cellular networks), Plain Old Telephone (POTS) networks, and wireless data networks (e.g., Institute of Electrical and Electronics Engineers (IEEE) 802.11 family of standards known as Wi-Fi(R), as the personal area network family of standards known as Bluetooth(R) that are promulgated by the Bluetooth Special Interest Group, peer-to-peer (P2P) networks, among others. In an example, the network interface device <b>1220</b> may include one or more physical jacks (e.g., Ethernet, coaxial, or phone jacks) or one or more antennas to connect to the communications network <b>1226</b>. In an example, the network interface device <b>1220</b> may include a plurality of antennas to wirelessly communicate using at least one of single-input multiple-output (SIMO), multiple-input multiple-output (MIMO), or multiple-input single-output (MISO) techniques. The term \\\"transmission medium\\\" shall be taken to include any intangible medium that is capable of storing, encoding, or carrying instructions for execution by the machine <b>1200</b>, and includes digital or analog communications signals or other intangible medium to facilitate communication of such software.</p><p>[0056] Each of the following non-limiting examples may stand on its own, or may be combined in various permutations or combinations with one or more of the other examples:</p><p>[0057] Example 1is a knee arthroplasty validation method for intraoperative validation of cut surfaces, the method comprising: outputting control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; positioning a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; validating, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and triggering an update of a display to indicate completion of the validation.</p><XXEMP><p>[0058] In Example 2. the subject matter of Example 1 includes, validating, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p><p>[0059] In Example 3, the subject matter of Example 2 includes, validating, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness,</p><p>[0060] In Example 4, the subject matter of Example 3 includes, validating, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p><p>[0061] In Example 5, the subject matter of Example 4 includes, instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; disposing the PKA validation device against the distal femoral resection; and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</p><p>[0062] In Example 6, the subject matter of Examples 2-5 includes, validating, using processing circuity of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p><p>[0063] In Example 7, the subject matter of Examples 1-6 includes, wherein the positioning of the knee arthroplasty validation device includes outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</p><p>[0064] In Example 8, the subject matter of Examples 1-7 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the method further including receiving a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</p><p>[0065] In Example 9, the subject matter of Examples 1-8 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p><p>[0066] In Example 10, the subject matter of Example 9 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</p><p>[0067] In Example 11, the subject matter of Examples 9-10 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</p><p>[0068] Example 12 is a knee arthroplasty validation system for intraoperative validation of cut surfaces, the system comprising: a robotic surgical device including processing circuitry, the robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; a knee arthroplasty validation device positioned to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; and an optical tracker fixedly attached to the knee arthroplasty validation device; wherein processing circuitry of the robotic surgical device validates the hod zontal resection validation surface based on a tracked validation position of the optical tracker and triggers an update of a display to indicate completion of the validation.</p><p>[0069] In Example 13, the subject matter of Example 12 includes, the processing circuitry of the robotic surgical device further to validate a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of a patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the hod zontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p><p>[0070] In Example 14, the subject matter of Example 13 includes, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p><p>[0071] In Example 15, the subject matter of Example 14 includes, wherein the substantially planar gap validation surface validates a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p><p>[0072] In Example 16, the subject matter of Example 15 includes, wherein in response to receipt of control instructions, the robotic surgical device is further to: assist in a distal femoral resection of patient femur corresponding to the patient tibia; and validate the distal femoral resection based on a tracked femoral position of the PKA validation device disposed against the distal femoral resection.</p><p>[0073] In Example 17, the subject matter of Examples 12-16 includes, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p><p>[0074] In Example 18, the subject matter of Examples 12-17 includes, wherein in response to receipt of control instructions, the control instructions further cause the robotic surgical device to position the knee arthroplasty validation device.</p><p>[0075] In Example 19, the subject matter of Examples 12-18 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the processing circuitry of the robotic surgical device further to receive a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</p><p>[0076] In Example 20, the subject matter of Examples 12-19 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p><p>[0077] In Example 21, the subject matter of Example 20 includes, an optical bone position tracker fixedly attached to the patient tibia or the patient femur, wherein the tracked bone position is based on the optical bone position tracker.</p><p>[0078] In Example 22, the subject matter of Examples 20-21 includes, a registration pointer fixedly attached to a robotic arm of the robotic surgical device, wherein the tracked bone position is based on a registration position of a registration pointer.</p><p>[0079] Example 23 is at least one non-transitory machine-readable storage medium, comprising a plurality of instructions that, responsive to being executed with processor circuitry of a computer-controlled device, cause the computer-controlled device to: output control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; position a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; validate, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and trigger an update of a display to indicate completion of the validation.</p><p>[0080] In Example 24, the subject matter of Example 23 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p><p>[0081] in Example 25, the subject matter of Example 24 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p><p>[0082] In Example 26, the subject matter of Example 25 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p><p>[0083] In Example 27, the subject matter of Example 26 includes, instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; disposing the PKA validation device against the distal femoral resection; and instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</p><p>[0084] In Example 28, the subject matter of Examples 24-27 includes, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p><p>[0085] In Example 29, the subject matter of Examples 23-28 includes, wherein the positioning of the knee arthroplasty validation device includes outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</p><p>[0086] In Example 30, the subject matter of Examples 23-29 includes, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the instructions further causing the computer-controlled device to receive a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</p><p>[0087] In Example 31, the subject matter of Examples 23-30 includes, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p><p>[0088] In Example 32, the subject matter of Example 31 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</p><p>[0089] In Example 33, the subject matter of Examples 31-32 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</p><p>[0090] Example 34 is a knee arthroplasty validation apparatus for intraoperative validation of cut surfaces, the apparatus comprising: means for outputting control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection; means for positioning a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; means for validating, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and means for triggering an update of a display to indicate completion of the validation.</p><p>[0091] In Example 35, the subject matter of Example 34 includes, means for validating, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p><p>[0092] In Example 36, the subject matter of Example 35 includes, means for validating, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p><p>[0093] In Example 37, the subject matter of Example 36 includes, means for validating, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p><p>[0094] In Example 38, the subject matter of Example 37 includes, means for instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; means for disposing the PKA validation device against the distal femoral resection; and means for instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</p><p>[0095] In Example 39, the subject matter of Examples 35-38 includes, means for validating, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut, validation device including: a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p><p>[0096] In Example 40, the subject matter of Examples 34-39 includes, wherein the means for positioning of the knee arthroplasty validation device includes means for outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</p><p>[0097] In Example 41, the subject matter of Examples 34-40 includes, wherein the means for positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the apparatus further including means for receiving a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</p><p>[0098] In Example 42, the subject matter of Examples 34-41 includes, wherein the validation of the horizontal resection validation surface further includes means for comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p><p>[0099] In Example 43, the subject matter of Example 42 includes, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</p><p>[0100] In Example 44, the subject matter of Examples 42-43 includes, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</p><p>[0101] Example 45 is at least one machine-readable medium including instructions that, when executed by processing circuitry, cause the processing circuitry to perform operations to implement of any of Examples 1-44.</p><p>[0102] Example 46 is an apparatus comprising means to implement of any of Examples 1-44.</p><p>[0103] Example 47 is a system to implement of any of Examples 1-44.</p><p>[0104] Example 48 is a method to implement of any of Examples 1-44.</p><p>[0105] Method examples described herein may be machine or computer-implemented at least in part. Some examples may include a computer-readable medium or machine-readable medium encoded with instructions operable to configure an electronic device to perform methods as described in the above examples. An implementation of such methods may include code, such as microcode, assembly language code, a higher-level language code, or the like. Such code may include computer readable instructions for performing various methods. The code may form portions of computer program products. Further, in an example, the code may be tangibly stored on one or more volatile, non-transitory, or non-volatile tangible computer-readable media, such as during execution or at other times. Examples of these tangible computer-readable media may include, but are not limited to, hard disks, removable magnetic disks, removable optical disks (e.g., compact disks and digital video disks), magnetic cassettes, memory cards or sticks, random access memories (RAMs), read only memories (ROMs), and the like.</p></XXEMP>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. A knee arthroplasty validation method for intraoperative validation of cut surfaces, the method comprising: <BR />outputting control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection;<BR />positioning a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device;<BR />validating, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and<BR />triggering an update of a display to indicate completion of the validation.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The method of claim 1, further including validating, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; <BR />wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and<BR />wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>3</b>. The method of claim 2, further including validating, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; <BR />wherein the PKA. validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"3\\\"><p><b>4</b>. The method of claim 3, further including validating, using processing circuitry of the robotic surgical device, a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"4\\\"><p><b>5</b>. The method of claim 4, further including instructing the robotic surgical device to assist in a distal femoral resection of the patient femur corresponding to the patient tibia; <BR />disposing the PKA validation device against the distal femoral resection; and<BR />instructing the robotic surgical device to validate the distal femoral resection based on a tracked femoral position of the PKA validation device.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>6</b>. The method of claim 2, further including validating, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; <BR />wherein the knee arthroplasty validation device includes an augment cut validation device, the augment cut validation device including: <BR />a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and<BR />a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>7</b>. The method of claim 1, wherein the positioning of the knee arthroplasty validation device includes outputting control instructions to cause the robotic surgical device to position the knee arthroplasty validation device.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>8</b>. The method of claim 1, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the method further including receiving a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>9</b>. The method of claim 1, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>10</b>. The method of claim 9, wherein the tracked bone position is based on an optical bone position tracker fixedly attached to the patient tibia or the patient femur.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>11</b>. The method of claim 9, wherein the tracked bone position is based on a registration position of a registration pointer, the registration pointer fixedly attached to a robotic arm of the robotic surgical device.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"yes\\\"><p><b>17</b>. A knee arthroplasty validation system for intraoperative validation of cut surfaces, the system comprising: <BR />a robotic surgical device including processing circuitry, the robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection;<BR />a knee arthroplasty validation device positioned to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device; and<BR />an optical tracker fixedly attached to the knee arthroplasty validation device;<BR />wherein processing circuitry of the robotic surgical device validates the horizontal resection validation surface based on a tracked validation position of the optical tracker and triggers an update of a display to indicate completion of the validation.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"yes\\\"><p><b>13</b>. The system of claim <b>12</b>, the processing circuitry of the robotic surgical device further to validate a vertical resection based on the tracked validation position of the optical tracker; <BR />wherein the tibiofemoral joint resection includes a tibial plateau resection of a patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and<BR />wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PKA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p><b>14</b>. The system of claim 13, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, a PKA flexion gap by comparing a PKA gap validation thickness of the PKA validation device against the PKA flexion gap formed by the patient tibia and the patient femur in flexion; <BR />wherein the PKA validation device further includes a substantially planar gap validation surface, the substantially planar gap validation surface substantially parallel to the horizontal resection validation surface and separated from the horizontal resection validation surface by the PKA gap validation thickness.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>15</b>. The system of claim 14, wherein the substantially planar gap validation surface validates a PKA extension gap by comparing the PKA gap validation thickness of the PKA validation device against the PKA extension gap formed by the patient tibia and the patient femur in extension.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"15\\\"><p><b>16</b>. The system of claim 15, wherein in response to receipt of control instructions, the robotic surgical device is further to: <BR />assist in a distal femoral resection of patient femur corresponding to the patient tibia; and<BR />validate the distal femoral resection based on a tracked femoral position of the PKA validation device disposed against the distal femoral resection.</p></Claim><p><b>17</b>. The system of claim <b>12</b>, the processing circuitry of the robotic surgical device further to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; <BR />wherein the knee arthroplasty validation device includes an augmentcut validation device, the augment cut validation device including: <BR />a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and<BR />a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p><Claim claimid=\\\"18\\\" independent=\\\"yes\\\"><p><b>18</b>. The system of claim <b>12</b>, wherein in response to receipt of control instructions, the control instructions further cause the robotic surgical device to position the knee arthroplasty validation device.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"yes\\\"><p><b>19</b>. The system of claim <b>12</b>, wherein the positioning of the knee arthroplasty validation device includes a surgeon positioning the knee arthroplasty validation device, the processing circuitry of the robotic surgical device further to receive a validation initiation input from the surgeon, the validation initiation input initiating the validation of the horizontal resection,</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"yes\\\"><p><b>20</b>. The system of claim <b>12</b>, wherein the validation of the horizontal resection validation surface further includes comparing the tracked validation position of the knee arthroplasty validation device against a tracked bone position.</p></Claim><Claim claimid=\\\"21\\\" independent=\\\"no\\\" parents=\\\"20\\\"><p><b>21</b>. The system of claim 20. further including an optical bone position tracker fixedly attached to the patient tibia or the patient femur, wherein the tracked bone position is based on the optical bone position tracker.</p></Claim><Claim claimid=\\\"22\\\" independent=\\\"no\\\" parents=\\\"20\\\"><p><b>22</b>. The system of claim 20, further including a registration pointer fixedly attached to a robotic arm of the robotic surgical device, wherein the tracked bone position is based on a registration position of a registration pointer.</p></Claim><Claim claimid=\\\"23\\\" independent=\\\"yes\\\"><p><b>23</b>. At least one non-transitory machine-readable storage medium, comprising a plurality of instructions that, responsive to being executed with processor circuity of a computer-controlled device, cause the computer-controlled device to: <BR />output control instructions to cause a robotic surgical device to assist in a tibiofemoral joint resection of a patient tibia or a patient femur, the tibiofemoral joint resection including a horizontal resection;<BR />position a knee arthroplasty validation device to contact the horizontal resection, the knee arthroplasty validation device including a horizontal resection validation surface and an optical tracker fixedly attached to the knee arthroplasty validation device;<BR />validate, using processing circuitry of the robotic surgical device, the horizontal resection based on a tracked validation position of the optical tracker; and<BR />trigger an update of a display to indicate completion of the validation.</p></Claim><Claim claimid=\\\"24\\\" independent=\\\"no\\\" parents=\\\"23\\\"><p><b>24</b>. The non-transitory machine-readable storage medium of claim 23, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, a vertical resection based on the tracked validation position of the optical tracker; <BR />wherein the tibiofemoral joint resection includes a tibial plateau resection of the patient tibia, the tibial plateau resection including the horizontal resection and a vertical resection; and<BR />wherein the knee arthroplasty validation device includes a partial knee arthroplasty (PICA) validation device, the PKA validation device including a vertical resection validation surface orthogonal to the horizontal resection validation surface, the vertical resection validation surface to contact and validate the vertical resection.</p></Claim><Claim claimid=\\\"25\\\" independent=\\\"no\\\" parents=\\\"24\\\"><p><b>25</b>. The non-transitory machine-readable storage medium of claim 24, the instructions further causing the computer-controlled device to validate, using processing circuitry of the robotic surgical device, an augment cut validation surface based on the tracked validation position of the optical tracker; <BR />wherein the knee arthroplasty validation device includes an augmentcut validation device, the augment cut validation device including: <BR />a first augment spacer fixedly attached to a first side of the horizontal resection validation surface, the first augment spacer to contact the horizontal resection validation surface; and<BR />a second augment spacer fixedly attached to a second side of the horizontal resection validation surface, the second augment spacer to contact the augment cut validation surface.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B17/16\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/30\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B17/154\",\n",
      "\n",
      "\"A61B17/1675\",\n",
      "\n",
      "\"A61B2034/2048\",\n",
      "\n",
      "\"A61B2034/2055\",\n",
      "\n",
      "\"A61B2034/2068\",\n",
      "\n",
      "\"A61B2090/3983\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61F2/30734\",\n",
      "\n",
      "\"A61F2/4657\",\n",
      "\n",
      "\"A61F2/4684\",\n",
      "\n",
      "\"A61F2002/30387\",\n",
      "\n",
      "\"A61F2002/30736\",\n",
      "\n",
      "\"A61F2002/4663\",\n",
      "\n",
      "\"A61F2002/4668\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"ORTHOSOFT ULC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20201216,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220120,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GOGARTY, EMILY;COUTURE, PIERRE;SANFORD, ADAM H;AND OTHERS;SIGNING DATES FROM 20211217 TO 20220117;REEL/FRAME:058706/0723, Owner: ORTHOSOFT ULC, CANADA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GOGARTY, EMILY;COUTURE, PIERRE;SANFORD, ADAM H;AND OTHERS;SIGNING DATES FROM 20211217 TO 20220117;REEL/FRAME:058706/0723\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ORTHOSOFT ULC, CANADA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220128,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220128,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220616,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230613,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230613,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230813,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230813,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230829,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230829,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231204,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231204,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240130,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FRM1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Final Rejection Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240130,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: FINAL REJECTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"FINAL REJECTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240430,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240430,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240530,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FRM1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Final Rejection Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240530,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: FINAL REJECTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"FINAL REJECTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240904,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240904,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240930,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240930,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240930,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"ZAAB\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"NOTICE OF ALLOWANCE MAILED\",\n",
      "\n",
      "\"details\": \"Text: ORIGINAL CODE: MN/=.\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ORIGINAL CODE: MN/=.\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241227,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPR1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Received\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241227,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241230,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPV1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Verified\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241230,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PAT1\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"Patented Case\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210552492\",\n",
      "\n",
      "\"ad\": 20211216,\n",
      "\n",
      "\"pn\": \"US2022183701\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STCF\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT GRANT\",\n",
      "\n",
      "\"details\": \"Text: PATENTED CASE\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PATENTED CASE\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=91933436\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20230466010\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"SYSTEMS AND METHODS FOR SELF-ALIGNMENT AND ADJUSTMENT OF ROBOTIC ENDOSCOPE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE</p><p>[0001] This application is a continuation of International Patent Application No. PCT/US2023/023139, filed May 22, 2023, which claims priority to U.S. Provisional Patent Application No. 63/345,287, filed on May 24, 2022, and U.S. Provisional Patent Application No. 63/347,179, filed on May 31, 2022, each of which is entirely incorporated herein by reference.</p></relapp><shortsum><p>BACKGROUND OF THE INVENTION</p><p>[0002] Endoscopy procedures use an endoscope to examine the interior of a hollow organ or cavity of the body. Unlike many other medical imaging techniques, endoscopes are inserted into the organ directly. Flexible endoscope that can deliver instinctive steering and control is useful in diagnosing and treating diseases that are accessible through any natural orifice in the body. Depending on the clinical indication, the endoscope may be designated as bronchoscope, ureteroscope, colonoscope, gastroscope, ENT scope, and various others. For example, flexible bronchoscope may be used for lung cancer diagnosis and/or surgical treatment. However, one challenge in bronchoscopy is reaching the upper lobe of the lung while navigating through the airways. In another example, flexible endoscopy has been used to inspect and treat disorders of the gastrointestinal (GI) tract without the need for creating an opening on the patient's body. The endoscope is introduced via the mouth or anus into the upper or lower GI tracts respectively. A miniature camera at the distal end captures images of the GI wall that help the clinician in their diagnosis of the GI diseases. Simple surgical procedures (like polypectomy and biopsy) can be performed by introducing a flexible tool via a working channel to reach the site of interest at the distal end.</p><p>[0003] Endoscopes are traditionally made to be re-usable, which may require thorough cleaning, dis-infection, and/or sterilization after each procedure. In most cases, cleaning, dis-infection, and sterilization may be aggressive processes to kill germs and/or bacteria. Such procedures may also be harsh on the endoscopes themselves. Therefore, the designs of such re-usable endoscopes can often be complicated, especially to ensure that the endoscopes can survive such harsh cleaning, dis-infection, and sterilization protocols. Periodical maintenance and repairs for such re-usable endoscopes may often be needed.</p><p>[0004] Low cost, disposable medical devices designated for a single-use have become popular for instruments that are difficult to clean properly. Single-use, disposable devices may be packaged in sterile wrappers to avoid the risk of pathogenic cross-contamination of diseases such as HIV, hepatitis, and other pathogens. Hospitals generally welcome the convenience of single-use disposable products because they no longer have to be concerned with product age, overuse, breakage, malfunction, and sterilization. Traditional endoscopes often include a handle that operators use to maneuver the endoscope. For single-use endoscopes, the handle usually encloses the camera, expensive electronics, and mechanical structures at proximal end in order to transmit the video and allow the users to maneuver the endoscope via a user interface. This may lead to high cost of the handle for a single-use endoscope.</p><p>[0005] The process for setting up of medical robots can be time-consuming and challenging due to the number of accessories to set up and the complex workflows. The setup times can result in longer room turnover time, causing procedural delays. Examples of the challenging steps of the current system workflow are: (1) positioning the robotic system in a location that is appropriate for the procedure and (2) aligning the instrument drive mechanism to the patient side. These steps are challenging because the user has to consider the placement of other equipment in the room that is needed for the procedure and gain spatial understanding of the workspace of the robotic components while manually performing these steps. During the procedure, if there are any changes to the setup of the room, the user has to manually make adjustments to the system to accommodate.</p><p>SUMMARY OF THE INVENTION</p><p>[0006] Recognized herein is a need for a robotic endoscopic platform or system that allows for autonomous self-adjustment of the robotic endoscopic system in response to real-time operating environment. The present disclosure addresses the above need by providing methods and systems capable of detecting and tracking the system's operating environment (e.g., external environment surrounding the system) and automatically making adjustments to the system thereby (1) simplifying the workflow for an operator during system setup and/or during the procedure and (2) enabling system configurations that are more optimal than placements completed manually which may be deemed acceptable but non-optimal. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base station (e.g., robotic cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, etc.</p><p>[0007] In an aspect of the present disclosure, a method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; autonomously actuating arm for collision avoidance; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed. The methods implemented for enabling autonomous repositioning of system components given 3D depth map inputs require algorithms for processing and filtering the image data, mapping image data to inputs for the robotics control algorithms, and algorithms for obstacle voidance.</p><p>[0008] Recognized also herein are devices and systems comprising endoscopes which may be disposable and may not require extensive cleaning procedures. The present disclosure provides low-cost, single-use articulatable endoscope for diagnosis and treatment in various applications such as bronchoscopy, urology, gynecology, arthroscopy, orthopedics, ENT, gastro-intestine endoscopy, neurosurgery, and various others. In some embodiments, the present disclosure provides a single-use, disposable, robotically controlled bronchoscope for use with a robotic system to enable diagnostic evaluation of lesions anywhere in the pulmonary anatomy. It should be noted that the provided endoscope systems can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0009] It should be noted that the provided autonomous configuration, alignment and collision avoidance methods, endoscope components and various components of the device can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0010] In an aspect, a method is provided for controlling a robotic endoscope system moving and operating in an environment. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0011] In a related yet separate aspect, a system is provided for controlling a robotic endoscope system The system comprises: a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to: generate a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuate a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuate the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0012] In some embodiments, the 3D depth map is generated based at least in part on 3D point cloud data. in some cases, the method further comprises processing the 3D depth map to detect the patient bed and computing a position and orientation of the robotic support system relative to the patient bed. In some embodiments, a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p><p>[0013] In some embodiments, the method further comprises controlling a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed. In some cases, the method further comprises loading a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end. In some cases, the method further comprises automatically adjusting a position of the IDM relative to the component upon detection of a buckling event.</p><p>[0014] In some embodiments, the method further comprises detecting and recognizing an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM. In some cases, the method further comprises detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient. In some instances, the method further comprises executing a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p><p>[0015] In some embodiments, the IDM is autonomously aligned to the component based at least in part on sensor data. In some cases, the sensor data is captured by electromagnetic sensors. In some cases, the sensor data is captured by a camera including a fiducial marker placed on the component and the 3D depth map comprises at least a 3D location of the fiducial marker.</p><p>[0016] Additional aspects and advantages of the present disclosure will become readily apparent to those skilled in this art from the following detailed description, wherein only illustrative embodiments of the present disclosure are shown and described. As will be realized, the present disclosure is capable of other and different embodiments, and its several details are capable of modifications in various obvious respects, all without departing from the disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.</p><p>INCORPORATION BY REFERENCE</p><p>[0017] All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. To the extent publications and patents or patent applications incorporated by reference contradict the disclosure contained in the specification, the specification is intended to supersede and/or take precedence over any such contradictory material.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0018] The novel features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that sets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings (also \\\"Figure\\\" and \\\"FIG.\\\" herein), of which:</p><p>[0019] FIG. <b>1</b> schematically shows a robotic platform, in accordance with some embodiments of the present disclosure.</p><p>[0020] FIG. <b>2</b> shows an example of a robotic catheter assembly with an anti-buckling device.</p><p>[0021] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation.</p><p>[0022] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment using optical sensor;</p><p>[0023] FIGS. <b>4</b>B and <b>4</b>C shows an example of auto-alignment of IDM (instrument drive mechanism) based on EM sensor data.</p><p>[0024] FIG. <b>5</b> and FIG. <b>6</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to a patient side mount.</p><p>[0025] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed.</p><p>[0026] FIG. <b>8</b> shows an example of self-alignment of a robotic endoscope system.</p><p>[0027] FIG. <b>9</b> shows an example process for autonomous alignment of the robotic endoscope system.</p><p>[0028] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance of the robotic endoscope system with respect to one or more objects in the operating environment before and during a surgical operation.</p><p>[0029] FIG. <b>11</b> shows an example of a robotic bronchoscope comprising a handle portion and a flexible elongate member.</p><p>[0030] FIG. <b>12</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of the robotic bronchoscope.</p><p>[0031] FIG. <b>13</b> shows an example of a collision avoidance algorithm.</p><p>[0032] FIG. <b>14</b> and FIG. <b>15</b> show examples of a flexible endoscope.</p><p>[0033] FIG. <b>16</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of a robotic bronchoscope.</p><p>[0034] FIG. <b>17</b> shows an example of a distal tip of an endoscope.</p><p>[0035] FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device.</p></shortdesdrw><p>DETAILED DESCRIPTION OF THE INVENTION</p><p>[0036] While various embodiments of the invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions may occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed.</p><p>[0037] The embodiments disclosed herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. The disclosed embodiments can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0038] While exemplary embodiments will be primarily directed at a device or system for bronchoscopy, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in various anatomical regions of a patient's body. The provided device or system can be utilized in urology, gynecology, rhinology, otology, laryngoscopy, gastroenterology with the endoscopes, combined devices including endoscope and instruments, endoscopes with localization functions, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in other anatomical regions of a patient's body, such as such as brain, heart, lungs, intestines, eyes, skin, kidney, liver, pancreas, stomach, uterus, ovaries, testicles, bladder, ear, nose, mouth, soft tissues such as bone marrow, adipose tissue, muscle, glandular and mucosal tissue, spinal and nerve tissue, cartilage, hard biological tissues such as teeth, bone and the like, as well as body lumens and passages such as the sinuses, ureter, colon, esophagus, lung passages, blood vessels and throat, and various others, in the forms of: NeuroendoScope, EncephaloScope, OphthalmoScope, OtoScope, RhinoScope, LaryngoScope, GastroScope, EsophagoScope, BronchoScope, ThoracoScope, PleuroScope, AngioScope, MediastinoScope, NephroScope, GastroScope, DuodenoScope, CholeodoScope, CholangioScope, LaparoScope, AmioScope, UreteroScope, HysteroScope, CystoScope, ProctoScope, ColonoScope, ArthroScope, SialendoScope, Orthopedic Endoscopes, and others, in combination with various tools or instruments.</p><p>[0039] The systems and apparatuses herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. Systems and apparatuses provided herein can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0040] Whenever the term \\\"at least,\\\" \\\"greater than,\\\" or \\\"greater than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"at least,\\\" \\\"greater than\\\" or \\\"greater than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, greater than or equal to 1, 2, or 3 is equivalent to greater than or equal to 1, greater than or equal to 2, or greater than or equal to 3.</p><p>[0041] Whenever the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, less than or equal to 3, 2, or 1 is equivalent to less than or equal to 3, less than or equal to 2, or less than or equal to 1.</p><p>[0042] As used herein, the terms distal and proximal may generally refer to locations referenced from the apparatus, and can be opposite of anatomical references. For example, a distal location of a primary shaft or catheter may correspond to a proximal location of an elongate member of the patient, and a proximal location of the primary sheath or catheter may correspond to a distal location of the elongate member of the patient.</p><p>[0043] As described above, setting up a robotic endoscopic system can be time consuming and challenging due to the complexity of the operating environment, requirement for accurate alignment between the instrument and patient body part and various other reasons. The present disclosure provides methods and systems capable of detecting and tracking the system's operating environment and automatically making adjustments to the system thereby simplifying the workflow during system setup and/or during the procedure. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base (robot cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, and other functions as described elsewhere herein.</p><p>[0044] The operating environment of the robotic endoscope system may comprise one or more objects. The robotic endoscope system may be capable of detecting the one or more objects in the operating environment, generating a 3D map with depth information, performing autonomous alignment of the instrument drive mechanism with respect to the patient bed or body part, and self-adjusting its placement and configuration to avoid collision with the one or more objects. The one or more objects may comprise, for example, system accessories (e.g., system monitor, a monitor pole), external monitors, patient bed, patient, imaging equipment (e.g., fluoroscopy c-arm), anesthesia cart and other equipment or subject (e.g., operator, surgeon) in the operating environment before or during surgical operation.</p><p>[0045] FIG. <b>1</b> schematically shows a robotic platform <b>100</b>. The platform may comprise a robotic endoscope system including one or more flexible articulatable surgical instruments <b>105</b>, and a support apparatus <b>110</b> such as a robotic manipulator (e.g., robotic arm) to drive, support, position or control the movements and/or operation of the robotic system. The robotic platform may further include peripheral devices and subsystems such as imaging systems that may assist and/or facilitate the navigation of the elongate member to the target site in the body of a subject <b>120</b>.</p><p>[0046] The robotic endoscope system is provided for performing surgical operations or diagnosis with improved performance at low cost. For example, the robotic endoscope system may comprise a steerable catheter that can be entirely disposable. As shown in FIG. <b>1</b>, the robotic endoscope system may comprise a steerable catheter assembly <b>105</b> and a robotic support system <b>110</b>, for supporting or carrying the steerable catheter assembly. The steerable catheter assembly can be an endoscope. In some embodiments, the steerable catheter assembly may be a single-use robotic endoscope. In some embodiments, the robotic endoscope system may comprise an instrument driving mechanism (IDM) <b>103</b> that is attached to the distal end of the robotic arm <b>107</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>105</b>. The mechanical interface may allow the steerable catheter assembly <b>105</b> to be releasably coupled to the instrument driving mechanism <b>103</b>. For instance, a handle portion <b>104</b> of the steerable catheter assembly can be attached to the instrument driving mechanism (IDM) via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool. The instrument driving mechanism may be used to control the elongate member or robotic catheter assembly in two or more degrees of freedom (e.g., articulation).</p><p>[0047] The robotic support system <b>110</b> may comprise a robotic arm <b>107</b> and a mobile base (e.g., robotic cart) <b>109</b>. The robotic arm <b>107</b> may initiate the positioning of the robotic catheter assembly or other robotic instrument. In some cases, a user interface, robotic control modules, and the robotic arm may be mounted to the mobile cart. The mobile cart may include various elements such as rechargeable power supply in electrical communication with an electric panel providing charging ports for portable electronic devices, converters, transformers and surge protectors for a plurality of AC and DC receptacles as power source for the on-board equipment including one or more computers storing application specific software for the user interface.</p><p>[0048] The robotic arm <b>107</b> may have redundant degrees of freedom allowing for its elbow to be algorithmically, or passively, moved into configurations that are convenient for an operator initiate the positioning of the robotic system or other robotic instrument. For example, the robotic arm may comprise a plurality of joints having redundant degrees of freedom such that the joints of the robotic arm can be driven through a range of differing configurations for a given end effector position (e.g., IDM position). The redundant degrees of freedom may beneficially allow the robotic arm to be self-adjusted to an optimal configuration to avoid collision with other object in the operating environment prior to or during a procedure. For example, the instrument drive mechanism may automatically align itself to a patient side mount (e.g., a support structure at the patient bed for holding and supporting the endoscope device in place) during setup procedure. During the setup procedure and the operation procedure, upon detection of motion of the patient side mount, the instrument drive mechanism (IDM) is able to auto-adjust accordingly to avoid collision while maintain the position of the IDM, eliminating any interruptions to the procedural workflow and avoiding misalignment. In another example, when the robotic arm is actuated during the setup procedure or surgical operation, the system may detect unwanted proximity between any portion of the robotic arm and other objects surrounding it (e.g., the monitor of the system) and the robotic arm may be automatically reconfigured, moved away from the monitor to avoid collision.</p><p>[0049] In some embodiments, in addition to the autonomous movement of the robotic arm such as automatically positioning the steerable catheter assembly <b>105</b> to an initial position (e.g., access point) to access the target tissue, the robot arm may be passively moved by an operator. In such case, an operator can push the arm in any position and the arm compliantly moves. The robotic arm can also be controlled in a compliant mode to improve human robot interaction. For example, the compliant motion control of the robot art may employ a collision avoidance strategy and the position-force control may be designed to save unnecessary energy consumption while reducing impact of possible collisions.</p><p>[0050] The steerable catheter assembly <b>105</b> may comprise a flexible elongate member that is coupled to a handle portion. The robotic endoscope system may comprise an anti-buckling device <b>101</b> for preventing the buckling of the elongate member during use.</p><p>[0051] FIG. <b>2</b> shows another example of a robotic catheter assembly with an anti-buckling device <b>201</b>. The steerable catheter assembly may comprise a handle portion <b>211</b> that may include components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>211</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly and the instrument driving mechanism <b>221</b>, and any other external system or devices. In another example, the handle portion <b>211</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>221</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>221</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0052] The steerable catheter assembly may comprise a flexible elongate member <b>213</b> (i.e., catheter) that is coupled to the handle portion <b>211</b>. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic endoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0053] The robotic endoscope can be releasably coupled to an instrument driving mechanism <b>221</b>. The instrument driving mechanism <b>221</b> may be mounted to the arm of the robotic support system or to any actuated support system as described above. The instrument driving mechanism may provide mechanical and electrical interface to the robotic endoscope. The mechanical interface may allow the robotic endoscope to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic endoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic endoscope may be coupled or released from the instrument driving mechanism manually without using a tool. In some embodiments, the instrument driving mechanism <b>221</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion <b>211</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the catheter.</p><p>[0054] The handle portion may be designed allowing the robotic endoscope to be disposable at reduced cost. For instance, classic manual and robotic endoscope may have a cable in the proximal end of the endoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as EM sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the endoscope. The provided robotic endoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic endoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0055] In some case, the handle portion may be housing or comprise components configured to process image data, provide power, or establish communication with other external devices. In some cases, the communication may be wireless communication. For example, the wireless communications may include Wi-Fi, radio communications, Bluetooth, IR communications, or other types of direct communications. Such wireless communication capability may allow the robotic bronchoscope function in a plug-and-play fashion and can be conveniently disposed after single use. In some cases, the handle portion may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED light source) disposed within the robotic bronchoscope or catheter.</p><p>[0056] The handle portion may be designed in conjunction with the catheter such that cables or fibers can be eliminated. For instance, the catheter portion may employ a design having a single working channel allowing instruments to pass through the robotic bronchoscope, as well as low cost electronics such as a chip-on-tip camera, illumination sources such as light emitting diode (LED) and EM sensors located at optimal locations in accordance with the mechanical structure of the catheter. This may allow for a simplified design of the handle portion. For instance, by using LEDs for illumination, the termination at the handle portion can be based on electrical soldering or wire crimping alone. For example, the handle portion may include a proximal board where the camera cable, LED cable, and EM sensor cable terminate to while the proximal board connects to the interface of the handle portion and establishes the electrical connections to the instrument driving mechanism. As described above, the instrument driving mechanism is attached to the robot arm (robotic support system) and provide a mechanical and electrical interface to the handle portion. This may advantageously improve the assembly and implementation efficiency as well as simplify the manufacturing process and cost. In some cases, the handle portion along with the catheter may be disposed of after a single use.</p><p>[0057] In some embodiments, the steerable catheter assembly may have a substantially integral design that one or more components may be integral to the catheter thereby simplifying the assembly, manufacturing process while preserving the kinematic, dynamic performance of the steerable catheter. As shown in the example, the steerable catheter assembly may comprise an elongate member <b>213</b> or a probing portion that is brought into proximity to the tissue and/or area that is to be examined. The elongate member <b>213</b> may, in some cases, also be referred to as catheter. The catheter <b>213</b> may comprise internal structures such as a working channel allowing tools to be inserted through. As an example, the working channel may have a dimension such as diameter of around 2 mm to be compatible with standard tools. The working channel may have any other suitable dimensions based on the application.</p><p>[0058] The catheter <b>213</b> may be composed of suitable materials for desired flexibility or bending stiffness. In some cases, the materials of the catheter may be selected such that it may maintain structural support to the internal structures (e.g., working channel) as well as being substantially flexible (e.g., able to bend in various directions and orientations). For example, the catheter can be made of any suitable material such as urethane, vinyl (such as polyvinyl chloride), Nylon (such as vestamid, grillamid), pellethane, polyethylene, polypropylene, polycarbonate, polyester, silicon elastomer, acetate and so forth. In some cases, the materials may be polymer material, bio-compatible polymer material and the catheter may be sufficiently flexible to be advancing through a path with a small curvature without causing pain to a subject. In some cases, the catheter may comprise a sheath. The sheath may not be the same length of the catheter. The sheath may be shorter than the catheter to provide desired support. Alternatively, the catheter may be substantially a single-piece component.</p><p>[0059] In some case, the distal portion or tip of the catheter may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple segments having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments, adding additional supporting components or any combination of the above. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0060] The distal portion of the catheter may be steered by one or more pull wires. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such it can be bent by the pull wires. In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0061] As described above, the pull wires may be made of any suitable material such as stainless steel (e.g. SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0062] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter or the interstitials of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0063] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise A/D converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0064] The illumination device may comprise one or more light sources positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0065] In some embodiments, the catheter may be designed to be flexible. When the flexible portions of catheter are inserted by extending mechanisms through endoscope into patients, one or more sections may bend or buckle.</p><p>[0066] The anti-buckling mechanism <b>201</b> may be coupled to the handle portion of the robotic endoscope to support the catheter. The anti-buckling mechanism is used for preventing buckling of the insertion shaft. The anti-buckling mechanism <b>201</b> may be a telescopic extending device with internal mechanism to achieve anti-buckling of catheter during the insertion and withdrawal. The anti-buckling mechanism may be detachably connected to the handle portion of the robotic bronchoscope at one end, and may be detachably connected to a support surface <b>203</b> at the other end. As shown in the example, the anti-buckling tube may be attached to a bracket on the instrument driving mechanism and may be removable and disposable after the procedure via quick release mechanism. In the examples illustrated in FIG. <b>2</b>, a support arm (e.g., ET tube mount support arm) may be supported by the robotic mobile cart that supports the endotracheal tube mount and provides a support surface for the distal end of the anti-buckling tube to press against as it is compressed. The support arm may be controlled to rotate, translate vertically up and down and/or may a boom arm that expands and contracts, such that it can be precisely positioned over the patients mouth and attached to the endotracheal tube mount. The support arm positioning may be synchronized with the movement of the robotic arm that it may track the location of the point of entry of the catheter.</p><p>[0067] The anti-buckling mechanism may require a relatively linear trajectory to be traveled. In some cases, such trajectory may be ensured via an alignment between the anti-buckling mechanism in a collapsed state and a patient-side connector. FIG. <b>1</b> shows an example of a patient side connector <b>121</b> and IDM <b>103</b>. For example, the patient-side connector may be fixed to a patient side mount <b>123</b> (e.g., attached to the patient bed). The alignment between the IDM and the patient side connector/mount may involve lining up a collapsed anti-buckling mechanism with the patient-side connector. The robotic arm may automatically move the IDM into a position such that the IDM is aligned to the patient-side connector. In some cases, the alignment may comprise generating a 3D depth map of the operating environment, moving the mobile cart into a desired position relative to the patient bed based on the depth map, and moving the IDM into a position and orientation in alignment with the patient-side connector based on a detection of the location/position of the patient-side connector.</p><p>[0068] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation <b>300</b>, <b>330</b>. The robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise a steerable catheter assembly <b>320</b> and a robotic support system <b>310</b>, for supporting or carrying the steerable catheter assembly. In some cases, the steerable catheter assembly may be a bronchoscope. The steerable catheter assembly can be the same as the endoscope device as described elsewhere herein. In some cases, the steerable catheter assembly may be a single-use robotic bronchoscope. In some embodiments, the robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise an instrument driving mechanism (IDM) <b>313</b> that is attached to the end of the robotic arm <b>311</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>320</b>. The mechanical interface may allow the steerable catheter assembly <b>320</b> to be releasably coupled to the instrument driving mechanism. For instance, a handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0069] The steerable catheter assembly <b>320</b> may comprise a handle portion <b>323</b> that may include components configured to processing image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>323</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>320</b> and the instrument driving mechanism <b>313</b>, and any other external system or devices. In another example, the handle portion <b>323</b> may comprise circuitry elements such as power sources for powering the electronics (e.g. camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>313</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>313</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0070] The steerable catheter assembly <b>320</b> may comprise a flexible elongate member <b>321</b> that is coupled to the handle portion. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic bronchoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly <b>320</b> including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0071] In some embodiments, the provided bronchoscope system may also comprise accessories such as a user interface. As illustrated in the example system <b>330</b>, during operation, one or more components of the system such as a treatment interface module <b>331</b> (user console side) and/or a treatment control module <b>333</b> (patient and robot side) may be brought into the operating environment. The one or more components or accessories may be added or removed from the operating environment before or during a surgical operation. The robotic arm <b>311</b> may have redundant degrees of freedom such that the joints of the robotic arm can be driven into a range of differing configurations for a given end effector position. For example, upon detection of a treatment interface module <b>311</b>, the robotic arm <b>311</b> may automatically move into a different configuration to avoid collision with the treatment interface module while the distal end of the arm of the IDM <b>313</b> maintains a particular state (e.g., a given position or velocity of the end effector). Details about the collision avoidance is described later herein.</p><p>[0072] The treatment interface module may allow an operator or user to interact with the bronchoscope during surgical procedures. In some embodiments, the treatment control module <b>333</b> may be a hand-held controller. The treatment control module <b>333</b> may allow a user to control a velocity of the tip of the bronchoscope as described elsewhere herein. The treatment control module may, in some cases, comprise a proprietary user input device and one or more add-on elements removably coupled to an existing user device to improve user input experience. For instance, physical trackball or roller can replace or supplement the function of at least one of the virtual graphical element (e.g., navigational arrow displayed on touchpad) displayed on a graphical user interface (GUI) by giving it similar functionality to the graphical element which it replaces. Examples of user devices may include, but are not limited to, mobile devices, smartphones/cellphones, tablets, personal digital assistants (PDAs), laptop or notebook computers, desktop computers, media content players, and the like. Details about the user interface device and user console are described later herein.</p><p>[0073] The robotic endoscope platform herein may be able to detect one or more objects in the operating environment surrounding the robotic endoscope system. In the cases, the detection of the operating environment may comprise generating an obstacle map. An obstacle map may be a three-dimensional (3D) map describing positions of objects detected in the 3D space.</p><p>[0074] The 3D map of the operating environment may be constructed based on sensing data. In some cases, the sensing data is received from one or more vision sensors, including depth information for the environment. The vision sensor may comprise a camera, a video camera, a three-dimensional (3D) depth camera, a stereo camera, a depth camera, a Red Green Blue Depth (RGB-D) camera, a time-of-flight (TOF) camera, an infrared camera, a charge coupled device (CCD) image sensor, or a complementary metal oxide semiconductor (CMOS) image sensor. For example, the vision sensor can include only one camera (monocular vision sensor). Alternatively, the vision sensor can include two (binocular vision sensor) or more cameras. The vision sensors may be disposed on the robotic endoscope system such as the robotic cart, the monitor and the like. Alternatively or additionally, the vision sensors may not be disposed on the robotic endoscope system. For instance, the vision sensors may be disposed on the walls, ceilings or other places in the operating environment (e.g., room). In embodiments where multiple vision sensors are used, each sensor can be located on a different portion of the robotic endoscope system, and the disparity between the image data collected by each sensor can be used to provide depth information for the environment. Depth information can be used herein to refer to information regarding distances of one or more objects from the robotic endoscope system and/or sensor. In embodiments where a single vision sensor is used, depth information can be obtained by capturing image data for a plurality of different positions and orientations of the vision sensor, and then using suitable image analysis techniques (e.g., structure from motion) to reconstruct the depth information.</p><p>[0075] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment <b>400</b> using optical sensor <b>401</b> such as a camera. As mentioned elsewhere herein, an operating environment of the system may generally comprise an environment external to the system (e.g., the room the system is in, within certain proximity, etc.) or the environment that the system can move or operate within. In some cases, the camera may be a plenoptic camera having a main lens and additional micro lens array (MLA). The plenoptic camera model may be used to calculate a depth map of the captured image data. In some cases, the image data captured by the camera may be grayscale image with depth information at each pixel coordinate (i.e., depth map). The camera may be calibrated such that intrinsic camera parameters such as focal length, focus distance, distance between the MLA and image sensor, pixel size and the like are obtained for improving the depth measurement accuracy. Other parameters such as distortion coefficients may also be calibrated to rectify the image for metric depth measurement.</p><p>[0076] In some cases, the image data may be received and processed by one or more processors of the robotic endoscope system. For example, pre-processing of the capture image data may be performed. In an embodiment, the pre-processing algorithm can include image processing algorithms, such as image smoothing, to mitigate the effect of sensor noise, or image histogram equalization to enhance the pixel intensity values. Next, optical approaches as described elsewhere herein may be employed to generate a depth map of the operating environment <b>400</b>. In some cases, computer vision (CV) techniques or computer vision systems may be used to process the sensing data to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. For example, the CV output data may be generated using passive methods that only require images. Passive methods may include, for example, object recognition, stereoscopy, monocular shape-from-motion, shape-from-shading, and Simultaneous Localization and Mapping (SLAM). Alternatively, active methods may be utilized which may require controlled light to be projected into the target scene and the active methods may include, for example structured light and Time-of-Flight (ToF). In some cases, computer vision techniques such as optical flow, computational stereo approaches, iterative method combined with predictive models, machine learning approaches, predictive filtering or any non-rigid registration methods may be used to generate the descriptions of the 3D scene.</p><p>[0077] In some cases, a fiducial marker <b>403</b> may be employed to align the IDM to a patient side mount. The fiducial marker may be placed on the patient bed or the patient side mount <b>405</b>. The fiducial marker may have a 2D shape or pattern. For example, the fiducial marker may be a 2D QR code, grids, or any asymmetric shape. By acquiring images of the 2D fiducial marker (e.g., from different angles), the location and orientation of the patient side mount that the fiducial marker is placed at in a camera frame can be determined (e.g., triangulation). Based on the known spatial relationship between the camera and the IDM, the orientation and location of the IDM with respect to the patient side mount can be calculated. Alternatively, the fiducial marker may be a 3D fiducial marker such that the marker is visible and discernable in a wide range of angels. For example, a 3D fiducial marker may be located within the view of the imaging system such that the fiducial marker is always discernable regardless the position of the optical sensor with respect to the patient bed or the marker. The fiducial marker(s) may have any suitable 2D/3D shape (non-isotropic) or pattern such that a projection of the fiducial mark(s) corresponding to a view/angle is discernable from that of another view/angle. Alternatively, the alignment of the IDM to the patient side mount may not require the fiducial marker. For instance, the patient side mount may be recognized using segmentation, and/or object recognition method as described elsewhere herein without the fiducial marker. In some cases, a 3D fiducial marker may be utilized to align the IDM to the patient side mount independent of using 3D point cloud. For example, sequence of image frames or video may be acquired containing the fiducial marker and may be processed to identify a spatial relationship (e.g., orientation, position) between the imaging device and patient side mount (i.e., fiducial marker). Based on a known geometric relationship between the IDM and the imaging device, the spatial relationship between the IDM and the patient side mount may be derived. Alternatively, the fiducial marker may be used in conjunction with a 3D depth map. For example, the object identity (e.g., patient side mount) may be identified by the fiducial marker and the depth data may be assigned to the object based on the 3D depth map. In some cases, the 3D depth map may include a 3D point cloud. Alternatively, the 3D depth map may be generated based on the optical image data. the 3D depth map may comprise at least an object (e.g., fiducial marker) with depth information obtained using optical method as described above.</p><p>[0078] In some cases, the imaging device may be used in conjunction with other types of sensors (e.g., proximity sensor, location sensor, positional sensor, etc.) to improve accuracy of the location information. For instance, the sensing data may further comprise sensor data from one or more proximity sensors. The proximity sensors can be any suitable type such as ultrasonic sensor (e.g., a wide angle sensor, an array sensor) or light detection and ranging (Lidar) sensor. Lidar can be used to obtain three-dimensional information of an environment by measuring distances to objects. The proximity sensors can also be disposed at the robotic endoscope system. The proximity sensors can be located near the vision sensors. Alternatively, the proximity sensors can be situated on a portion of the robotic endoscope system different from the portions used to carry the vision sensors.</p><p>[0079] In some cases, the 3D depth map may be generated using a single modality sensor data (e.g., image data, Lidar, proximity data, etc.). Alternatively, the 3D depth map may be generated using multi-modality data. For example, the image data and 3D point cloud generated by the Lidar system may be fused using Kalman filter or deep learning model to generate a 3D map. The 3D map may then be used for automatic alignment of the IDM, self-positioning of the robotic car, collision avoidance and various other functions as described elsewhere herein.</p><p>[0080] In some embodiments, the autonomous alignment of the IDM to a patient side mount may be based on positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the IDM and electromagnetic coils located on the patient side mount along with an electromagnetic tracking system. The position and orientation of the IDM and the patient side mount may be detected, and the difference may be used to generate a command to move robotic arm thereby achieving an autonomous alignment between the IDM and patient side mount.</p><p>[0081] As shown in FIG. <b>4</b>B, the system may comprise an EM field generator <b>415</b> that transmits an EM field in the environment. The EM field generator may be positioned next to the patient torso during procedure, such as on the bed <b>407</b>, on the robotic cart, or any other suitable place in the environment <b>410</b>. The system may comprise a first EM sensor <b>411</b> located at the IDM to measure position and orientation of the IDM, and a second EM sensor <b>413</b> located at the patient side mount to measure position and orientation of the patient side mount. FIG. <b>4</b>C shows an example of the patient side mount <b>431</b> and the associated EM sensors <b>430</b>. Referring back to FIG. <b>4</b>B, the EM field generator and the two sets of EM sensors <b>411</b>, <b>413</b> may be utilized by the system to locate the position and orientation of the IDM and the patient side mount in 3D space.</p><p>[0082] The position and orientation measured by the two sets of EM sensors may be expressed in the filed-generator frame. The EM sensor may be 6 DOF sensor (e.g., X, Y, Z, Roll, Pitch, Yaw) that is able to sense the EM signals generated by the EM field generator and measures the six degrees of freedom spatial data for the IDM and the patient-side-mount. Alternatively, a pair a 5 DOF EM sensors may be located at the IDM and/or the patient side mount to measure the position and orientation. For example, a single 5DOF sensor may generate signals in X, Y, Z, Pitch, and Yaw, without Roll. By placing 2 5DOF sensors inside a device such that they are fixed relative to each other and their center axes are not in parallel with each other, the pair of 5 DOF signals can be used to calculate roll information.</p><p>[0083] The spatial data about the IDM and the patient side mount may then be processed by the system to determine if the cart is properly positioned for auto-alignment and move the IDM to align with the patient-side-mount such as using a closed-loop-feedback (e.g., controlling robot arm movements to correct the IDM position/orientation based on EM sensor data) and/or other control method as described elsewhere herein.</p><p>[0084] FIG. <b>5</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to the patient side mount. In the illustrated example <b>510</b>, the instrument drive mechanism may be initially positioned not in alignment with the patient side mount. During setup procedure, actuators of the plurality of links/joints of the robotic arm may be actuated and automatically align the IDM to the patient side mount <b>520</b>. In some cases, after alignment, the IDM may be moved to a proper position at a pre-determined distance <b>610</b> from the patient side mount <b>600</b> for loading an instrument such as the catheter assembly or endoscope device, as shown in FIG. <b>6</b>. For example, a flexible endoscope apparatus may be coupled to the IDM at the proximal end of the endoscope and coupled to the patient side mount via the connector at the distal end. In some cases, the pre-determined distance <b>610</b> may be generated based on a dimension of the endoscope device or empirical data. A user may be permitted to further manually adjust the position of the IDM as needed by switching the robotic arm into a passive mode. For example, once the passive model is enabled, the robotic arm can be placed into any configuration, position or orientation by a user applying force directly and will maintain the desired position and orientation.</p><p>[0085] As described above, the robotic arm may have redundant degrees of freedom. For instance, the robotic arm may have six, seven, eight, nine or more degrees of freedom (DOF) such that the IDM is able to be oriented in five or six degree of freedom (DOF) space. For example, the robotic arm end effector (e.g., IDM) that can be positioned with six degrees of freedom may in some cases have nine degrees of freedom (six end effector degrees of freedom-three for location, and three for orientation-plus three degrees of freedom to comply with the access site constraints), or ten or more degrees of freedom. Highly configurable robotic arm having more degrees of freedom than are needed for a given end effector position can beneficially sufficient degrees of freedom to allow a range of joint states for an end effector position in a workspace. During the procedure, if there is any motion of the patient side mount, the instrument drive mechanism may be able to auto-adjust accordingly to retain alignment with the patient side mount, eliminating any interruptions to the procedural workflow and avoiding misalignment.</p><p>[0086] In some embodiments, the 3D depth map generated by the platform may be used for automatic mobile/robotic cart placement. For example, the 3D depth map may comprise description about the operating environment such as identification of equipment, patient bed, human operator, patient and the like and such 3D depth map can be used to generate an optimal location of the mobile cart relative to the patient bed. As described above, computer vision (CV) techniques or computer vision systems may be used to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. The 3D map information and sensor data (e.g., proximity sensor, imaging sensor) may be used to detect whether the robotic cart is within an optimal zone with respect to the patient bed.</p><p>[0087] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed. In some embodiments, a propulsion unit may include a plurality of wheels that may permit the robotic cart to roll over an underlying surface. In some examples, two, three or four wheels may be provided which may permit the robotic cart to stand stably while not moving. In some instances, stabilization may occur with aid of one or more wheels or other stabilization platforms, such as gyroscopic platforms. The wheels may vary in size or be the same size. In some cases, the wheels can have a diameter of at least about 1 cm, 2 cm, 3 cm, 4 cm, 5 cm, 8 cm, 9 cm, 10 cm, 15 cm, 20 cm, 25 cm, 30 cm, 35 cm, 40 cm, 45 cm, 50 cm, 55 cm, 60 cm, 65 cm, 70 cm, 75 cm, 80 cm, 85 cm, 90 cm, 95 cm, 100 cm, 150 cm, or 200 cm. The wheels can have a smooth or treaded surface. The wheels may also permit the robotic cart to move laterally and/or rotate in place. The robotic cart may be capable of making any combination of translational or rotational movement. The propulsion unit may be driven with aid of one or more actuators. For example, a motor, engine, drive train, or any other component may be provided that may aid in driving the propulsion of the robotic cart.</p><p>[0088] Based on the 3D depth map, an optimal location of the robotic cart may be generated. The optimal location may be generated based on a dimension of the robotic cart, the dimension of the robotic arm (workspace), the dimension of the endoscope device and the 3D depth map. Real-time sensor data (e.g., proximity sensor) may be collected and may be used to determine whether the robotic cart is in the proper location relative to the patient bed. As shown in FIG. <b>8</b>, when the self-propelled robotic cart is detected not in a proper location with respect to the patient bed <b>810</b>, the robotic cart may automatically move to the proper location <b>820</b>. The proper location, the movement speed, moving acceleration, and the movement trajectory may be calculated by one or more processors of the platform based at least in part on the 3D depth map.</p><p>[0089] Alternatively or additionally, the system may inform the user of an non-optimal placement of the robotic cart and may prompt the user to intervene, For instance, message, warning or notification may be displayed on the screen along with recommendations for placing the robotic cart (e.g., specify that the cart should be closer to the bed) and/or displaying a 2D/3D depth map of the operating environment and the robotic system. For example, the robotic system may generate a preferred relative cart position and orientation, respective to the current cart position and display an animation to a user to guide the user changing the position of the robotic cart.</p><p>[0090] FIG. <b>9</b> shows an example process <b>900</b> for autonomous alignment of the robotic endoscope system. It should be noted that in the illustrated process though Lidar data (e.g., 3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. In the exemplary process, runtime sensor data (e.g., 3D point cloud) may be captured as input <b>901</b>. The runtime sensor data may include data captured by a Lidar (light detection and ranging). The Lidar may obtain three-dimensional information of the operating environment/scene by measuring distances to objects. For example, the emitting apparatus of a Lidar system may generate a sequence of light pulses emitted within short time durations such that the sequence of light pulses may be used to derive a distance measurement point. The Lidar system may provide three-dimensional (3D) imaging (e.g., 3D point cloud). In some cases, the 3D point cloud or the 3D images may be further processed by one or more processors of the robotic endoscope system for obstacles detection or collision avoidance <b>903</b>. Various suitable image processing method (e.g., image segmentation) may be utilized to recognize an object such as the patient bed. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the patient bed).</p><p>[0091] The positions and orientations of the Lidar sensor may be obtained based on kinematic mapping and such information along with the depth information of the bed is used to estimate relative orientation and position between the bed and the robotic endoscope system. Based on the relative position and orientation, a movement path for moving the robotic cart to an optimal placement with respect to the patient bed may be generated. The optimal placement may be generated by the system automatically without user intervention. In some cases, instead of or in addition to performing the robotic cart self-placement, the algorithm may inform a user an invalid location of the robotic cart relative to the patient bed and a property location of the robotic cart may be displayed to the user on a GUI. The user may be able to provide input via a GUI indicating a selection of the property placement of the robotic cart. Upon receiving the user confirmation, the system may generate the movement path for the robotic cart. Alternatively, a user may be guided to manually move the robotic cart to a desired place as described above.</p><p>[0092] As described above, any suitable method may be employed to process the real-time sensor data (e.g., 3D point cloud) for segmentation, object recognition and collision avoidance. One or more objects may be segmented in the workspace of the robotic arm. For example, deep learning techniques such as an automated pipeline engine may be provided for processing the lidar data. The pipeline engine may comprise multiple components or layers. The pipeline engine may be configured to preprocess continuous streams of raw Lidar data or batch data transmitted from a Lidar system. In some cases, data may be processed so it can be fed into machine learning analyses. In some cases, data may be processed to provide details at different understanding levels, which understanding may include, by way of non-limiting example, dimensions, weight, composition, identity, degree of collision risk, mobility, and so forth. In some case, the pipeline engine may comprise multiple components to perform different functions for extracting different levels of information from the 3D point cloud data. In some cases, the pipeline engine may further include basic data processing such as, data normalization, labeling data with metadata, tagging, data alignment, data segmentation, and various others. In some cases, the processing methodology is programmable through APIs by the developers constructing the pipeline.</p><p>[0093] In some embodiments, the pipeline engine may utilize machine learning techniques for processing data. In some embodiments, raw Lidar data may be supplied to a first layer of the pipeline engine which may employ a deep learning architecture to extract primitives, such as edges, corners, surfaces, of one or more target objects. In some cases, the deep learning architecture may be a convolutional neuron network (CNN). CNN systems commonly are composed of layers of different types: convolution, pooling, upscaling, and fully-connected neuron network. In some cases, an activation function such as rectified linear unit may be used in some of the layers. In a CNN system, there can be one or more layers for each type of operation. The input data of the CNN system may be the data to be analyzed such as 3D radar data. The simplest architecture of a convolutional neural networks starts with an input layer (e.g., images) followed by a sequence of convolutional layers and pooling layers, and ends with fully-connected layers. In some cases, the convolutional layers are followed by a layer of ReLU activation function. Other activation functions can also be used, for example the saturating hyperbolic tangent, identity, binary step, logistic, arcTan, softsign, parameteric rectified linear unit, exponential linear unit. softPlus, bent identity, softExponential, Sinusoid, Sinc, Gaussian, the sigmoid function and various others. The convolutional, pooling and ReLU layers may act as learnable features extractors, while the fully connected layers acts as a machine learning classifier.</p><p>[0094] In some cases, the convolutional layers and fully-connected layers may include parameters or weights. These parameters or weights can be learned in a training phase. The parameters may be trained with gradient descent so that the class scores that the CNN computes are consistent with the labels in the training set for each 3D point cloud image. The parameters may be obtained from a back propagation neural network training process that may or may not be performed using the same hardware as the production or application process.</p><p>[0095] A convolution layer may comprise one or more filters. These filters will activate when they see same specific structure in the input data. In some cases, the input data may be 3D images, and in the convolution layer one or more filter operations may be applied to the pixels of the image. A convolution layer may comprise a set of learnable filters that slide over the image spatially, computing dot products between the entries of the filter and the input image. The filter operations may be implemented as convolution of a kernel over the entire image. A kernel may comprise one or more parameters. Results of the filter operations may be summed together across channels to provide an output from the convolution layer to the next pooling layer. A convolution layer may perform high-dimension convolutions. For example, the three-dimensional feature maps or input 3D data are processed by a group of three-dimensional kernels in a convolution layer.</p><p>[0096] The output produced by the first layer of the pipeline engine may be supplied to a second layer which is configured to extract understanding of a target object such as shapes, materials, sub-surface structure and the like. In some cases, the second layer can also be implemented using a machine learning architecture.</p><p>[0097] The output produced by the second layer may then be supplied to a third layer of the pipeline engine which is configured for perform interpretations and decision makings, such as object recognition, separation, segmentation, collision avoidance, target dynamics (e.g., mobility), identity recognition, type classification and the like. In some cases, the dynamics or mobility of an object may be used for determining a collision avoidance scheme.</p><p>[0098] The pipeline engine described herein can be implemented by one or more processors. In some embodiments, the one or more processors may be a programmable processor (e.g., a central processing unit (CPU), a graphic processing unit (GPU), a general-purpose processing unit or a microcontroller), in the form of fine-grained spatial architectures such as a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), and/or one or more Advanced RISC Machine (ARM) processors. In some embodiments, the processor may be a processing unit of a computer system.</p><p>[0099] In some cases, to mitigate for noise effects, Bayesian estimation techniques (e.g. Kalman filtering) is applied to fit a point cloud to a rigid model of the bed for aligning the robotic cart to the bed. In some cases, if the target object (e.g., patient bed) is not detected <b>907</b>, the process may proceed with informing user that the robotic cart position is invalid (e.g., out of the proper region) <b>921</b>. For example, if the bed is not detected, a notification may be displayed on the GUI to inform the user that the cart position is invalid.</p><p>[0100] If a target object such as a patient bed is detected <b>907</b>, the method may proceed with model coordinate registration <b>909</b> by mapping the position and orientation of the patient bed model to the coordinates of the robotic system (e.g., via a pre-registration of the sensor and robotic system coordinate frames). Next, the method may comprise an algorithm to determine whether the system is in an acceptable workspace. As an example, the algorithm may include computing the position and orientation error <b>911</b> and compare it against a threshold to determine whether the system is in an acceptable workspace.</p><p>[0101] In some cases, the system may provide both an autonomous mode and a manual positioning mode. If autonomous mode once is enabled (e.g., selected by a user) <b>913</b>, the system may execute an active system positioning algorithm. The active system positioning algorithm may, for example, perform path planning <b>915</b> to generate a path from current position to a target position at a target orientation, with proper moving speed, acceleration, and the like. For instance, a robotic cart wheel path plan is generated, or updated, to eliminate the relative orientation error between the desired and the sensed orientation that is computed in the previous operation <b>911</b>. The path is executed <b>917</b> and the robotic wheel motion is controlled to move the robotic cart to the desired orientation and position.</p><p>[0102] If the system disables the active system positioning algorithm such as in a semi-autonomous mode <b>913</b>, the user may be informed of the state of validity of the system positioning relative to the bed e.g., acceptable or not acceptable position <b>919</b>, and the user may manually control the position of the robotic cart.</p><p>[0103] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance. As described above, the real-time 3D map of the operating environment may comprise obstacle information and relative location to one or more components of the robotic endoscope system. The system may determine a proximity threshold and may autonomously move one or more components of the system to avoid collision. For example, as shown in FIG. <b>10</b>, when the robotic arm is actuated during the procedure, the sensor may detect unwanted proximity between the arm and the monitor of the system <b>1000</b>. The proximity is detected via the analysis of sensor signals that may contain depth information. In response to the proximity, the user may be informed of the potential collision via a UI warning. Alternatively or additionally, the robotic arm may be automatically moved away from the monitor, or the monitor may be actuated away from the arm <b>1010</b>. For example, the monitor may be mounted to a robotic support that can be actuated to move the monitor in response to control signals. As illustrated in the example <b>1010</b>, upon detection of potential collision between the monitor and any portion of the robotic arm, both the robotic support for the monitor and the robotic arm may be actuated to move away from each other. Alternatively, as illustrated in FIG. <b>11</b>, upon detection of potential collision between the monitor and any portion of the robotic arm <b>1100</b>, the robotic arm may be actuated to move away from the monitor.</p><p>[0104] FIG. <b>11</b> shows an example when an operator moves a monitor into the workspace of the robotic arm, the system may detect a proximity to the monitor is approaching a threshold and may automatically change a configuration of the robotic arm. The redundancy of the robotic arm may beneficially allow for the change of configuration of the robotic arm while maintaining the position and orientation of the IDM. FIG. <b>12</b> shows an example that during the procedure, equipment such as the fluoroscopy C-arm <b>1207</b> is brought into the workspace. The system may detect the equipment <b>1207</b> and a proximity to the equipment, then automatically adjust a configuration of the robotic arm and/or the robotic cart to provide enough clearance for the C-arm to take fluoroscopy images of the patient and/or complete sweep(s) for tomosynthesis. The configuration of the robotic arm may change from a first configuration <b>1203</b> to a second configuration <b>1205</b> while the position and orientation of IDM <b>1201</b> remain the same.</p><p>[0105] FIG. <b>13</b> shows an example of a collision avoidance algorithm <b>1300</b>. It should be noted that in the illustrated process though Lidar data (3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. The steps about capturing the 3D point cloud data and segmentation of object within a workspace to detect an object can be the same as those described in FIG. <b>9</b>. For example, the input data may comprise 3D point cloud data captured by Lidar device <b>1301</b> and deep learning techniques as described above may be employed to perform object segmentation, object recognition <b>1303</b> for collision avoidance. For instance, when an object (e.g., display device, monitor) is brought into the operating environment, a segmentation algorithm may be executed to detect and recognize the object. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the monitor).</p><p>[0106] In some cases, signal filtering is performed to mitigate effects of sensing uncertainty <b>1305</b>. In some cases, when the object being detected can be represented by a precomputed model, Bayesian filtering methods, such as Kalman filtering, may be applied. For example, the precomputed model may include an estimation algorithm for an object, where the sensed point cloud of a monitor is an input to the estimation algorithm which estimates the state (position, orientation) of a monitoring device.</p><p>[0107] Next, position and orientation of the point cloud are mapped to arm base coordinate frame <b>1307</b>. Position of components of the robotic endoscope system such as the robotic arm and position of the points in the segmented monitor region may be registered in the same frame of reference for determining a minimum proximity between any portion of the monitor and any portion of the robotic arm (computing distance-to-contact) <b>1309</b>. The positions and orientations of the robotic arm may be obtained based on kinematic mapping.</p><p>[0108] In some cases, a proximity determination algorithm may be executed to determine a potential collision whether the minimum proximity violates a predetermined proximity threshold <b>1311</b>. In some cases, upon determine a violation of the predetermined proximity threshold, the system may activate an admittance controller to reconfigure a configuration of the robotic arm (e.g., by actuating one or more joints/links of the robotic arm) <b>1313</b>. For example, the admittance controller may execute an admittance control algorithm which defines a proximal-most point of collision as repulsive force and reconfigure the robotic arm to increase the (minimum) distance between the robotic arm and the monitor. An algorithm is executed to reposition the arm for the purpose of avoiding a collision prior to occurring. In some cases, the admittance control algorithm may map a sensed proximity between any portion of the robotic arm and an object to a virtual force applied on the robotic arm and calculate an output motion for the robotic arm as if an equivalent contact force was applied to the robotic arm (through contact). For instance, the admittance control algorithm comprises mapping a shortest distance between a link of an arm and obstacle to an input to a commanded task-space velocity of the arm link. The Geometric Jacobian of the arm may be used to map a task-space motion command to arm joint commands in order to achieve arm motion which increases the distance between the arm and obstacle.</p><p>[0109] In the event of the robotic system having more degrees-of-freedom than the task, i.e. actuation redundancy existing, the avoidance may be implemented via a redundancy resolution algorithm. For exempt the elbow of the arm may be repositioned to prevent a collision without affecting the position and orientation of the end-effector. Such algorithm may be implemented by defining a secondary redundancy-resolution task where the arm is repositioned such that the distance between the arm and the object (prior to collision) is increased. In some cases, when there is more than one degree-of-freedom in redundancy, the robot command can be chosen to actuate the joint, for example, as one where the norm of joint rates is minimized.</p><p>[0110] In some embodiments, the robotic endoscope system may be capable of autonomously adjusting position of the IDM relative to the patient side mount based on buckling detection. When the flexible endoscope is pushed at the proximal end, during insertion of the flexible device into the anatomy, the flexible endoscope may deform when navigating through turns and buckling. The deformation may happen during insertion, as the flexible device may take on a minimum-energy shape, which may be a \\\"hugging\\\" of the shaft against tissue. The buckling may happen when resistance force is encountered in the distal portion of the shaft.</p><p>[0111] During retraction of the flexible device, the distance of the shaft that is \\\"lost to buckling\\\" may become slack when the system actuation direction is reversed (e.g., backward moving or retraction). This phenomenon will result in a perceived dead-zone or system delay (a user input to command movement of the robotic endoscope tip does not map directly to robotic endoscope tip motion). When an endoscope is buckled, a retraction of the endoscope may result in robotic actuation with little endoscope tip translation.</p><p>[0112] The prolapse or kink may result in potential damage as it may expose the sharp edges of the kinked elongate device and complicate the surgical procedure. Moreover, a bent or kinked elongate device may render the system losing location/shape control of the device during both insertion and retraction and it may block the passage of an instrument. Furthermore, a device that prolapses or kinks may not be able to provide adequate reach towards the target anatomy for performing the intended task.</p><p>[0113] The robotic endoscope system herein may employ a responsive insertion and retraction velocity control of the flexible endoscope. The responsive velocity control method herein may automatically correct for motion differences between the endoscope tip and a velocity command (e.g., instrument driving mechanism (IDM) command). In some cases, the velocity control of the endoscope may be performed in conjunction with the collision avoidance algorithm as described above such that the robotic arm may autonomously reconfigure to avoid collision with other objects while the velocity of the endoscope tip is maintained and not influenced.</p><p>[0114] Unlike the conventional methods for buckling detection based on the difference in the position of the endoscope device (e.g., expected position and measure tip position), the methods and systems herein may automatically correct the buckling/deformation during insertion and retraction based on the velocity measured at an endoscope tip and a velocity control command. This beneficially avoids shape sensing or using extra imaging approaches to determine the shape or position of the endoscope device.</p><p>[0115] During insertion of an endoscope device, a velocity command may be received by the endoscope device. The velocity command may be provided by a user input. For example, a user may provide a control instruction via a control interface of the endoscope device indicating a desired/expected velocity of the endoscope tip. The control interface may include various devices such as touchscreen monitors, joysticks, keyboards and other interactive devices. A user may be able to navigate and/or control the motion of the robotic arm and the motion (e.g., tip velocity) of the catheter using a user input device. The user input device can have any type user interactive component, such as a button, mouse, joystick, trackball, touchpad, pen, image capturing device, motion capture device, microphone, touchscreen, hand-held wrist gimbals, exoskeletal gloves, or other user interaction system such as virtual reality systems, augmented reality systems and the like.</p><p>[0116] The user input for commanding a velocity of the endoscope tip may be received via the input device. For instance, a press on a joystick may be mapped to an analog value indicating a speed/velocity of the tip. For example, half press on the joystick may be mapped to 3 mm/s, full press may be mapped to 6 mm/s and no press may be mapped to 0 mm/s.</p><p>[0117] Based on the specific user input device, the user input may be processed to be converted to a desired/commanded velocity of the tip of the catheter/endoscope. Next, a tip velocity error is computed. The tip velocity error is the difference between the desired/commanded tip velocity and the velocity of the endoscope tip. In some embodiments, the velocity of the endoscope tip may be measured based on sensor data. In some cases, the sensor data may comprise position and orientation information of the distal tip of the endoscope.</p><p>[0118] In some cases, the sensor signals may be acquired by positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the distal end used with an electromagnetic tracking system to detect the position and orientation of the distal end of the endoscope. For example, positioning sensors such as electromagnetic (EM) sensors may be embedded at the distal tip of the catheter and an EM field generator may be positioned next to the patient torso during procedure. The EM field generator may locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. The endoscope tip position measured by the EM sensor p<sub>e</sub>=EM sensor (tip) position, may be expressed in field-generator frame.</p><p>[0119] Next, the linear velocity of the endoscope tip may be computed. The linear velocity may be computed using time derivative method such as backward Euler derivative. In some cases, the endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction. This projection may be required because the user provided velocity command is based on integrated position changes that occur in the direction of n. In some cases, a low pass filter may be applied to generate the filtered time derivative data.</p><p>[0120] The endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction of n using a projection matrix v<sub>n</sub>=nn<sup>T </sup>filt(dp<sub>e</sub>/dt).</p><p>[0121] Where n is a unit vector that indicates the endoscope tip heading direction, expressed in field-generator frame. There may be mechanical offset between the EM sensor and the scope tip and the mechanical offset may be calibrated for each endoscope device. dp<sub>e</sub>/dt represents the time derivative of the endoscope tip, nn<sup>T </sup>is the projection matrix that maps the aforementioned velocity to the heading direction of the endoscope tip (i.e. velocities that are not in the direction of the heading are ignored). The velocity v<sub>n </sub>may not be affected by articulation as the endoscope tip translation owing to articulation is orthogonal to n.</p><p>[0122] After the endoscope tip velocity is computed, the endoscope tip velocity error may be computed and may be further processed for safety checks. In some cases, the safety checks may comprise a plurality of checks. For example, the plurality of checks may comprise determining whether the endoscope tip has been stationary (e.g., tip velocity is about zero) while the handle portion of the endoscope (e.g., IDM) has an insertion distance is beyond a distance threshold. In another example, the plurality of checks may comprise determining whether the endoscope tip is retracting (e.g., negative tip velocity error) while the handle portion of the endoscope (e.g., IDM) is inserting, if yes, a prolapse may be occurring. In a further example, the plurality of checks may comprise determining whether the insertion force is beyond a force threshold. The term \\\"insertion distance\\\" as utilized herein may refer to the distance along the navigation path.</p><p>[0123] The method may comprise a closed loop control of the tip velocity to reduce the tip velocity error. The tip velocity of the endoscope may be controlled based on the tip velocity error computed at operation which is used as the feedback signal for commanding the motion of the IDM. The user velocity input may be mapped to the command to control the motion of IDM (e.g., velocity to move the IDM along the insertion axis). The command may be control signals to control the motors of the robotic arm thereby controlling a motion of the IDM (i.e., proximal end of the endoscope). The endoscope tip velocity may be calculated based on the robotic arm inverse kinematics. The endoscope tip velocity is then used to calculate the tip velocity in the heading direction by projection in the heading direction of n using a projection matrix as described above. In some cases, the feedback signal may be the projection of tip velocity processed by a low-pass filter.</p><p>[0124] Based on the control algorithm, a motion command is generated to actuate the robotic arm thereby affecting a tip velocity of the endoscope. The effect on the motion of the IDM (e.g., insertion velocity, insertion distance) and on the motion of the endoscope tip (e.g., tip velocity) may not perfectly match due to the tortuosity of the navigation path, the buckling and/or prolapse as described above.</p><p>[0125] During retraction of the medical device, a previously determined deformation-loss during insertion may be used as a feed-forward term to perform a backslash compensation for the retraction control. This reduces the deformation-loss incurred during the insertion prior to the tip motion during the retraction.</p><p>[0126] FIG. <b>14</b> illustrates an example of a flexible endoscope <b>1400</b>, in accordance with some embodiments of the present disclosure. As shown in FIG. <b>14</b>, the flexible endoscope <b>1400</b> may comprise a handle/proximal portion <b>1409</b> and a flexible elongate member to be inserted inside of a subject. The flexible elongate member can be the same as the one described above. In some embodiments, the flexible elongate member may comprise a proximal shaft (e.g., insertion shaft <b>1401</b>), steerable tip (e.g., tip <b>1405</b>), and a steerable section (active bending section <b>1403</b>). The active bending section, and the proximal shaft section can be the same as those described elsewhere herein. The endoscope <b>1400</b> may also be referred to as steerable catheter assembly as described elsewhere herein. In some cases, the endoscope <b>1400</b> may be a single-use robotic endoscope. In some cases, the entire catheter assembly may be disposable. In some cases, at least a portion of the catheter assembly may be disposable. In some cases, the entire endoscope may be released from an instrument driving mechanism and can be disposed of. In some embodiment, the endoscope may contain varying levels of stiffness along the shaft, as to improve functional operation.</p><p>[0127] The endoscope or steerable catheter assembly <b>1400</b> may comprise a handle portion <b>1409</b> that may include one or more components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>1400</b> and an instrument driving mechanism (not shown), and any other external system or devices. In another example, the handle portion <b>1409</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera, electromagnetic sensor and LED lights) of the endoscope.</p><p>[0128] The one or more components located at the handle may be optimized such that expensive and complicated components may be allocated to the robotic support system, a hand-held controller or an instrument driving mechanism thereby reducing the cost and simplifying the design the disposable endoscope. The handle portion or proximal portion may provide an electrical and mechanical interface to allow for electrical communication and mechanical communication with the instrument driving mechanism. The instrument driving mechanism may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley/capstans assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0129] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0130] The electrical interface (e.g., printed circuit board) may allow image/video data and/or sensor data to be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. In some cases, the electrical interface may establish electrical communication without cables or wires. For example, the interface may comprise pins soldered onto an electronics board such as a printed circuit board (PCB). For instance, receptacle connector (e.g., the female connector) is provided on the instrument driving mechanism as the mating interface. This may beneficially allow the endoscope to be quickly plugged into the instrument driving mechanism or robotic support without utilizing extra cables. Such type of electrical interface may also serve as a mechanical interface such that when the handle portion is plugged into the instrument driving mechanism, both mechanical and electrical coupling is established. Alternatively or in addition to, the instrument driving mechanism may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0131] In some cases, the handle portion <b>1409</b> may comprise one or more mechanical control modules such as lure <b>1411</b> for interfacing the irrigation system/aspiration system. In some cases, the handle portion may include lever/knob for articulation control. Alternatively, the articulation control may be located at a separate controller attached to the handle portion via the instrument driving mechanism.</p><p>[0132] The endoscope may be attached to a robotic support system or a hand-held controller via the instrument driving mechanism. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>1400</b>. The mechanical interface may allow the steerable catheter assembly <b>1400</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0133] In the illustrated example, the distal tip of the catheter or endoscope shaft is configured to be articulated/bent in two or more degrees of freedom to provide a desired camera view or control the direction of the endoscope. As illustrated in the example, imaging device (e.g., camera), position sensors (e.g., electromagnetic sensor) <b>1407</b> is located at the tip of the catheter or endoscope shaft <b>1405</b>. For example, line of sight of the camera may be controlled by controlling the articulation of the active bending section <b>1403</b>. In some instances, the angle of the camera may be adjustable such that the line of sight can be adjusted without or in addition to articulating the distal tip of the catheter or endoscope shaft. For example, the camera may be oriented at an angle (e.g., tilt) with respect to the axial direction of the tip of the endoscope with aid of an optimal component.</p><p>[0134] The distal tip <b>1405</b> may be a rigid component that allow for positioning sensors such as electromagnetic (EM) sensors, imaging devices (e.g., camera) and other electronic components (e.g., LED light source) being embedded at the distal tip.</p><p>[0135] In real-time EM tracking, the EM sensor comprising of one or more sensor coils embedded in one or more locations and orientations in the medical instrument (e.g., tip of the endoscopic tool) measures the variation in the EM field created by one or more static EM field generators positioned at a location close to a patient. The location information detected by the EM sensors is stored as EM data. The EM field generator (or transmitter), may be placed close to the patient to create a low intensity magnetic field that the embedded sensor may detect. The magnetic field induces small currents in the sensor coils of the EM sensor, which may be analyzed to determine the distance and angle between the EM sensor and the EM field generator. For example, the EM field generator may be positioned close to the patient torso during procedure to locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. This may provide a visual guide to an operator when driving the bronchoscope towards the target site.</p><p>[0136] The endoscope may have a unique design in the elongate member. In some cases, the active bending section <b>1403</b>, and the proximal shaft of the endoscope may consist of a single tube that incorporates a series of cuts (e.g., reliefs, slits, etc.) along its length to allow for improved flexibility, a desirable stiffness as well as the anti-prolapse feature (e.g., features to define a minimum bend radius).</p><p>[0137] As described above, the active bending section <b>1403</b> may be designed to allow for bending in two or more degrees of freedom (e.g., articulation). A greater bending degree such as 180 and 270 degrees (or other articulation parameters for clinical indications) can be achieved by the unique structure of the active bending section. In some cases, a variable minimum bend radius along the axial axis of the elongate member may be provided such that an active bending section may comprise two or more different minimum bend radii.</p><p>[0138] The articulation of the endoscope may be controlled by applying force to the distal end of the endoscope via one or multiple pull wires. The one or more pull wires may be attached to the distal end of the endoscope. In the case of multiple pull wires, pulling one wire at a time may change the orientation of the distal tip to pitch up, down, left, right or any direction needed. In some cases, the pull wires may be anchored at the distal tip of the endoscope, running through the bending section, and entering the handle where they are coupled to a driving component (e.g., pulley). This handle pulley may interact with an output shaft from the robotic system.</p><p>[0139] In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, capstans, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end/portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0140] The pull wires may be made of any suitable material such as stainless steel (e.g., SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0141] The proximal design may improve the reliability of the device without introducing extra cost allowing for a low-cost single-use endoscope. In another aspect of the invention, a single-use robotic endoscope is provided. The robotic endoscope may be a bronchoscope and can be the same as the steerable catheter assembly as described elsewhere herein. Traditional endoscopes can be complex in design and are usually designed to be re-used after procedures, which require thorough cleaning, dis-infection, or sterilization after each procedure. The existing endoscopes are often designed with complex structures to ensure the endoscopes can endure the cleaning, dis-infection, and sterilization processes. The provided robotic bronchoscope can be a single-use endoscope that may beneficially reduce cross-contamination between patients and infections. In some cases, the robotic bronchoscope may be delivered to the medical practitioner in a pre-sterilized package and are intended to be disposed of after a single-use.</p><p>[0142] As shown in FIG. <b>15</b>, a robotic bronchoscope <b>1510</b> may comprise a handle portion <b>1513</b> and a flexible elongate member <b>1511</b>. In some embodiments, the flexible elongate member <b>1111</b> may comprise a shaft, steerable tip, and a steerable/active bending section. The robotic bronchoscope <b>1510</b> can be the same as the steerable catheter assembly as described in FIG. <b>14</b>. The robotic bronchoscope may be a single-use robotic endoscope. In some cases, only the catheter may be disposable. In some cases, at least a portion of the catheter may be disposable. In some cases, the entire robotic bronchoscope may be released from the instrument driving mechanism and can be disposed of. In some cases, the bronchoscope may contain varying levels of stiffness along its shaft, as to improve functional operation. In some cases, a minimum bend radius along the shaft may vary.</p><p>[0143] The robotic bronchoscope can be releasably coupled to an instrument driving mechanism <b>1520</b>. The instrument driving mechanism <b>1520</b> may be mounted to the arm of the robotic support system or to any actuated support system as described elsewhere herein. The instrument driving mechanism may provide mechanical and electrical interface to the robotic bronchoscope <b>1510</b>. The mechanical interface may allow the robotic bronchoscope <b>1510</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic bronchoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic bronchoscope may be coupled or released from the instrument driving mechanism manually without using a tool.</p><p>[0144] FIG. <b>16</b> shows an example of an instrument driving mechanism <b>1620</b> providing mechanical interface to the handle portion <b>1613</b> of the robotic bronchoscope. As shown in the example, the instrument driving mechanism <b>1620</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the flexible endoscope or catheter. The handle portion <b>1613</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies or capstans are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0145] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive, adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0146] FIG. <b>17</b> shows an example of a distal tip <b>1700</b> of an endoscope. In some cases, the distal portion or tip of the catheter <b>1700</b> may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). The catheter may comprise a tip portion, bending section, and insertion shaft. In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple sections having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments (e.g., cuts, patterns), adding additional supporting components or any combination of the above. In some embodiments, the catheter may have variable minimum bend radius along the longitudinal axis direction. The selection of different minimum bend radius at different location long the catheter may beneficially provide anti-prolapse capability while still allow the catheter to reach hard-to-reach regions. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0147] The distal portion of the catheter may be steered by one or more pull wires <b>1705</b>. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such that it can be bent by the pull wires. In some embodiments, the proximal end or terminal end of one or more pull wires <b>1705</b> may be coupled to a driving mechanism (e.g., gears, pulleys, capstan etc.) via the anchoring mechanism as described above.</p><p>[0148] The pull wire <b>1705</b> may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire <b>1705</b> can also be made of natural or organic materials or fibers. The pull wire <b>1705</b> can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires <b>1705</b> may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0149] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0150] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera <b>1713</b>. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise A/D converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0151] The illumination device may comprise one or more light sources <b>1711</b> positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0152] The imaging device and the illumination device may be integrated to the catheter. For example, the distal portion of the catheter may comprise suitable structures matching at least a dimension of the imaging device and the illumination device. The imaging device and the illumination device may be embedded into the catheter. FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device. A camera may be located at the distal portion. The distal tip may have a structure to receive the camera, illumination device and/or the location sensor. For example, the camera may be embedded into a cavity <b>1810</b> at the distal tip of the catheter. The cavity <b>1810</b> may be integrally formed with the distal portion of the cavity and may have a dimension matching a length/width of the camera such that the camera may not move relative to the catheter. The camera may be adjacent to the working channel <b>1820</b> of the catheter to provide near field view of the tissue or the organs. In some cases, the attitude or orientation of the imaging device may be controlled by controlling a rotational movement (e.g., roll) of the catheter.</p><p>[0153] The power to the camera may be provided by a wired cable. In some cases, the cable wire may be in a wire bundle providing power to the camera as well as illumination elements or other circuitry at the distal tip of the catheter. The camera and/or light source may be supplied with power from a power source located at the handle portion via wires, copper wires, or via any other suitable means running through the length of the catheter. In some cases, real-time images or video of the tissue or organ may be transmitted to an external user interface or display wirelessly. The wireless communication may be WiFi, Bluetooth, RF communication or other forms of communication. In some cases, images or videos captured by the camera may be broadcasted to a plurality of devices or systems. In some cases, image and/or video data from the camera may be transmitted down the length of the catheter to the processors situated in the handle portion via wires, copper wires, or via any other suitable means. The image or video data may be transmitted via the wireless communication component in the handle portion to an external device/system. In some cases, the system may be designed such that no wires are visible or exposed to operators.</p><p>[0154] In conventional endoscopy, illumination light may be provided by fiber cables that transfer the light of a light source located at the proximal end of the endoscope, to the distal end of the robotic endoscope. In some embodiments of the disclosure, miniaturized LED lights may be employed and embedded into the distal portion of the catheter to reduce the design complexity. In some cases, the distal portion may comprise a structure <b>1430</b> having a dimension matching a dimension of the miniaturized LED light source. As shown in the illustrated example, two cavities <b>1430</b> may be integrally formed with the catheter to receive two LED light sources. For instance, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm) and diameter of the working channel of the catheter may be around 2 mm such that two LED light sources may be embedded at the distal end. The outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool's dimensional or specific application. Any number of light sources may be included. The internal structure of the distal portion may be designed to fit any number of light sources.</p><p>[0155] In some cases, each of the LEDs may be connected to power wires which may run to the proximal handle. In some embodiment, the LEDs may be soldered to separated power wires that later bundle together to form a single strand. In some embodiments, the LEDs may be soldered to pull wires that supply power. In other embodiments, the LEDs may be crimped or connected directly to a single pair of power wires. In some cases, a protection layer such as a thin layer of biocompatible glue may be applied to the front surface of the LEDs to provide protection while allowing light emitted out. In some cases, an additional cover <b>1431</b> may be placed at the forwarding end face of the distal tip providing precise positioning of the LEDs as well as sufficient room for the glue. The cover <b>1831</b> may be composed of transparent material matching the refractive index of the glue so that the illumination light may not be obstructed.</p><p>[0156] While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A method for controlling a robotic endoscope system, the method comprising: <BR />generating a 3D depth map of an environment surrounding the robotic endoscope system;<BR />autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and<BR />actuating the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of claim 1, wherein the 3D depth map is generated based at least in part on 3D point cloud data.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>3. The method of claim 2, further comprising processing the 3D depth map to detect the patient bed and computing a position and orientation of the robotic support system relative to the patient bed.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>4. The method of claim 1, wherein a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>5. The method of claim 1, further comprising controlling a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"5\\\"><p>6. The method of claim 5, further comprising loading a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"5\\\"><p>7. The method of claim 5, further comprising automatically adjusting a position of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The method of claim 1, further comprising detecting and recognizing an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p>9. The method of claim 8, further comprising detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>10. The method of claim 9, further comprising executing a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>11. The method of claim 1, wherein the IDM is autonomously aligned to the component based at least in part on sensor data.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p>12. The method of claim 11, wherein the sensor data is captured by electromagnetic sensors.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p>13. The method of claim 11, wherein the sensor data is captured by a camera including a fiducial marker placed on the component and wherein the 3D depth map comprises at least a 3D location of the fiducial marker.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"yes\\\"><p>14. A system for controlling a robotic endoscope system, the system comprising: a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to: <BR />generate a 3D depth map of an environment surrounding the robotic endoscope system;<BR />autonomously actuate a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and<BR />actuate the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>15. The system of claim 14, wherein the 3D depth map is generated based at least in part on 3D point cloud data.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>16. The system of claim 14, wherein the one or more processors are further configured to process the 3D depth map to detect the patient bed and compute a position and orientation of the robotic support system relative to the patient bed.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>17. The system of claim 14, wherein a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>18. The system of claim 14, wherein the one or more processors are further configured to control a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p>19. The system of claim 18, wherein the one or more processors are further configured to load a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p>20. The system of claim 18, wherein the one or more processors are further configured to automatically adjust a position of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"21\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>21. The system of claim 14, wherein the one or more processors are further configured to detect and recognize an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM.</p></Claim><Claim claimid=\\\"22\\\" independent=\\\"no\\\" parents=\\\"21\\\"><p>22. The system of claim 21, wherein the one or more processors are further configured to detect a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"23\\\" independent=\\\"no\\\" parents=\\\"22\\\"><p>23. The system of claim 22, wherein the one or more processors are further configured to execute a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p></Claim><Claim claimid=\\\"24\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>24. The system of claim 14, wherein the IDM is autonomously aligned to the component based at least in part on sensor data.</p></Claim><Claim claimid=\\\"25\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>25. The system of claim 14, wherein the sensor data is captured by electromagnetic sensors.</p></Claim><Claim claimid=\\\"26\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>26. The system of claim 14, wherein the sensor data is captured by a camera including a fiducial marker placed on the component and wherein the 3D depth map comprises at least a 3D location of the fiducial marker.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/32\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B1/00097\",\n",
      "\n",
      "\"A61B1/0016\",\n",
      "\n",
      "\"A61B1/005\",\n",
      "\n",
      "\"A61B1/018\",\n",
      "\n",
      "\"A61B1/05\",\n",
      "\n",
      "\"A61B1/0676\",\n",
      "\n",
      "\"A61B1/0684\",\n",
      "\n",
      "\"A61B2034/107\",\n",
      "\n",
      "\"A61B2034/2051\",\n",
      "\n",
      "\"A61B2034/2055\",\n",
      "\n",
      "\"A61B2034/2074\",\n",
      "\n",
      "\"A61B2034/301\",\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/32\",\n",
      "\n",
      "\"A61B90/361\",\n",
      "\n",
      "\"A61B90/37\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"NOAH MEDICAL CORP\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20220524,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US11950868\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20240409,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US11950868\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20430522,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=100080311\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20230466010\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"SYSTEMS AND METHODS FOR SELF-ALIGNMENT AND ADJUSTMENT OF ROBOTIC ENDOSCOPE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE</p><p>[0001] This application is a continuation of International Patent Application No. PCT/US2023/023139, filed May 22, 2023, which claims priority to U.S. Provisional Patent Application No. 63/345,287, filed on May 24, 2022, and U.S. Provisional Patent Application No. 63/347,179, filed on May 31, 2022, each of which is entirely incorporated herein by reference.</p></relapp><shortsum><p>BACKGROUND OF THE INVENTION</p><p>[0002] Endoscopy procedures use an endoscope to examine the interior of a hollow organ or cavity of the body. Unlike many other medical imaging techniques, endoscopes are inserted into the organ directly. Flexible endoscope that can deliver instinctive steering and control is useful in diagnosing and treating diseases that are accessible through any natural orifice in the body. Depending on the clinical indication, the endoscope may be designated as bronchoscope, ureteroscope, colonoscope, gastroscope, ENT scope, and various others. For example, flexible bronchoscope may be used for lung cancer diagnosis and/or surgical treatment. However, one challenge in bronchoscopy is reaching the upper lobe of the lung while navigating through the airways. In another example, flexible endoscopy has been used to inspect and treat disorders of the gastrointestinal (GI) tract without the need for creating an opening on the patient's body. The endoscope is introduced via the mouth or anus into the upper or lower GI tracts respectively. A miniature camera at the distal end captures images of the GI wall that help the clinician in their diagnosis of the GI diseases. Simple surgical procedures (like polypectomy and biopsy) can be performed by introducing a flexible tool via a working channel to reach the site of interest at the distal end.</p><p>[0003] Endoscopes are traditionally made to be re-usable, which may require thorough cleaning, dis-infection, and/or sterilization after each procedure. In most cases, cleaning, dis-infection, and sterilization may be aggressive processes to kill germs and/or bacteria. Such procedures may also be harsh on the endoscopes themselves. Therefore, the designs of such re-usable endoscopes can often be complicated, especially to ensure that the endoscopes can survive such harsh cleaning, dis-infection, and sterilization protocols. Periodical maintenance and repairs for such re-usable endoscopes may often be needed.</p><p>[0004] Low cost, disposable medical devices designated for a single-use have become popular for instruments that are difficult to clean properly. Single-use, disposable devices may be packaged in sterile wrappers to avoid the risk of pathogenic cross-contamination of diseases such as HIV, hepatitis, and other pathogens. Hospitals generally welcome the convenience of single-use disposable products because they no longer have to be concerned with product age, overuse, breakage, malfunction, and sterilization. Traditional endoscopes often include a handle that operators use to maneuver the endoscope. For single-use endoscopes, the handle usually encloses the camera, expensive electronics, and mechanical structures at proximal end in order to transmit the video and allow the users to maneuver the endoscope via a user interface. This may lead to high cost of the handle for a single-use endoscope.</p><p>[0005] The process for setting up of medical robots can be time-consuming and challenging due to the number of accessories to set up and the complex workflows. The setup times can result in longer room turnover time, causing procedural delays. Examples of the challenging steps of the current system workflow are: (1) positioning the robotic system in a location that is appropriate for the procedure and (2) aligning the instrument drive mechanism to the patient side. These steps are challenging because the user has to consider the placement of other equipment in the room that is needed for the procedure and gain spatial understanding of the workspace of the robotic components while manually performing these steps. During the procedure, if there are any changes to the setup of the room, the user has to manually make adjustments to the system to accommodate.</p><p>SUMMARY OF THE INVENTION</p><p>[0006] Recognized herein is a need for a robotic endoscopic platform or system that allows for autonomous self-adjustment of the robotic endoscopic system in response to real-time operating environment. The present disclosure addresses the above need by providing methods and systems capable of detecting and tracking the system's operating environment (e.g., external environment surrounding the system) and automatically making adjustments to the system thereby (1) simplifying the workflow for an operator during system setup and/or during the procedure and (2) enabling system configurations that are more optimal than placements completed manually which may be deemed acceptable but non-optimal. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base station (e.g., robotic cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, etc.</p><p>[0007] In an aspect of the present disclosure, a method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; autonomously actuating arm for collision avoidance; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed. The methods implemented for enabling autonomous repositioning of system components given 3D depth map inputs require algorithms for processing and filtering the image data, mapping image data to inputs for the robotics control algorithms, and algorithms for obstacle voidance.</p><p>[0008] Recognized also herein are devices and systems comprising endoscopes which may be disposable and may not require extensive cleaning procedures. The present disclosure provides low-cost, single-use articulatable endoscope for diagnosis and treatment in various applications such as bronchoscopy, urology, gynecology, arthroscopy, orthopedics, ENT, gastro-intestine endoscopy, neurosurgery, and various others. In some embodiments, the present disclosure provides a single-use, disposable, robotically controlled bronchoscope for use with a robotic system to enable diagnostic evaluation of lesions anywhere in the pulmonary anatomy. It should be noted that the provided endoscope systems can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0009] It should be noted that the provided autonomous configuration, alignment and collision avoidance methods, endoscope components and various components of the device can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0010] In an aspect, a method is provided for controlling a robotic endoscope system moving and operating in an environment. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0011] In a related yet separate aspect, a system is provided for controlling a robotic endoscope system The system comprises: a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to: generate a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuate a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuate the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0012] In some embodiments, the 3D depth map is generated based at least in part on 3D point cloud data. in some cases, the method further comprises processing the 3D depth map to detect the patient bed and computing a position and orientation of the robotic support system relative to the patient bed. In some embodiments, a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p><p>[0013] In some embodiments, the method further comprises controlling a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed. In some cases, the method further comprises loading a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end. In some cases, the method further comprises automatically adjusting a position of the IDM relative to the component upon detection of a buckling event.</p><p>[0014] In some embodiments, the method further comprises detecting and recognizing an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM. In some cases, the method further comprises detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient. In some instances, the method further comprises executing a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p><p>[0015] In some embodiments, the IDM is autonomously aligned to the component based at least in part on sensor data. In some cases, the sensor data is captured by electromagnetic sensors. In some cases, the sensor data is captured by a camera including a fiducial marker placed on the component and the 3D depth map comprises at least a 3D location of the fiducial marker.</p><p>[0016] Additional aspects and advantages of the present disclosure will become readily apparent to those skilled in this art from the following detailed description, wherein only illustrative embodiments of the present disclosure are shown and described. As will be realized, the present disclosure is capable of other and different embodiments, and its several details are capable of modifications in various obvious respects, all without departing from the disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.</p><p>INCORPORATION BY REFERENCE</p><p>[0017] All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. To the extent publications and patents or patent applications incorporated by reference contradict the disclosure contained in the specification, the specification is intended to supersede and/or take precedence over any such contradictory material.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0018] The novel features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that sets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings (also \\\"Figure\\\" and \\\"FIG.\\\" herein), of which:</p><p>[0019] FIG. <b>1</b> schematically shows a robotic platform, in accordance with some embodiments of the present disclosure.</p><p>[0020] FIG. <b>2</b> shows an example of a robotic catheter assembly with an anti-buckling device.</p><p>[0021] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation.</p><p>[0022] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment using optical sensor;</p><p>[0023] FIGS. <b>4</b>B and <b>4</b>C shows an example of auto-alignment of IDM (instrument drive mechanism) based on EM sensor data.</p><p>[0024] FIG. <b>5</b> and FIG. <b>6</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to a patient side mount.</p><p>[0025] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed.</p><p>[0026] FIG. <b>8</b> shows an example of self-alignment of a robotic endoscope system.</p><p>[0027] FIG. <b>9</b> shows an example process for autonomous alignment of the robotic endoscope system.</p><p>[0028] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance of the robotic endoscope system with respect to one or more objects in the operating environment before and during a surgical operation.</p><p>[0029] FIG. <b>11</b> shows an example of a robotic bronchoscope comprising a handle portion and a flexible elongate member.</p><p>[0030] FIG. <b>12</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of the robotic bronchoscope.</p><p>[0031] FIG. <b>13</b> shows an example of a collision avoidance algorithm.</p><p>[0032] FIG. <b>14</b> and FIG. <b>15</b> show examples of a flexible endoscope.</p><p>[0033] FIG. <b>16</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of a robotic bronchoscope.</p><p>[0034] FIG. <b>17</b> shows an example of a distal tip of an endoscope.</p><p>[0035] FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device.</p></shortdesdrw><p>DETAILED DESCRIPTION OF THE INVENTION</p><p>[0036] While various embodiments of the invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions may occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed.</p><p>[0037] The embodiments disclosed herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. The disclosed embodiments can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0038] While exemplary embodiments will be primarily directed at a device or system for bronchoscopy, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in various anatomical regions of a patient's body. The provided device or system can be utilized in urology, gynecology, rhinology, otology, laryngoscopy, gastroenterology with the endoscopes, combined devices including endoscope and instruments, endoscopes with localization functions, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in other anatomical regions of a patient's body, such as such as brain, heart, lungs, intestines, eyes, skin, kidney, liver, pancreas, stomach, uterus, ovaries, testicles, bladder, ear, nose, mouth, soft tissues such as bone marrow, adipose tissue, muscle, glandular and mucosal tissue, spinal and nerve tissue, cartilage, hard biological tissues such as teeth, bone and the like, as well as body lumens and passages such as the sinuses, ureter, colon, esophagus, lung passages, blood vessels and throat, and various others, in the forms of: NeuroendoScope, EncephaloScope, OphthalmoScope, OtoScope, RhinoScope, LaryngoScope, GastroScope, EsophagoScope, BronchoScope, ThoracoScope, PleuroScope, AngioScope, MediastinoScope, NephroScope, GastroScope, DuodenoScope, CholeodoScope, CholangioScope, LaparoScope, AmioScope, UreteroScope, HysteroScope, CystoScope, ProctoScope, ColonoScope, ArthroScope, SialendoScope, Orthopedic Endoscopes, and others, in combination with various tools or instruments.</p><p>[0039] The systems and apparatuses herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. Systems and apparatuses provided herein can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0040] Whenever the term \\\"at least,\\\" \\\"greater than,\\\" or \\\"greater than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"at least,\\\" \\\"greater than\\\" or \\\"greater than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, greater than or equal to 1, 2, or 3 is equivalent to greater than or equal to 1, greater than or equal to 2, or greater than or equal to 3.</p><p>[0041] Whenever the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, less than or equal to 3, 2, or 1 is equivalent to less than or equal to 3, less than or equal to 2, or less than or equal to 1.</p><p>[0042] As used herein, the terms distal and proximal may generally refer to locations referenced from the apparatus, and can be opposite of anatomical references. For example, a distal location of a primary shaft or catheter may correspond to a proximal location of an elongate member of the patient, and a proximal location of the primary sheath or catheter may correspond to a distal location of the elongate member of the patient.</p><p>[0043] As described above, setting up a robotic endoscopic system can be time consuming and challenging due to the complexity of the operating environment, requirement for accurate alignment between the instrument and patient body part and various other reasons. The present disclosure provides methods and systems capable of detecting and tracking the system's operating environment and automatically making adjustments to the system thereby simplifying the workflow during system setup and/or during the procedure. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base (robot cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, and other functions as described elsewhere herein.</p><p>[0044] The operating environment of the robotic endoscope system may comprise one or more objects. The robotic endoscope system may be capable of detecting the one or more objects in the operating environment, generating a 3D map with depth information, performing autonomous alignment of the instrument drive mechanism with respect to the patient bed or body part, and self-adjusting its placement and configuration to avoid collision with the one or more objects. The one or more objects may comprise, for example, system accessories (e.g., system monitor, a monitor pole), external monitors, patient bed, patient, imaging equipment (e.g., fluoroscopy c-arm), anesthesia cart and other equipment or subject (e.g., operator, surgeon) in the operating environment before or during surgical operation.</p><p>[0045] FIG. <b>1</b> schematically shows a robotic platform <b>100</b>. The platform may comprise a robotic endoscope system including one or more flexible articulatable surgical instruments <b>105</b>, and a support apparatus <b>110</b> such as a robotic manipulator (e.g., robotic arm) to drive, support, position or control the movements and/or operation of the robotic system. The robotic platform may further include peripheral devices and subsystems such as imaging systems that may assist and/or facilitate the navigation of the elongate member to the target site in the body of a subject <b>120</b>.</p><p>[0046] The robotic endoscope system is provided for performing surgical operations or diagnosis with improved performance at low cost. For example, the robotic endoscope system may comprise a steerable catheter that can be entirely disposable. As shown in FIG. <b>1</b>, the robotic endoscope system may comprise a steerable catheter assembly <b>105</b> and a robotic support system <b>110</b>, for supporting or carrying the steerable catheter assembly. The steerable catheter assembly can be an endoscope. In some embodiments, the steerable catheter assembly may be a single-use robotic endoscope. In some embodiments, the robotic endoscope system may comprise an instrument driving mechanism (IDM) <b>103</b> that is attached to the distal end of the robotic arm <b>107</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>105</b>. The mechanical interface may allow the steerable catheter assembly <b>105</b> to be releasably coupled to the instrument driving mechanism <b>103</b>. For instance, a handle portion <b>104</b> of the steerable catheter assembly can be attached to the instrument driving mechanism (IDM) via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool. The instrument driving mechanism may be used to control the elongate member or robotic catheter assembly in two or more degrees of freedom (e.g., articulation).</p><p>[0047] The robotic support system <b>110</b> may comprise a robotic arm <b>107</b> and a mobile base (e.g., robotic cart) <b>109</b>. The robotic arm <b>107</b> may initiate the positioning of the robotic catheter assembly or other robotic instrument. In some cases, a user interface, robotic control modules, and the robotic arm may be mounted to the mobile cart. The mobile cart may include various elements such as rechargeable power supply in electrical communication with an electric panel providing charging ports for portable electronic devices, converters, transformers and surge protectors for a plurality of AC and DC receptacles as power source for the on-board equipment including one or more computers storing application specific software for the user interface.</p><p>[0048] The robotic arm <b>107</b> may have redundant degrees of freedom allowing for its elbow to be algorithmically, or passively, moved into configurations that are convenient for an operator initiate the positioning of the robotic system or other robotic instrument. For example, the robotic arm may comprise a plurality of joints having redundant degrees of freedom such that the joints of the robotic arm can be driven through a range of differing configurations for a given end effector position (e.g., IDM position). The redundant degrees of freedom may beneficially allow the robotic arm to be self-adjusted to an optimal configuration to avoid collision with other object in the operating environment prior to or during a procedure. For example, the instrument drive mechanism may automatically align itself to a patient side mount (e.g., a support structure at the patient bed for holding and supporting the endoscope device in place) during setup procedure. During the setup procedure and the operation procedure, upon detection of motion of the patient side mount, the instrument drive mechanism (IDM) is able to auto-adjust accordingly to avoid collision while maintain the position of the IDM, eliminating any interruptions to the procedural workflow and avoiding misalignment. In another example, when the robotic arm is actuated during the setup procedure or surgical operation, the system may detect unwanted proximity between any portion of the robotic arm and other objects surrounding it (e.g., the monitor of the system) and the robotic arm may be automatically reconfigured, moved away from the monitor to avoid collision.</p><p>[0049] In some embodiments, in addition to the autonomous movement of the robotic arm such as automatically positioning the steerable catheter assembly <b>105</b> to an initial position (e.g., access point) to access the target tissue, the robot arm may be passively moved by an operator. In such case, an operator can push the arm in any position and the arm compliantly moves. The robotic arm can also be controlled in a compliant mode to improve human robot interaction. For example, the compliant motion control of the robot art may employ a collision avoidance strategy and the position-force control may be designed to save unnecessary energy consumption while reducing impact of possible collisions.</p><p>[0050] The steerable catheter assembly <b>105</b> may comprise a flexible elongate member that is coupled to a handle portion. The robotic endoscope system may comprise an anti-buckling device <b>101</b> for preventing the buckling of the elongate member during use.</p><p>[0051] FIG. <b>2</b> shows another example of a robotic catheter assembly with an anti-buckling device <b>201</b>. The steerable catheter assembly may comprise a handle portion <b>211</b> that may include components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>211</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly and the instrument driving mechanism <b>221</b>, and any other external system or devices. In another example, the handle portion <b>211</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>221</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>221</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0052] The steerable catheter assembly may comprise a flexible elongate member <b>213</b> (i.e., catheter) that is coupled to the handle portion <b>211</b>. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic endoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0053] The robotic endoscope can be releasably coupled to an instrument driving mechanism <b>221</b>. The instrument driving mechanism <b>221</b> may be mounted to the arm of the robotic support system or to any actuated support system as described above. The instrument driving mechanism may provide mechanical and electrical interface to the robotic endoscope. The mechanical interface may allow the robotic endoscope to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic endoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic endoscope may be coupled or released from the instrument driving mechanism manually without using a tool. In some embodiments, the instrument driving mechanism <b>221</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion <b>211</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the catheter.</p><p>[0054] The handle portion may be designed allowing the robotic endoscope to be disposable at reduced cost. For instance, classic manual and robotic endoscope may have a cable in the proximal end of the endoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as EM sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the endoscope. The provided robotic endoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic endoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0055] In some case, the handle portion may be housing or comprise components configured to process image data, provide power, or establish communication with other external devices. In some cases, the communication may be wireless communication. For example, the wireless communications may include Wi-Fi, radio communications, Bluetooth, IR communications, or other types of direct communications. Such wireless communication capability may allow the robotic bronchoscope function in a plug-and-play fashion and can be conveniently disposed after single use. In some cases, the handle portion may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED light source) disposed within the robotic bronchoscope or catheter.</p><p>[0056] The handle portion may be designed in conjunction with the catheter such that cables or fibers can be eliminated. For instance, the catheter portion may employ a design having a single working channel allowing instruments to pass through the robotic bronchoscope, as well as low cost electronics such as a chip-on-tip camera, illumination sources such as light emitting diode (LED) and EM sensors located at optimal locations in accordance with the mechanical structure of the catheter. This may allow for a simplified design of the handle portion. For instance, by using LEDs for illumination, the termination at the handle portion can be based on electrical soldering or wire crimping alone. For example, the handle portion may include a proximal board where the camera cable, LED cable, and EM sensor cable terminate to while the proximal board connects to the interface of the handle portion and establishes the electrical connections to the instrument driving mechanism. As described above, the instrument driving mechanism is attached to the robot arm (robotic support system) and provide a mechanical and electrical interface to the handle portion. This may advantageously improve the assembly and implementation efficiency as well as simplify the manufacturing process and cost. In some cases, the handle portion along with the catheter may be disposed of after a single use.</p><p>[0057] In some embodiments, the steerable catheter assembly may have a substantially integral design that one or more components may be integral to the catheter thereby simplifying the assembly, manufacturing process while preserving the kinematic, dynamic performance of the steerable catheter. As shown in the example, the steerable catheter assembly may comprise an elongate member <b>213</b> or a probing portion that is brought into proximity to the tissue and/or area that is to be examined. The elongate member <b>213</b> may, in some cases, also be referred to as catheter. The catheter <b>213</b> may comprise internal structures such as a working channel allowing tools to be inserted through. As an example, the working channel may have a dimension such as diameter of around 2 mm to be compatible with standard tools. The working channel may have any other suitable dimensions based on the application.</p><p>[0058] The catheter <b>213</b> may be composed of suitable materials for desired flexibility or bending stiffness. In some cases, the materials of the catheter may be selected such that it may maintain structural support to the internal structures (e.g., working channel) as well as being substantially flexible (e.g., able to bend in various directions and orientations). For example, the catheter can be made of any suitable material such as urethane, vinyl (such as polyvinyl chloride), Nylon (such as vestamid, grillamid), pellethane, polyethylene, polypropylene, polycarbonate, polyester, silicon elastomer, acetate and so forth. In some cases, the materials may be polymer material, bio-compatible polymer material and the catheter may be sufficiently flexible to be advancing through a path with a small curvature without causing pain to a subject. In some cases, the catheter may comprise a sheath. The sheath may not be the same length of the catheter. The sheath may be shorter than the catheter to provide desired support. Alternatively, the catheter may be substantially a single-piece component.</p><p>[0059] In some case, the distal portion or tip of the catheter may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple segments having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments, adding additional supporting components or any combination of the above. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0060] The distal portion of the catheter may be steered by one or more pull wires. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such it can be bent by the pull wires. In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0061] As described above, the pull wires may be made of any suitable material such as stainless steel (e.g. SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0062] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter or the interstitials of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0063] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise A/D converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0064] The illumination device may comprise one or more light sources positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0065] In some embodiments, the catheter may be designed to be flexible. When the flexible portions of catheter are inserted by extending mechanisms through endoscope into patients, one or more sections may bend or buckle.</p><p>[0066] The anti-buckling mechanism <b>201</b> may be coupled to the handle portion of the robotic endoscope to support the catheter. The anti-buckling mechanism is used for preventing buckling of the insertion shaft. The anti-buckling mechanism <b>201</b> may be a telescopic extending device with internal mechanism to achieve anti-buckling of catheter during the insertion and withdrawal. The anti-buckling mechanism may be detachably connected to the handle portion of the robotic bronchoscope at one end, and may be detachably connected to a support surface <b>203</b> at the other end. As shown in the example, the anti-buckling tube may be attached to a bracket on the instrument driving mechanism and may be removable and disposable after the procedure via quick release mechanism. In the examples illustrated in FIG. <b>2</b>, a support arm (e.g., ET tube mount support arm) may be supported by the robotic mobile cart that supports the endotracheal tube mount and provides a support surface for the distal end of the anti-buckling tube to press against as it is compressed. The support arm may be controlled to rotate, translate vertically up and down and/or may a boom arm that expands and contracts, such that it can be precisely positioned over the patients mouth and attached to the endotracheal tube mount. The support arm positioning may be synchronized with the movement of the robotic arm that it may track the location of the point of entry of the catheter.</p><p>[0067] The anti-buckling mechanism may require a relatively linear trajectory to be traveled. In some cases, such trajectory may be ensured via an alignment between the anti-buckling mechanism in a collapsed state and a patient-side connector. FIG. <b>1</b> shows an example of a patient side connector <b>121</b> and IDM <b>103</b>. For example, the patient-side connector may be fixed to a patient side mount <b>123</b> (e.g., attached to the patient bed). The alignment between the IDM and the patient side connector/mount may involve lining up a collapsed anti-buckling mechanism with the patient-side connector. The robotic arm may automatically move the IDM into a position such that the IDM is aligned to the patient-side connector. In some cases, the alignment may comprise generating a 3D depth map of the operating environment, moving the mobile cart into a desired position relative to the patient bed based on the depth map, and moving the IDM into a position and orientation in alignment with the patient-side connector based on a detection of the location/position of the patient-side connector.</p><p>[0068] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation <b>300</b>, <b>330</b>. The robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise a steerable catheter assembly <b>320</b> and a robotic support system <b>310</b>, for supporting or carrying the steerable catheter assembly. In some cases, the steerable catheter assembly may be a bronchoscope. The steerable catheter assembly can be the same as the endoscope device as described elsewhere herein. In some cases, the steerable catheter assembly may be a single-use robotic bronchoscope. In some embodiments, the robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise an instrument driving mechanism (IDM) <b>313</b> that is attached to the end of the robotic arm <b>311</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>320</b>. The mechanical interface may allow the steerable catheter assembly <b>320</b> to be releasably coupled to the instrument driving mechanism. For instance, a handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0069] The steerable catheter assembly <b>320</b> may comprise a handle portion <b>323</b> that may include components configured to processing image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>323</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>320</b> and the instrument driving mechanism <b>313</b>, and any other external system or devices. In another example, the handle portion <b>323</b> may comprise circuitry elements such as power sources for powering the electronics (e.g. camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>313</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>313</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0070] The steerable catheter assembly <b>320</b> may comprise a flexible elongate member <b>321</b> that is coupled to the handle portion. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic bronchoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly <b>320</b> including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0071] In some embodiments, the provided bronchoscope system may also comprise accessories such as a user interface. As illustrated in the example system <b>330</b>, during operation, one or more components of the system such as a treatment interface module <b>331</b> (user console side) and/or a treatment control module <b>333</b> (patient and robot side) may be brought into the operating environment. The one or more components or accessories may be added or removed from the operating environment before or during a surgical operation. The robotic arm <b>311</b> may have redundant degrees of freedom such that the joints of the robotic arm can be driven into a range of differing configurations for a given end effector position. For example, upon detection of a treatment interface module <b>311</b>, the robotic arm <b>311</b> may automatically move into a different configuration to avoid collision with the treatment interface module while the distal end of the arm of the IDM <b>313</b> maintains a particular state (e.g., a given position or velocity of the end effector). Details about the collision avoidance is described later herein.</p><p>[0072] The treatment interface module may allow an operator or user to interact with the bronchoscope during surgical procedures. In some embodiments, the treatment control module <b>333</b> may be a hand-held controller. The treatment control module <b>333</b> may allow a user to control a velocity of the tip of the bronchoscope as described elsewhere herein. The treatment control module may, in some cases, comprise a proprietary user input device and one or more add-on elements removably coupled to an existing user device to improve user input experience. For instance, physical trackball or roller can replace or supplement the function of at least one of the virtual graphical element (e.g., navigational arrow displayed on touchpad) displayed on a graphical user interface (GUI) by giving it similar functionality to the graphical element which it replaces. Examples of user devices may include, but are not limited to, mobile devices, smartphones/cellphones, tablets, personal digital assistants (PDAs), laptop or notebook computers, desktop computers, media content players, and the like. Details about the user interface device and user console are described later herein.</p><p>[0073] The robotic endoscope platform herein may be able to detect one or more objects in the operating environment surrounding the robotic endoscope system. In the cases, the detection of the operating environment may comprise generating an obstacle map. An obstacle map may be a three-dimensional (3D) map describing positions of objects detected in the 3D space.</p><p>[0074] The 3D map of the operating environment may be constructed based on sensing data. In some cases, the sensing data is received from one or more vision sensors, including depth information for the environment. The vision sensor may comprise a camera, a video camera, a three-dimensional (3D) depth camera, a stereo camera, a depth camera, a Red Green Blue Depth (RGB-D) camera, a time-of-flight (TOF) camera, an infrared camera, a charge coupled device (CCD) image sensor, or a complementary metal oxide semiconductor (CMOS) image sensor. For example, the vision sensor can include only one camera (monocular vision sensor). Alternatively, the vision sensor can include two (binocular vision sensor) or more cameras. The vision sensors may be disposed on the robotic endoscope system such as the robotic cart, the monitor and the like. Alternatively or additionally, the vision sensors may not be disposed on the robotic endoscope system. For instance, the vision sensors may be disposed on the walls, ceilings or other places in the operating environment (e.g., room). In embodiments where multiple vision sensors are used, each sensor can be located on a different portion of the robotic endoscope system, and the disparity between the image data collected by each sensor can be used to provide depth information for the environment. Depth information can be used herein to refer to information regarding distances of one or more objects from the robotic endoscope system and/or sensor. In embodiments where a single vision sensor is used, depth information can be obtained by capturing image data for a plurality of different positions and orientations of the vision sensor, and then using suitable image analysis techniques (e.g., structure from motion) to reconstruct the depth information.</p><p>[0075] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment <b>400</b> using optical sensor <b>401</b> such as a camera. As mentioned elsewhere herein, an operating environment of the system may generally comprise an environment external to the system (e.g., the room the system is in, within certain proximity, etc.) or the environment that the system can move or operate within. In some cases, the camera may be a plenoptic camera having a main lens and additional micro lens array (MLA). The plenoptic camera model may be used to calculate a depth map of the captured image data. In some cases, the image data captured by the camera may be grayscale image with depth information at each pixel coordinate (i.e., depth map). The camera may be calibrated such that intrinsic camera parameters such as focal length, focus distance, distance between the MLA and image sensor, pixel size and the like are obtained for improving the depth measurement accuracy. Other parameters such as distortion coefficients may also be calibrated to rectify the image for metric depth measurement.</p><p>[0076] In some cases, the image data may be received and processed by one or more processors of the robotic endoscope system. For example, pre-processing of the capture image data may be performed. In an embodiment, the pre-processing algorithm can include image processing algorithms, such as image smoothing, to mitigate the effect of sensor noise, or image histogram equalization to enhance the pixel intensity values. Next, optical approaches as described elsewhere herein may be employed to generate a depth map of the operating environment <b>400</b>. In some cases, computer vision (CV) techniques or computer vision systems may be used to process the sensing data to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. For example, the CV output data may be generated using passive methods that only require images. Passive methods may include, for example, object recognition, stereoscopy, monocular shape-from-motion, shape-from-shading, and Simultaneous Localization and Mapping (SLAM). Alternatively, active methods may be utilized which may require controlled light to be projected into the target scene and the active methods may include, for example structured light and Time-of-Flight (ToF). In some cases, computer vision techniques such as optical flow, computational stereo approaches, iterative method combined with predictive models, machine learning approaches, predictive filtering or any non-rigid registration methods may be used to generate the descriptions of the 3D scene.</p><p>[0077] In some cases, a fiducial marker <b>403</b> may be employed to align the IDM to a patient side mount. The fiducial marker may be placed on the patient bed or the patient side mount <b>405</b>. The fiducial marker may have a 2D shape or pattern. For example, the fiducial marker may be a 2D QR code, grids, or any asymmetric shape. By acquiring images of the 2D fiducial marker (e.g., from different angles), the location and orientation of the patient side mount that the fiducial marker is placed at in a camera frame can be determined (e.g., triangulation). Based on the known spatial relationship between the camera and the IDM, the orientation and location of the IDM with respect to the patient side mount can be calculated. Alternatively, the fiducial marker may be a 3D fiducial marker such that the marker is visible and discernable in a wide range of angels. For example, a 3D fiducial marker may be located within the view of the imaging system such that the fiducial marker is always discernable regardless the position of the optical sensor with respect to the patient bed or the marker. The fiducial marker(s) may have any suitable 2D/3D shape (non-isotropic) or pattern such that a projection of the fiducial mark(s) corresponding to a view/angle is discernable from that of another view/angle. Alternatively, the alignment of the IDM to the patient side mount may not require the fiducial marker. For instance, the patient side mount may be recognized using segmentation, and/or object recognition method as described elsewhere herein without the fiducial marker. In some cases, a 3D fiducial marker may be utilized to align the IDM to the patient side mount independent of using 3D point cloud. For example, sequence of image frames or video may be acquired containing the fiducial marker and may be processed to identify a spatial relationship (e.g., orientation, position) between the imaging device and patient side mount (i.e., fiducial marker). Based on a known geometric relationship between the IDM and the imaging device, the spatial relationship between the IDM and the patient side mount may be derived. Alternatively, the fiducial marker may be used in conjunction with a 3D depth map. For example, the object identity (e.g., patient side mount) may be identified by the fiducial marker and the depth data may be assigned to the object based on the 3D depth map. In some cases, the 3D depth map may include a 3D point cloud. Alternatively, the 3D depth map may be generated based on the optical image data. the 3D depth map may comprise at least an object (e.g., fiducial marker) with depth information obtained using optical method as described above.</p><p>[0078] In some cases, the imaging device may be used in conjunction with other types of sensors (e.g., proximity sensor, location sensor, positional sensor, etc.) to improve accuracy of the location information. For instance, the sensing data may further comprise sensor data from one or more proximity sensors. The proximity sensors can be any suitable type such as ultrasonic sensor (e.g., a wide angle sensor, an array sensor) or light detection and ranging (Lidar) sensor. Lidar can be used to obtain three-dimensional information of an environment by measuring distances to objects. The proximity sensors can also be disposed at the robotic endoscope system. The proximity sensors can be located near the vision sensors. Alternatively, the proximity sensors can be situated on a portion of the robotic endoscope system different from the portions used to carry the vision sensors.</p><p>[0079] In some cases, the 3D depth map may be generated using a single modality sensor data (e.g., image data, Lidar, proximity data, etc.). Alternatively, the 3D depth map may be generated using multi-modality data. For example, the image data and 3D point cloud generated by the Lidar system may be fused using Kalman filter or deep learning model to generate a 3D map. The 3D map may then be used for automatic alignment of the IDM, self-positioning of the robotic car, collision avoidance and various other functions as described elsewhere herein.</p><p>[0080] In some embodiments, the autonomous alignment of the IDM to a patient side mount may be based on positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the IDM and electromagnetic coils located on the patient side mount along with an electromagnetic tracking system. The position and orientation of the IDM and the patient side mount may be detected, and the difference may be used to generate a command to move robotic arm thereby achieving an autonomous alignment between the IDM and patient side mount.</p><p>[0081] As shown in FIG. <b>4</b>B, the system may comprise an EM field generator <b>415</b> that transmits an EM field in the environment. The EM field generator may be positioned next to the patient torso during procedure, such as on the bed <b>407</b>, on the robotic cart, or any other suitable place in the environment <b>410</b>. The system may comprise a first EM sensor <b>411</b> located at the IDM to measure position and orientation of the IDM, and a second EM sensor <b>413</b> located at the patient side mount to measure position and orientation of the patient side mount. FIG. <b>4</b>C shows an example of the patient side mount <b>431</b> and the associated EM sensors <b>430</b>. Referring back to FIG. <b>4</b>B, the EM field generator and the two sets of EM sensors <b>411</b>, <b>413</b> may be utilized by the system to locate the position and orientation of the IDM and the patient side mount in 3D space.</p><p>[0082] The position and orientation measured by the two sets of EM sensors may be expressed in the filed-generator frame. The EM sensor may be 6 DOF sensor (e.g., X, Y, Z, Roll, Pitch, Yaw) that is able to sense the EM signals generated by the EM field generator and measures the six degrees of freedom spatial data for the IDM and the patient-side-mount. Alternatively, a pair a 5 DOF EM sensors may be located at the IDM and/or the patient side mount to measure the position and orientation. For example, a single 5DOF sensor may generate signals in X, Y, Z, Pitch, and Yaw, without Roll. By placing 2 5DOF sensors inside a device such that they are fixed relative to each other and their center axes are not in parallel with each other, the pair of 5 DOF signals can be used to calculate roll information.</p><p>[0083] The spatial data about the IDM and the patient side mount may then be processed by the system to determine if the cart is properly positioned for auto-alignment and move the IDM to align with the patient-side-mount such as using a closed-loop-feedback (e.g., controlling robot arm movements to correct the IDM position/orientation based on EM sensor data) and/or other control method as described elsewhere herein.</p><p>[0084] FIG. <b>5</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to the patient side mount. In the illustrated example <b>510</b>, the instrument drive mechanism may be initially positioned not in alignment with the patient side mount. During setup procedure, actuators of the plurality of links/joints of the robotic arm may be actuated and automatically align the IDM to the patient side mount <b>520</b>. In some cases, after alignment, the IDM may be moved to a proper position at a pre-determined distance <b>610</b> from the patient side mount <b>600</b> for loading an instrument such as the catheter assembly or endoscope device, as shown in FIG. <b>6</b>. For example, a flexible endoscope apparatus may be coupled to the IDM at the proximal end of the endoscope and coupled to the patient side mount via the connector at the distal end. In some cases, the pre-determined distance <b>610</b> may be generated based on a dimension of the endoscope device or empirical data. A user may be permitted to further manually adjust the position of the IDM as needed by switching the robotic arm into a passive mode. For example, once the passive model is enabled, the robotic arm can be placed into any configuration, position or orientation by a user applying force directly and will maintain the desired position and orientation.</p><p>[0085] As described above, the robotic arm may have redundant degrees of freedom. For instance, the robotic arm may have six, seven, eight, nine or more degrees of freedom (DOF) such that the IDM is able to be oriented in five or six degree of freedom (DOF) space. For example, the robotic arm end effector (e.g., IDM) that can be positioned with six degrees of freedom may in some cases have nine degrees of freedom (six end effector degrees of freedom-three for location, and three for orientation-plus three degrees of freedom to comply with the access site constraints), or ten or more degrees of freedom. Highly configurable robotic arm having more degrees of freedom than are needed for a given end effector position can beneficially sufficient degrees of freedom to allow a range of joint states for an end effector position in a workspace. During the procedure, if there is any motion of the patient side mount, the instrument drive mechanism may be able to auto-adjust accordingly to retain alignment with the patient side mount, eliminating any interruptions to the procedural workflow and avoiding misalignment.</p><p>[0086] In some embodiments, the 3D depth map generated by the platform may be used for automatic mobile/robotic cart placement. For example, the 3D depth map may comprise description about the operating environment such as identification of equipment, patient bed, human operator, patient and the like and such 3D depth map can be used to generate an optimal location of the mobile cart relative to the patient bed. As described above, computer vision (CV) techniques or computer vision systems may be used to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. The 3D map information and sensor data (e.g., proximity sensor, imaging sensor) may be used to detect whether the robotic cart is within an optimal zone with respect to the patient bed.</p><p>[0087] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed. In some embodiments, a propulsion unit may include a plurality of wheels that may permit the robotic cart to roll over an underlying surface. In some examples, two, three or four wheels may be provided which may permit the robotic cart to stand stably while not moving. In some instances, stabilization may occur with aid of one or more wheels or other stabilization platforms, such as gyroscopic platforms. The wheels may vary in size or be the same size. In some cases, the wheels can have a diameter of at least about 1 cm, 2 cm, 3 cm, 4 cm, 5 cm, 8 cm, 9 cm, 10 cm, 15 cm, 20 cm, 25 cm, 30 cm, 35 cm, 40 cm, 45 cm, 50 cm, 55 cm, 60 cm, 65 cm, 70 cm, 75 cm, 80 cm, 85 cm, 90 cm, 95 cm, 100 cm, 150 cm, or 200 cm. The wheels can have a smooth or treaded surface. The wheels may also permit the robotic cart to move laterally and/or rotate in place. The robotic cart may be capable of making any combination of translational or rotational movement. The propulsion unit may be driven with aid of one or more actuators. For example, a motor, engine, drive train, or any other component may be provided that may aid in driving the propulsion of the robotic cart.</p><p>[0088] Based on the 3D depth map, an optimal location of the robotic cart may be generated. The optimal location may be generated based on a dimension of the robotic cart, the dimension of the robotic arm (workspace), the dimension of the endoscope device and the 3D depth map. Real-time sensor data (e.g., proximity sensor) may be collected and may be used to determine whether the robotic cart is in the proper location relative to the patient bed. As shown in FIG. <b>8</b>, when the self-propelled robotic cart is detected not in a proper location with respect to the patient bed <b>810</b>, the robotic cart may automatically move to the proper location <b>820</b>. The proper location, the movement speed, moving acceleration, and the movement trajectory may be calculated by one or more processors of the platform based at least in part on the 3D depth map.</p><p>[0089] Alternatively or additionally, the system may inform the user of an non-optimal placement of the robotic cart and may prompt the user to intervene, For instance, message, warning or notification may be displayed on the screen along with recommendations for placing the robotic cart (e.g., specify that the cart should be closer to the bed) and/or displaying a 2D/3D depth map of the operating environment and the robotic system. For example, the robotic system may generate a preferred relative cart position and orientation, respective to the current cart position and display an animation to a user to guide the user changing the position of the robotic cart.</p><p>[0090] FIG. <b>9</b> shows an example process <b>900</b> for autonomous alignment of the robotic endoscope system. It should be noted that in the illustrated process though Lidar data (e.g., 3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. In the exemplary process, runtime sensor data (e.g., 3D point cloud) may be captured as input <b>901</b>. The runtime sensor data may include data captured by a Lidar (light detection and ranging). The Lidar may obtain three-dimensional information of the operating environment/scene by measuring distances to objects. For example, the emitting apparatus of a Lidar system may generate a sequence of light pulses emitted within short time durations such that the sequence of light pulses may be used to derive a distance measurement point. The Lidar system may provide three-dimensional (3D) imaging (e.g., 3D point cloud). In some cases, the 3D point cloud or the 3D images may be further processed by one or more processors of the robotic endoscope system for obstacles detection or collision avoidance <b>903</b>. Various suitable image processing method (e.g., image segmentation) may be utilized to recognize an object such as the patient bed. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the patient bed).</p><p>[0091] The positions and orientations of the Lidar sensor may be obtained based on kinematic mapping and such information along with the depth information of the bed is used to estimate relative orientation and position between the bed and the robotic endoscope system. Based on the relative position and orientation, a movement path for moving the robotic cart to an optimal placement with respect to the patient bed may be generated. The optimal placement may be generated by the system automatically without user intervention. In some cases, instead of or in addition to performing the robotic cart self-placement, the algorithm may inform a user an invalid location of the robotic cart relative to the patient bed and a property location of the robotic cart may be displayed to the user on a GUI. The user may be able to provide input via a GUI indicating a selection of the property placement of the robotic cart. Upon receiving the user confirmation, the system may generate the movement path for the robotic cart. Alternatively, a user may be guided to manually move the robotic cart to a desired place as described above.</p><p>[0092] As described above, any suitable method may be employed to process the real-time sensor data (e.g., 3D point cloud) for segmentation, object recognition and collision avoidance. One or more objects may be segmented in the workspace of the robotic arm. For example, deep learning techniques such as an automated pipeline engine may be provided for processing the lidar data. The pipeline engine may comprise multiple components or layers. The pipeline engine may be configured to preprocess continuous streams of raw Lidar data or batch data transmitted from a Lidar system. In some cases, data may be processed so it can be fed into machine learning analyses. In some cases, data may be processed to provide details at different understanding levels, which understanding may include, by way of non-limiting example, dimensions, weight, composition, identity, degree of collision risk, mobility, and so forth. In some case, the pipeline engine may comprise multiple components to perform different functions for extracting different levels of information from the 3D point cloud data. In some cases, the pipeline engine may further include basic data processing such as, data normalization, labeling data with metadata, tagging, data alignment, data segmentation, and various others. In some cases, the processing methodology is programmable through APIs by the developers constructing the pipeline.</p><p>[0093] In some embodiments, the pipeline engine may utilize machine learning techniques for processing data. In some embodiments, raw Lidar data may be supplied to a first layer of the pipeline engine which may employ a deep learning architecture to extract primitives, such as edges, corners, surfaces, of one or more target objects. In some cases, the deep learning architecture may be a convolutional neuron network (CNN). CNN systems commonly are composed of layers of different types: convolution, pooling, upscaling, and fully-connected neuron network. In some cases, an activation function such as rectified linear unit may be used in some of the layers. In a CNN system, there can be one or more layers for each type of operation. The input data of the CNN system may be the data to be analyzed such as 3D radar data. The simplest architecture of a convolutional neural networks starts with an input layer (e.g., images) followed by a sequence of convolutional layers and pooling layers, and ends with fully-connected layers. In some cases, the convolutional layers are followed by a layer of ReLU activation function. Other activation functions can also be used, for example the saturating hyperbolic tangent, identity, binary step, logistic, arcTan, softsign, parameteric rectified linear unit, exponential linear unit. softPlus, bent identity, softExponential, Sinusoid, Sinc, Gaussian, the sigmoid function and various others. The convolutional, pooling and ReLU layers may act as learnable features extractors, while the fully connected layers acts as a machine learning classifier.</p><p>[0094] In some cases, the convolutional layers and fully-connected layers may include parameters or weights. These parameters or weights can be learned in a training phase. The parameters may be trained with gradient descent so that the class scores that the CNN computes are consistent with the labels in the training set for each 3D point cloud image. The parameters may be obtained from a back propagation neural network training process that may or may not be performed using the same hardware as the production or application process.</p><p>[0095] A convolution layer may comprise one or more filters. These filters will activate when they see same specific structure in the input data. In some cases, the input data may be 3D images, and in the convolution layer one or more filter operations may be applied to the pixels of the image. A convolution layer may comprise a set of learnable filters that slide over the image spatially, computing dot products between the entries of the filter and the input image. The filter operations may be implemented as convolution of a kernel over the entire image. A kernel may comprise one or more parameters. Results of the filter operations may be summed together across channels to provide an output from the convolution layer to the next pooling layer. A convolution layer may perform high-dimension convolutions. For example, the three-dimensional feature maps or input 3D data are processed by a group of three-dimensional kernels in a convolution layer.</p><p>[0096] The output produced by the first layer of the pipeline engine may be supplied to a second layer which is configured to extract understanding of a target object such as shapes, materials, sub-surface structure and the like. In some cases, the second layer can also be implemented using a machine learning architecture.</p><p>[0097] The output produced by the second layer may then be supplied to a third layer of the pipeline engine which is configured for perform interpretations and decision makings, such as object recognition, separation, segmentation, collision avoidance, target dynamics (e.g., mobility), identity recognition, type classification and the like. In some cases, the dynamics or mobility of an object may be used for determining a collision avoidance scheme.</p><p>[0098] The pipeline engine described herein can be implemented by one or more processors. In some embodiments, the one or more processors may be a programmable processor (e.g., a central processing unit (CPU), a graphic processing unit (GPU), a general-purpose processing unit or a microcontroller), in the form of fine-grained spatial architectures such as a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), and/or one or more Advanced RISC Machine (ARM) processors. In some embodiments, the processor may be a processing unit of a computer system.</p><p>[0099] In some cases, to mitigate for noise effects, Bayesian estimation techniques (e.g. Kalman filtering) is applied to fit a point cloud to a rigid model of the bed for aligning the robotic cart to the bed. In some cases, if the target object (e.g., patient bed) is not detected <b>907</b>, the process may proceed with informing user that the robotic cart position is invalid (e.g., out of the proper region) <b>921</b>. For example, if the bed is not detected, a notification may be displayed on the GUI to inform the user that the cart position is invalid.</p><p>[0100] If a target object such as a patient bed is detected <b>907</b>, the method may proceed with model coordinate registration <b>909</b> by mapping the position and orientation of the patient bed model to the coordinates of the robotic system (e.g., via a pre-registration of the sensor and robotic system coordinate frames). Next, the method may comprise an algorithm to determine whether the system is in an acceptable workspace. As an example, the algorithm may include computing the position and orientation error <b>911</b> and compare it against a threshold to determine whether the system is in an acceptable workspace.</p><p>[0101] In some cases, the system may provide both an autonomous mode and a manual positioning mode. If autonomous mode once is enabled (e.g., selected by a user) <b>913</b>, the system may execute an active system positioning algorithm. The active system positioning algorithm may, for example, perform path planning <b>915</b> to generate a path from current position to a target position at a target orientation, with proper moving speed, acceleration, and the like. For instance, a robotic cart wheel path plan is generated, or updated, to eliminate the relative orientation error between the desired and the sensed orientation that is computed in the previous operation <b>911</b>. The path is executed <b>917</b> and the robotic wheel motion is controlled to move the robotic cart to the desired orientation and position.</p><p>[0102] If the system disables the active system positioning algorithm such as in a semi-autonomous mode <b>913</b>, the user may be informed of the state of validity of the system positioning relative to the bed e.g., acceptable or not acceptable position <b>919</b>, and the user may manually control the position of the robotic cart.</p><p>[0103] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance. As described above, the real-time 3D map of the operating environment may comprise obstacle information and relative location to one or more components of the robotic endoscope system. The system may determine a proximity threshold and may autonomously move one or more components of the system to avoid collision. For example, as shown in FIG. <b>10</b>, when the robotic arm is actuated during the procedure, the sensor may detect unwanted proximity between the arm and the monitor of the system <b>1000</b>. The proximity is detected via the analysis of sensor signals that may contain depth information. In response to the proximity, the user may be informed of the potential collision via a UI warning. Alternatively or additionally, the robotic arm may be automatically moved away from the monitor, or the monitor may be actuated away from the arm <b>1010</b>. For example, the monitor may be mounted to a robotic support that can be actuated to move the monitor in response to control signals. As illustrated in the example <b>1010</b>, upon detection of potential collision between the monitor and any portion of the robotic arm, both the robotic support for the monitor and the robotic arm may be actuated to move away from each other. Alternatively, as illustrated in FIG. <b>11</b>, upon detection of potential collision between the monitor and any portion of the robotic arm <b>1100</b>, the robotic arm may be actuated to move away from the monitor.</p><p>[0104] FIG. <b>11</b> shows an example when an operator moves a monitor into the workspace of the robotic arm, the system may detect a proximity to the monitor is approaching a threshold and may automatically change a configuration of the robotic arm. The redundancy of the robotic arm may beneficially allow for the change of configuration of the robotic arm while maintaining the position and orientation of the IDM. FIG. <b>12</b> shows an example that during the procedure, equipment such as the fluoroscopy C-arm <b>1207</b> is brought into the workspace. The system may detect the equipment <b>1207</b> and a proximity to the equipment, then automatically adjust a configuration of the robotic arm and/or the robotic cart to provide enough clearance for the C-arm to take fluoroscopy images of the patient and/or complete sweep(s) for tomosynthesis. The configuration of the robotic arm may change from a first configuration <b>1203</b> to a second configuration <b>1205</b> while the position and orientation of IDM <b>1201</b> remain the same.</p><p>[0105] FIG. <b>13</b> shows an example of a collision avoidance algorithm <b>1300</b>. It should be noted that in the illustrated process though Lidar data (3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. The steps about capturing the 3D point cloud data and segmentation of object within a workspace to detect an object can be the same as those described in FIG. <b>9</b>. For example, the input data may comprise 3D point cloud data captured by Lidar device <b>1301</b> and deep learning techniques as described above may be employed to perform object segmentation, object recognition <b>1303</b> for collision avoidance. For instance, when an object (e.g., display device, monitor) is brought into the operating environment, a segmentation algorithm may be executed to detect and recognize the object. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the monitor).</p><p>[0106] In some cases, signal filtering is performed to mitigate effects of sensing uncertainty <b>1305</b>. In some cases, when the object being detected can be represented by a precomputed model, Bayesian filtering methods, such as Kalman filtering, may be applied. For example, the precomputed model may include an estimation algorithm for an object, where the sensed point cloud of a monitor is an input to the estimation algorithm which estimates the state (position, orientation) of a monitoring device.</p><p>[0107] Next, position and orientation of the point cloud are mapped to arm base coordinate frame <b>1307</b>. Position of components of the robotic endoscope system such as the robotic arm and position of the points in the segmented monitor region may be registered in the same frame of reference for determining a minimum proximity between any portion of the monitor and any portion of the robotic arm (computing distance-to-contact) <b>1309</b>. The positions and orientations of the robotic arm may be obtained based on kinematic mapping.</p><p>[0108] In some cases, a proximity determination algorithm may be executed to determine a potential collision whether the minimum proximity violates a predetermined proximity threshold <b>1311</b>. In some cases, upon determine a violation of the predetermined proximity threshold, the system may activate an admittance controller to reconfigure a configuration of the robotic arm (e.g., by actuating one or more joints/links of the robotic arm) <b>1313</b>. For example, the admittance controller may execute an admittance control algorithm which defines a proximal-most point of collision as repulsive force and reconfigure the robotic arm to increase the (minimum) distance between the robotic arm and the monitor. An algorithm is executed to reposition the arm for the purpose of avoiding a collision prior to occurring. In some cases, the admittance control algorithm may map a sensed proximity between any portion of the robotic arm and an object to a virtual force applied on the robotic arm and calculate an output motion for the robotic arm as if an equivalent contact force was applied to the robotic arm (through contact). For instance, the admittance control algorithm comprises mapping a shortest distance between a link of an arm and obstacle to an input to a commanded task-space velocity of the arm link. The Geometric Jacobian of the arm may be used to map a task-space motion command to arm joint commands in order to achieve arm motion which increases the distance between the arm and obstacle.</p><p>[0109] In the event of the robotic system having more degrees-of-freedom than the task, i.e. actuation redundancy existing, the avoidance may be implemented via a redundancy resolution algorithm. For exempt the elbow of the arm may be repositioned to prevent a collision without affecting the position and orientation of the end-effector. Such algorithm may be implemented by defining a secondary redundancy-resolution task where the arm is repositioned such that the distance between the arm and the object (prior to collision) is increased. In some cases, when there is more than one degree-of-freedom in redundancy, the robot command can be chosen to actuate the joint, for example, as one where the norm of joint rates is minimized.</p><p>[0110] In some embodiments, the robotic endoscope system may be capable of autonomously adjusting position of the IDM relative to the patient side mount based on buckling detection. When the flexible endoscope is pushed at the proximal end, during insertion of the flexible device into the anatomy, the flexible endoscope may deform when navigating through turns and buckling. The deformation may happen during insertion, as the flexible device may take on a minimum-energy shape, which may be a \\\"hugging\\\" of the shaft against tissue. The buckling may happen when resistance force is encountered in the distal portion of the shaft.</p><p>[0111] During retraction of the flexible device, the distance of the shaft that is \\\"lost to buckling\\\" may become slack when the system actuation direction is reversed (e.g., backward moving or retraction). This phenomenon will result in a perceived dead-zone or system delay (a user input to command movement of the robotic endoscope tip does not map directly to robotic endoscope tip motion). When an endoscope is buckled, a retraction of the endoscope may result in robotic actuation with little endoscope tip translation.</p><p>[0112] The prolapse or kink may result in potential damage as it may expose the sharp edges of the kinked elongate device and complicate the surgical procedure. Moreover, a bent or kinked elongate device may render the system losing location/shape control of the device during both insertion and retraction and it may block the passage of an instrument. Furthermore, a device that prolapses or kinks may not be able to provide adequate reach towards the target anatomy for performing the intended task.</p><p>[0113] The robotic endoscope system herein may employ a responsive insertion and retraction velocity control of the flexible endoscope. The responsive velocity control method herein may automatically correct for motion differences between the endoscope tip and a velocity command (e.g., instrument driving mechanism (IDM) command). In some cases, the velocity control of the endoscope may be performed in conjunction with the collision avoidance algorithm as described above such that the robotic arm may autonomously reconfigure to avoid collision with other objects while the velocity of the endoscope tip is maintained and not influenced.</p><p>[0114] Unlike the conventional methods for buckling detection based on the difference in the position of the endoscope device (e.g., expected position and measure tip position), the methods and systems herein may automatically correct the buckling/deformation during insertion and retraction based on the velocity measured at an endoscope tip and a velocity control command. This beneficially avoids shape sensing or using extra imaging approaches to determine the shape or position of the endoscope device.</p><p>[0115] During insertion of an endoscope device, a velocity command may be received by the endoscope device. The velocity command may be provided by a user input. For example, a user may provide a control instruction via a control interface of the endoscope device indicating a desired/expected velocity of the endoscope tip. The control interface may include various devices such as touchscreen monitors, joysticks, keyboards and other interactive devices. A user may be able to navigate and/or control the motion of the robotic arm and the motion (e.g., tip velocity) of the catheter using a user input device. The user input device can have any type user interactive component, such as a button, mouse, joystick, trackball, touchpad, pen, image capturing device, motion capture device, microphone, touchscreen, hand-held wrist gimbals, exoskeletal gloves, or other user interaction system such as virtual reality systems, augmented reality systems and the like.</p><p>[0116] The user input for commanding a velocity of the endoscope tip may be received via the input device. For instance, a press on a joystick may be mapped to an analog value indicating a speed/velocity of the tip. For example, half press on the joystick may be mapped to 3 mm/s, full press may be mapped to 6 mm/s and no press may be mapped to 0 mm/s.</p><p>[0117] Based on the specific user input device, the user input may be processed to be converted to a desired/commanded velocity of the tip of the catheter/endoscope. Next, a tip velocity error is computed. The tip velocity error is the difference between the desired/commanded tip velocity and the velocity of the endoscope tip. In some embodiments, the velocity of the endoscope tip may be measured based on sensor data. In some cases, the sensor data may comprise position and orientation information of the distal tip of the endoscope.</p><p>[0118] In some cases, the sensor signals may be acquired by positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the distal end used with an electromagnetic tracking system to detect the position and orientation of the distal end of the endoscope. For example, positioning sensors such as electromagnetic (EM) sensors may be embedded at the distal tip of the catheter and an EM field generator may be positioned next to the patient torso during procedure. The EM field generator may locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. The endoscope tip position measured by the EM sensor p<sub>e</sub>=EM sensor (tip) position, may be expressed in field-generator frame.</p><p>[0119] Next, the linear velocity of the endoscope tip may be computed. The linear velocity may be computed using time derivative method such as backward Euler derivative. In some cases, the endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction. This projection may be required because the user provided velocity command is based on integrated position changes that occur in the direction of n. In some cases, a low pass filter may be applied to generate the filtered time derivative data.</p><p>[0120] The endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction of n using a projection matrix v<sub>n</sub>=nn<sup>T </sup>filt(dp<sub>e</sub>/dt).</p><p>[0121] Where n is a unit vector that indicates the endoscope tip heading direction, expressed in field-generator frame. There may be mechanical offset between the EM sensor and the scope tip and the mechanical offset may be calibrated for each endoscope device. dp<sub>e</sub>/dt represents the time derivative of the endoscope tip, nn<sup>T </sup>is the projection matrix that maps the aforementioned velocity to the heading direction of the endoscope tip (i.e. velocities that are not in the direction of the heading are ignored). The velocity v<sub>n </sub>may not be affected by articulation as the endoscope tip translation owing to articulation is orthogonal to n.</p><p>[0122] After the endoscope tip velocity is computed, the endoscope tip velocity error may be computed and may be further processed for safety checks. In some cases, the safety checks may comprise a plurality of checks. For example, the plurality of checks may comprise determining whether the endoscope tip has been stationary (e.g., tip velocity is about zero) while the handle portion of the endoscope (e.g., IDM) has an insertion distance is beyond a distance threshold. In another example, the plurality of checks may comprise determining whether the endoscope tip is retracting (e.g., negative tip velocity error) while the handle portion of the endoscope (e.g., IDM) is inserting, if yes, a prolapse may be occurring. In a further example, the plurality of checks may comprise determining whether the insertion force is beyond a force threshold. The term \\\"insertion distance\\\" as utilized herein may refer to the distance along the navigation path.</p><p>[0123] The method may comprise a closed loop control of the tip velocity to reduce the tip velocity error. The tip velocity of the endoscope may be controlled based on the tip velocity error computed at operation which is used as the feedback signal for commanding the motion of the IDM. The user velocity input may be mapped to the command to control the motion of IDM (e.g., velocity to move the IDM along the insertion axis). The command may be control signals to control the motors of the robotic arm thereby controlling a motion of the IDM (i.e., proximal end of the endoscope). The endoscope tip velocity may be calculated based on the robotic arm inverse kinematics. The endoscope tip velocity is then used to calculate the tip velocity in the heading direction by projection in the heading direction of n using a projection matrix as described above. In some cases, the feedback signal may be the projection of tip velocity processed by a low-pass filter.</p><p>[0124] Based on the control algorithm, a motion command is generated to actuate the robotic arm thereby affecting a tip velocity of the endoscope. The effect on the motion of the IDM (e.g., insertion velocity, insertion distance) and on the motion of the endoscope tip (e.g., tip velocity) may not perfectly match due to the tortuosity of the navigation path, the buckling and/or prolapse as described above.</p><p>[0125] During retraction of the medical device, a previously determined deformation-loss during insertion may be used as a feed-forward term to perform a backslash compensation for the retraction control. This reduces the deformation-loss incurred during the insertion prior to the tip motion during the retraction.</p><p>[0126] FIG. <b>14</b> illustrates an example of a flexible endoscope <b>1400</b>, in accordance with some embodiments of the present disclosure. As shown in FIG. <b>14</b>, the flexible endoscope <b>1400</b> may comprise a handle/proximal portion <b>1409</b> and a flexible elongate member to be inserted inside of a subject. The flexible elongate member can be the same as the one described above. In some embodiments, the flexible elongate member may comprise a proximal shaft (e.g., insertion shaft <b>1401</b>), steerable tip (e.g., tip <b>1405</b>), and a steerable section (active bending section <b>1403</b>). The active bending section, and the proximal shaft section can be the same as those described elsewhere herein. The endoscope <b>1400</b> may also be referred to as steerable catheter assembly as described elsewhere herein. In some cases, the endoscope <b>1400</b> may be a single-use robotic endoscope. In some cases, the entire catheter assembly may be disposable. In some cases, at least a portion of the catheter assembly may be disposable. In some cases, the entire endoscope may be released from an instrument driving mechanism and can be disposed of. In some embodiment, the endoscope may contain varying levels of stiffness along the shaft, as to improve functional operation.</p><p>[0127] The endoscope or steerable catheter assembly <b>1400</b> may comprise a handle portion <b>1409</b> that may include one or more components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>1400</b> and an instrument driving mechanism (not shown), and any other external system or devices. In another example, the handle portion <b>1409</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera, electromagnetic sensor and LED lights) of the endoscope.</p><p>[0128] The one or more components located at the handle may be optimized such that expensive and complicated components may be allocated to the robotic support system, a hand-held controller or an instrument driving mechanism thereby reducing the cost and simplifying the design the disposable endoscope. The handle portion or proximal portion may provide an electrical and mechanical interface to allow for electrical communication and mechanical communication with the instrument driving mechanism. The instrument driving mechanism may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley/capstans assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0129] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0130] The electrical interface (e.g., printed circuit board) may allow image/video data and/or sensor data to be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. In some cases, the electrical interface may establish electrical communication without cables or wires. For example, the interface may comprise pins soldered onto an electronics board such as a printed circuit board (PCB). For instance, receptacle connector (e.g., the female connector) is provided on the instrument driving mechanism as the mating interface. This may beneficially allow the endoscope to be quickly plugged into the instrument driving mechanism or robotic support without utilizing extra cables. Such type of electrical interface may also serve as a mechanical interface such that when the handle portion is plugged into the instrument driving mechanism, both mechanical and electrical coupling is established. Alternatively or in addition to, the instrument driving mechanism may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0131] In some cases, the handle portion <b>1409</b> may comprise one or more mechanical control modules such as lure <b>1411</b> for interfacing the irrigation system/aspiration system. In some cases, the handle portion may include lever/knob for articulation control. Alternatively, the articulation control may be located at a separate controller attached to the handle portion via the instrument driving mechanism.</p><p>[0132] The endoscope may be attached to a robotic support system or a hand-held controller via the instrument driving mechanism. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>1400</b>. The mechanical interface may allow the steerable catheter assembly <b>1400</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0133] In the illustrated example, the distal tip of the catheter or endoscope shaft is configured to be articulated/bent in two or more degrees of freedom to provide a desired camera view or control the direction of the endoscope. As illustrated in the example, imaging device (e.g., camera), position sensors (e.g., electromagnetic sensor) <b>1407</b> is located at the tip of the catheter or endoscope shaft <b>1405</b>. For example, line of sight of the camera may be controlled by controlling the articulation of the active bending section <b>1403</b>. In some instances, the angle of the camera may be adjustable such that the line of sight can be adjusted without or in addition to articulating the distal tip of the catheter or endoscope shaft. For example, the camera may be oriented at an angle (e.g., tilt) with respect to the axial direction of the tip of the endoscope with aid of an optimal component.</p><p>[0134] The distal tip <b>1405</b> may be a rigid component that allow for positioning sensors such as electromagnetic (EM) sensors, imaging devices (e.g., camera) and other electronic components (e.g., LED light source) being embedded at the distal tip.</p><p>[0135] In real-time EM tracking, the EM sensor comprising of one or more sensor coils embedded in one or more locations and orientations in the medical instrument (e.g., tip of the endoscopic tool) measures the variation in the EM field created by one or more static EM field generators positioned at a location close to a patient. The location information detected by the EM sensors is stored as EM data. The EM field generator (or transmitter), may be placed close to the patient to create a low intensity magnetic field that the embedded sensor may detect. The magnetic field induces small currents in the sensor coils of the EM sensor, which may be analyzed to determine the distance and angle between the EM sensor and the EM field generator. For example, the EM field generator may be positioned close to the patient torso during procedure to locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. This may provide a visual guide to an operator when driving the bronchoscope towards the target site.</p><p>[0136] The endoscope may have a unique design in the elongate member. In some cases, the active bending section <b>1403</b>, and the proximal shaft of the endoscope may consist of a single tube that incorporates a series of cuts (e.g., reliefs, slits, etc.) along its length to allow for improved flexibility, a desirable stiffness as well as the anti-prolapse feature (e.g., features to define a minimum bend radius).</p><p>[0137] As described above, the active bending section <b>1403</b> may be designed to allow for bending in two or more degrees of freedom (e.g., articulation). A greater bending degree such as 180 and 270 degrees (or other articulation parameters for clinical indications) can be achieved by the unique structure of the active bending section. In some cases, a variable minimum bend radius along the axial axis of the elongate member may be provided such that an active bending section may comprise two or more different minimum bend radii.</p><p>[0138] The articulation of the endoscope may be controlled by applying force to the distal end of the endoscope via one or multiple pull wires. The one or more pull wires may be attached to the distal end of the endoscope. In the case of multiple pull wires, pulling one wire at a time may change the orientation of the distal tip to pitch up, down, left, right or any direction needed. In some cases, the pull wires may be anchored at the distal tip of the endoscope, running through the bending section, and entering the handle where they are coupled to a driving component (e.g., pulley). This handle pulley may interact with an output shaft from the robotic system.</p><p>[0139] In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, capstans, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end/portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0140] The pull wires may be made of any suitable material such as stainless steel (e.g., SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0141] The proximal design may improve the reliability of the device without introducing extra cost allowing for a low-cost single-use endoscope. In another aspect of the invention, a single-use robotic endoscope is provided. The robotic endoscope may be a bronchoscope and can be the same as the steerable catheter assembly as described elsewhere herein. Traditional endoscopes can be complex in design and are usually designed to be re-used after procedures, which require thorough cleaning, dis-infection, or sterilization after each procedure. The existing endoscopes are often designed with complex structures to ensure the endoscopes can endure the cleaning, dis-infection, and sterilization processes. The provided robotic bronchoscope can be a single-use endoscope that may beneficially reduce cross-contamination between patients and infections. In some cases, the robotic bronchoscope may be delivered to the medical practitioner in a pre-sterilized package and are intended to be disposed of after a single-use.</p><p>[0142] As shown in FIG. <b>15</b>, a robotic bronchoscope <b>1510</b> may comprise a handle portion <b>1513</b> and a flexible elongate member <b>1511</b>. In some embodiments, the flexible elongate member <b>1111</b> may comprise a shaft, steerable tip, and a steerable/active bending section. The robotic bronchoscope <b>1510</b> can be the same as the steerable catheter assembly as described in FIG. <b>14</b>. The robotic bronchoscope may be a single-use robotic endoscope. In some cases, only the catheter may be disposable. In some cases, at least a portion of the catheter may be disposable. In some cases, the entire robotic bronchoscope may be released from the instrument driving mechanism and can be disposed of. In some cases, the bronchoscope may contain varying levels of stiffness along its shaft, as to improve functional operation. In some cases, a minimum bend radius along the shaft may vary.</p><p>[0143] The robotic bronchoscope can be releasably coupled to an instrument driving mechanism <b>1520</b>. The instrument driving mechanism <b>1520</b> may be mounted to the arm of the robotic support system or to any actuated support system as described elsewhere herein. The instrument driving mechanism may provide mechanical and electrical interface to the robotic bronchoscope <b>1510</b>. The mechanical interface may allow the robotic bronchoscope <b>1510</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic bronchoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic bronchoscope may be coupled or released from the instrument driving mechanism manually without using a tool.</p><p>[0144] FIG. <b>16</b> shows an example of an instrument driving mechanism <b>1620</b> providing mechanical interface to the handle portion <b>1613</b> of the robotic bronchoscope. As shown in the example, the instrument driving mechanism <b>1620</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the flexible endoscope or catheter. The handle portion <b>1613</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies or capstans are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0145] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive, adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0146] FIG. <b>17</b> shows an example of a distal tip <b>1700</b> of an endoscope. In some cases, the distal portion or tip of the catheter <b>1700</b> may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). The catheter may comprise a tip portion, bending section, and insertion shaft. In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple sections having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments (e.g., cuts, patterns), adding additional supporting components or any combination of the above. In some embodiments, the catheter may have variable minimum bend radius along the longitudinal axis direction. The selection of different minimum bend radius at different location long the catheter may beneficially provide anti-prolapse capability while still allow the catheter to reach hard-to-reach regions. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0147] The distal portion of the catheter may be steered by one or more pull wires <b>1705</b>. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such that it can be bent by the pull wires. In some embodiments, the proximal end or terminal end of one or more pull wires <b>1705</b> may be coupled to a driving mechanism (e.g., gears, pulleys, capstan etc.) via the anchoring mechanism as described above.</p><p>[0148] The pull wire <b>1705</b> may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire <b>1705</b> can also be made of natural or organic materials or fibers. The pull wire <b>1705</b> can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires <b>1705</b> may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0149] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0150] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera <b>1713</b>. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise A/D converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0151] The illumination device may comprise one or more light sources <b>1711</b> positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0152] The imaging device and the illumination device may be integrated to the catheter. For example, the distal portion of the catheter may comprise suitable structures matching at least a dimension of the imaging device and the illumination device. The imaging device and the illumination device may be embedded into the catheter. FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device. A camera may be located at the distal portion. The distal tip may have a structure to receive the camera, illumination device and/or the location sensor. For example, the camera may be embedded into a cavity <b>1810</b> at the distal tip of the catheter. The cavity <b>1810</b> may be integrally formed with the distal portion of the cavity and may have a dimension matching a length/width of the camera such that the camera may not move relative to the catheter. The camera may be adjacent to the working channel <b>1820</b> of the catheter to provide near field view of the tissue or the organs. In some cases, the attitude or orientation of the imaging device may be controlled by controlling a rotational movement (e.g., roll) of the catheter.</p><p>[0153] The power to the camera may be provided by a wired cable. In some cases, the cable wire may be in a wire bundle providing power to the camera as well as illumination elements or other circuitry at the distal tip of the catheter. The camera and/or light source may be supplied with power from a power source located at the handle portion via wires, copper wires, or via any other suitable means running through the length of the catheter. In some cases, real-time images or video of the tissue or organ may be transmitted to an external user interface or display wirelessly. The wireless communication may be WiFi, Bluetooth, RF communication or other forms of communication. In some cases, images or videos captured by the camera may be broadcasted to a plurality of devices or systems. In some cases, image and/or video data from the camera may be transmitted down the length of the catheter to the processors situated in the handle portion via wires, copper wires, or via any other suitable means. The image or video data may be transmitted via the wireless communication component in the handle portion to an external device/system. In some cases, the system may be designed such that no wires are visible or exposed to operators.</p><p>[0154] In conventional endoscopy, illumination light may be provided by fiber cables that transfer the light of a light source located at the proximal end of the endoscope, to the distal end of the robotic endoscope. In some embodiments of the disclosure, miniaturized LED lights may be employed and embedded into the distal portion of the catheter to reduce the design complexity. In some cases, the distal portion may comprise a structure <b>1430</b> having a dimension matching a dimension of the miniaturized LED light source. As shown in the illustrated example, two cavities <b>1430</b> may be integrally formed with the catheter to receive two LED light sources. For instance, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm) and diameter of the working channel of the catheter may be around 2 mm such that two LED light sources may be embedded at the distal end. The outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool's dimensional or specific application. Any number of light sources may be included. The internal structure of the distal portion may be designed to fit any number of light sources.</p><p>[0155] In some cases, each of the LEDs may be connected to power wires which may run to the proximal handle. In some embodiment, the LEDs may be soldered to separated power wires that later bundle together to form a single strand. In some embodiments, the LEDs may be soldered to pull wires that supply power. In other embodiments, the LEDs may be crimped or connected directly to a single pair of power wires. In some cases, a protection layer such as a thin layer of biocompatible glue may be applied to the front surface of the LEDs to provide protection while allowing light emitted out. In some cases, an additional cover <b>1431</b> may be placed at the forwarding end face of the distal tip providing precise positioning of the LEDs as well as sufficient room for the glue. The cover <b>1831</b> may be composed of transparent material matching the refractive index of the glue so that the illumination light may not be obstructed.</p><p>[0156] While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. A method for controlling a robotic endoscope system, the method comprising: <BR />generating a 3D depth map of an environment surrounding the robotic endoscope system;<BR />autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and<BR />actuating the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The method of claim 1, wherein the 3D depth map is generated based at least in part on 3D point cloud data.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>3</b>. The method of claim 2, further comprising processing the 3D depth map to detect the patient bed and computing a position and orientation of the robotic support system relative to the patient bed.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>4</b>. The method of claim 1, wherein a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>5</b>. The method of claim 1, further comprising controlling a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"5\\\"><p><b>6</b>. The method of claim 5, further comprising loading a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"5\\\"><p><b>7</b>. The method of claim 5, further comprising automatically adjusting a position of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>8</b>. The method of claim 1, further comprising detecting and recognizing an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p><b>9</b>. The method of claim 8, further comprising detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>10</b>. The method of claim 9, further comprising executing a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>11</b>. The method of claim 1, wherein the IDM is autonomously aligned to the component based at least in part on sensor data.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p><b>12</b>. The method of claim 11, wherein the sensor data is captured by electromagnetic sensors.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p><b>13</b>. The method of claim 11, wherein the sensor data is captured by a camera including a fiducial marker placed on the component and wherein the 3D depth map comprises at least a 3D location of the fiducial marker.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"yes\\\"><p><b>14</b>. A system for controlling a robotic endoscope system, the system comprising: a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to: <BR />generate a 3D depth map of an environment surrounding the robotic endoscope system;<BR />autonomously actuate a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and<BR />actuate the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>15</b>. The system of claim 14, wherein the 3D depth map is generated based at least in part on 3D point cloud data.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>16</b>. The system of claim 14, wherein the one or more processors are further configured to process the 3D depth map to detect the patient bed and compute a position and orientation of the robotic support system relative to the patient bed.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>17</b>. The system of claim 14, wherein a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>18</b>. The system of claim 14, wherein the one or more processors are further configured to control a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p><b>19</b>. The system of claim 18, wherein the one or more processors are further configured to load a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p><b>20</b>. The system of claim 18, wherein the one or more processors are further configured to automatically adjust a position of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"21\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>21</b>. The system of claim 14, wherein the one or more processors are further configured to detect and recognize an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM.</p></Claim><Claim claimid=\\\"22\\\" independent=\\\"no\\\" parents=\\\"21\\\"><p><b>22</b>. The system of claim 21, wherein the one or more processors are further configured to detect a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"23\\\" independent=\\\"no\\\" parents=\\\"22\\\"><p><b>23</b>. The system of claim 22, wherein the one or more processors are further configured to execute a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p></Claim><Claim claimid=\\\"24\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>24</b>. The system of claim 14, wherein the IDM is autonomously aligned to the component based at least in part on sensor data.</p></Claim><Claim claimid=\\\"25\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>25</b>. The system of claim 14, wherein the sensor data is captured by electromagnetic sensors.</p></Claim><Claim claimid=\\\"26\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>26</b>. The system of claim 14, wherein the sensor data is captured by a camera including a fiducial marker placed on the component and wherein the 3D depth map comprises at least a 3D location of the fiducial marker.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/32\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B1/00097\",\n",
      "\n",
      "\"A61B1/0016\",\n",
      "\n",
      "\"A61B1/005\",\n",
      "\n",
      "\"A61B1/018\",\n",
      "\n",
      "\"A61B1/05\",\n",
      "\n",
      "\"A61B1/0676\",\n",
      "\n",
      "\"A61B1/0684\",\n",
      "\n",
      "\"A61B2034/107\",\n",
      "\n",
      "\"A61B2034/2051\",\n",
      "\n",
      "\"A61B2034/2055\",\n",
      "\n",
      "\"A61B2034/2074\",\n",
      "\n",
      "\"A61B2034/301\",\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/32\",\n",
      "\n",
      "\"A61B90/361\",\n",
      "\n",
      "\"A61B90/37\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"NOAH MEDICAL CORP\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20220524,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230913,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HUNG, CAROL KAYEE;SLAWINSKI, PIOTR ROBERT;THOMPSON, HENDRIK;AND OTHERS;SIGNING DATES FROM 20230712 TO 20230724;REEL/FRAME:064884/0349, Owner: NOAH MEDICAL CORPORATION, CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HUNG, CAROL KAYEE;SLAWINSKI, PIOTR ROBERT;THOMPSON, HENDRIK;AND OTHERS;SIGNING DATES FROM 20230712 TO 20230724;REEL/FRAME:064884/0349\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOAH MEDICAL CORPORATION, CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230913,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FEPP\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"FEE PAYMENT PROCEDURE\",\n",
      "\n",
      "\"details\": \"Text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230920,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FEPP\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"FEE PAYMENT PROCEDURE\",\n",
      "\n",
      "\"details\": \"Text: ENTITY STATUS SET TO SMALL (ORIGINAL EVENT CODE: SMAL); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ENTITY STATUS SET TO SMALL (ORIGINAL EVENT CODE: SMAL); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231110,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231110,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240118,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240126,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240126,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240305,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPV1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Verified\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240305,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240320,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PAT1\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"Patented Case\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230466010\",\n",
      "\n",
      "\"ad\": 20230913,\n",
      "\n",
      "\"pn\": \"US2024016557\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240320,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STCF\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT GRANT\",\n",
      "\n",
      "\"details\": \"Text: PATENTED CASE\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PATENTED CASE\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=100080311\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20240594287\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"SYSTEMS AND METHODS FOR SELF-ALIGNMENT AND ADJUSTMENT OF ROBOTIC ENDOSCOPE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE</p><p>[0001] This application is a continuation of U.S. patent application Ser. No. 18/466,010, filed Sep. 13, 2023, which is a continuation of International Patent Application No. PCT/US2023/023139, filed May 22, 2023, which claims priority to U.S. Provisional Patent Application No. 63/345,287, filed on May 24, 2022, and U.S. Provisional Patent Application No. 63/347,179, filed on May 31, 2022, each of which is entirely incorporated herein by reference.</p></relapp><shortsum><p>BACKGROUND OF THE INVENTION</p><p>[0002] Endoscopy procedures use an endoscope to examine the interior of a hollow organ or cavity of the body. Unlike many other medical imaging techniques, endoscopes are inserted into the organ directly. Flexible endoscope that can deliver instinctive steering and control is useful in diagnosing and treating diseases that are accessible through any natural orifice in the body. Depending on the clinical indication, the endoscope may be designated as bronchoscope, ureteroscope, colonoscope, gastroscope, ENT scope, and various others. For example, flexible bronchoscope may be used for lung cancer diagnosis and/or surgical treatment. However, one challenge in bronchoscopy is reaching the upper lobe of the lung while navigating through the airways. In another example, flexible endoscopy has been used to inspect and treat disorders of the gastrointestinal (GI) tract without the need for creating an opening on the patient's body. The endoscope is introduced via the mouth or anus into the upper or lower GI tracts respectively. A miniature camera at the distal end captures images of the GI wall that help the clinician in their diagnosis of the GI diseases. Simple surgical procedures (like polypectomy and biopsy) can be performed by introducing a flexible tool via a working channel to reach the site of interest at the distal end.</p><p>[0003] Endoscopes are traditionally made to be re-usable, which may require thorough cleaning, dis-infection, and/or sterilization after each procedure. In most cases, cleaning, dis-infection, and sterilization may be aggressive processes to kill germs and/or bacteria. Such procedures may also be harsh on the endoscopes themselves. Therefore, the designs of such re-usable endoscopes can often be complicated, especially to ensure that the endoscopes can survive such harsh cleaning, dis-infection, and sterilization protocols. Periodical maintenance and repairs for such re-usable endoscopes may often be needed.</p><p>[0004] Low cost, disposable medical devices designated for a single-use have become popular for instruments that are difficult to clean properly. Single-use, disposable devices may be packaged in sterile wrappers to avoid the risk of pathogenic cross-contamination of diseases such as HIV, hepatitis, and other pathogens. Hospitals generally welcome the convenience of single-use disposable products because they no longer have to be concerned with product age, overuse, breakage, malfunction, and sterilization. Traditional endoscopes often include a handle that operators use to maneuver the endoscope. For single-use endoscopes, the handle usually encloses the camera, expensive electronics, and mechanical structures at proximal end in order to transmit the video and allow the users to maneuver the endoscope via a user interface. This may lead to high cost of the handle for a single-use endoscope.</p><p>[0005] The process for setting up of medical robots can be time-consuming and challenging due to the number of accessories to set up and the complex workflows. The setup times can result in longer room turnover time, causing procedural delays. Examples of the challenging steps of the current system workflow are: (1) positioning the robotic system in a location that is appropriate for the procedure and (2) aligning the instrument drive mechanism to the patient side. These steps are challenging because the user has to consider the placement of other equipment in the room that is needed for the procedure and gain spatial understanding of the workspace of the robotic components while manually performing these steps. During the procedure, if there are any changes to the setup of the room, the user has to manually make adjustments to the system to accommodate.</p><p>SUMMARY OF THE INVENTION</p><p>[0006] Recognized herein is a need for a robotic endoscopic platform or system that allows for autonomous self-adjustment of the robotic endoscopic system in response to real-time operating environment. The present disclosure addresses the above need by providing methods and systems capable of detecting and tracking the system's operating environment (e.g., external environment surrounding the system) and automatically making adjustments to the system thereby (1) simplifying the workflow for an operator during system setup and/or during the procedure and (2) enabling system configurations that are more optimal than placements completed manually which may be deemed acceptable but non-optimal. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base station (e.g., robotic cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, etc.</p><p>[0007] In an aspect of the present disclosure, a method is provided for a robotic endoscope system. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and coupled to a flexible endoscope apparatus via an instrument driving mechanism (IDM) at a distal end; autonomously actuating arm for collision avoidance; and actuating the robotic arm to align the IDM to a component coupled to or part of the patient bed. The methods implemented for enabling autonomous repositioning of system components given 3D depth map inputs require algorithms for processing and filtering the image data, mapping image data to inputs for the robotics control algorithms, and algorithms for obstacle voidance.</p><p>[0008] Recognized also herein are devices and systems comprising endoscopes which may be disposable and may not require extensive cleaning procedures. The present disclosure provides low-cost, single-use articulatable endoscope for diagnosis and treatment in various applications such as bronchoscopy, urology, gynecology, arthroscopy, orthopedics, ENT, gastro-intestine endoscopy, neurosurgery, and various others. In some embodiments, the present disclosure provides a single-use, disposable, robotically controlled bronchoscope for use with a robotic system to enable diagnostic evaluation of lesions anywhere in the pulmonary anatomy. It should be noted that the provided endoscope systems can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0009] It should be noted that the provided autonomous configuration, alignment and collision avoidance methods, endoscope components and various components of the device can be used in various minimally invasive surgical procedures, therapeutic or diagnostic procedures that involve various types of tissue including heart, bladder and lung tissue, and in other anatomical regions of a patient's body such as a digestive system, including but not limited to the esophagus, liver, stomach, colon, urinary tract, or a respiratory system, including but not limited to the bronchus, the lung, and various others.</p><p>[0010] In an aspect, a method is provided for controlling a robotic endoscope system moving and operating in an environment. The method comprises: generating a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuating a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, where the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuating the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0011] In a related yet separate aspect, a system is provided for controlling a robotic endoscope system The system comprises: a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to: generate a 3D depth map of an environment surrounding the robotic endoscope system; autonomously actuate a self-propelled base of a robotic support system to a desired location relative to a patient bed based on the 3D depth map, wherein the robotic support system comprises a robotic arm coupled to the self-propelled base at a proximal end and an instrument driving mechanism (IDM) at a distal end; and actuate the robotic arm to autonomously align the IDM to a component coupled to or as a part of the patient bed.</p><p>[0012] In some embodiments, the 3D depth map is generated based at least in part on 3D point cloud data. in some cases, the method further comprises processing the 3D depth map to detect the patient bed and computing a position and orientation of the robotic support system relative to the patient bed. In some embodiments, a flexible endoscope apparatus is releasably coupled to the IDM after the IDM is aligned to the component coupled to or as a part of the patient bed.</p><p>[0013] In some embodiments, the method further comprises controlling a movement of the robotic arm to move the IDM to a predetermined distance from the component coupled to or as a part of the patient bed. In some cases, the method further comprises loading a flexible endoscope apparatus to be coupled to the IDM at a proximal end and coupled to the component at a distal end. In some cases, the method further comprises automatically adjusting a position of the IDM relative to the component upon detection of a buckling event.</p><p>[0014] In some embodiments, the method further comprises detecting and recognizing an object in the environment and reconfiguring the robotic arm to avoid collision with the object while maintaining a position and orientation of the IDM. In some cases, the method further comprises detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient. In some instances, the method further comprises executing a responsive velocity control algorithm to control a velocity of the tip of the flexible catheter while reconfiguring the robotic arm to avoid collision with the object.</p><p>[0015] In some embodiments, the IDM is autonomously aligned to the component based at least in part on sensor data. In some cases, the sensor data is captured by electromagnetic sensors. In some cases, the sensor data is captured by a camera including a fiducial marker placed on the component and the 3D depth map comprises at least a 3D location of the fiducial marker.</p><p>[0016] Additional aspects and advantages of the present disclosure will become readily apparent to those skilled in this art from the following detailed description, wherein only illustrative embodiments of the present disclosure are shown and described. As will be realized, the present disclosure is capable of other and different embodiments, and its several details are capable of modifications in various obvious respects, all without departing from the disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.</p><p>INCORPORATION BY REFERENCE</p><p>[0017] All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. To the extent publications and patents or patent applications incorporated by reference contradict the disclosure contained in the specification, the specification is intended to supersede and/or take precedence over any such contradictory material.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0018] The novel features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that sets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings (also \\\"Figure\\\" and \\\"FIG.\\\" herein), of which:</p><p>[0019] FIG. <b>1</b> schematically shows a robotic platform, in accordance with some embodiments of the present disclosure.</p><p>[0020] FIG. <b>2</b> shows an example of a robotic catheter assembly with an anti-buckling device.</p><p>[0021] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation.</p><p>[0022] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment using optical sensor;</p><p>[0023] FIGS. <b>4</b>B and <b>4</b>C shows an example of auto-alignment of IDM (instrument drive mechanism) based on EM sensor data.</p><p>[0024] FIG. <b>5</b> and FIG. <b>6</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to a patient side mount.</p><p>[0025] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed.</p><p>[0026] FIG. <b>8</b> shows an example of self-alignment of a robotic endoscope system.</p><p>[0027] FIG. <b>9</b> shows an example process for autonomous alignment of the robotic endoscope system.</p><p>[0028] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance of the robotic endoscope system with respect to one or more objects in the operating environment before and during a surgical operation.</p><p>[0029] FIG. <b>11</b> shows an example of a robotic bronchoscope comprising a handle portion and a flexible elongate member.</p><p>[0030] FIG. <b>12</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of the robotic bronchoscope.</p><p>[0031] FIG. <b>13</b> shows an example of a collision avoidance algorithm.</p><p>[0032] FIG. <b>14</b> and FIG. <b>15</b> show examples of a flexible endoscope.</p><p>[0033] FIG. <b>16</b> shows an example of an instrument driving mechanism providing mechanical interface to the handle portion of a robotic bronchoscope.</p><p>[0034] FIG. <b>17</b> shows an example of a distal tip of an endoscope.</p><p>[0035] FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device.</p></shortdesdrw><p>DETAILED DESCRIPTION OF THE INVENTION</p><p>[0036] While various embodiments of the invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions may occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed.</p><p>[0037] The embodiments disclosed herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. The disclosed embodiments can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0038] While exemplary embodiments will be primarily directed at a device or system for bronchoscopy, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in various anatomical regions of a patient's body. The provided device or system can be utilized in urology, gynecology, rhinology, otology, laryngoscopy, gastroenterology with the endoscopes, combined devices including endoscope and instruments, endoscopes with localization functions, one of skill in the art will appreciate that this is not intended to be limiting, and the devices described herein may be used for other therapeutic or diagnostic procedures and in other anatomical regions of a patient's body, such as such as brain, heart, lungs, intestines, eyes, skin, kidney, liver, pancreas, stomach, uterus, ovaries, testicles, bladder, ear, nose, mouth, soft tissues such as bone marrow, adipose tissue, muscle, glandular and mucosal tissue, spinal and nerve tissue, cartilage, hard biological tissues such as teeth, bone and the like, as well as body lumens and passages such as the sinuses, ureter, colon, esophagus, lung passages, blood vessels and throat, and various others, in the forms of: NeuroendoScope, EncephaloScope, OphthalmoScope, OtoScope, RhinoScope, LaryngoScope, GastroScope, EsophagoScope, BronchoScope, ThoracoScope, PleuroScope, AngioScope, MediastinoScope, NephroScope, GastroScope, DuodenoScope, CholeodoScope, CholangioScope, LaparoScope, AmioScope, UreteroScope, HysteroScope, CystoScope, ProctoScope, ColonoScope, ArthroScope, SialendoScope, Orthopedic Endoscopes, and others, in combination with various tools or instruments.</p><p>[0039] The systems and apparatuses herein can be combined in one or more of many ways to provide improved diagnosis and therapy to a patient. Systems and apparatuses provided herein can be combined with existing methods and apparatus to provide improved treatment, such as combination with known methods of pulmonary diagnosis, surgery and surgery of other tissues and organs, for example. It is to be understood that any one or more of the structures and steps as described herein can be combined with any one or more additional structures and steps of the methods and apparatus as described herein, the drawings and supporting text provide descriptions in accordance with embodiments.</p><p>[0040] Whenever the term \\\"at least,\\\" \\\"greater than,\\\" or \\\"greater than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"at least,\\\" \\\"greater than\\\" or \\\"greater than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, greater than or equal to 1, 2, or 3 is equivalent to greater than or equal to 1, greater than or equal to 2, or greater than or equal to 3.</p><p>[0041] Whenever the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" precedes the first numerical value in a series of two or more numerical values, the term \\\"no more than,\\\" \\\"less than,\\\" or \\\"less than or equal to\\\" applies to each of the numerical values in that series of numerical values. For example, less than or equal to 3, 2, or 1 is equivalent to less than or equal to 3, less than or equal to 2, or less than or equal to 1.</p><p>[0042] As used herein, the terms distal and proximal may generally refer to locations referenced from the apparatus, and can be opposite of anatomical references. For example, a distal location of a primary shaft or catheter may correspond to a proximal location of an elongate member of the patient, and a proximal location of the primary sheath or catheter may correspond to a distal location of the elongate member of the patient.</p><p>[0043] As described above, setting up a robotic endoscopic system can be time consuming and challenging due to the complexity of the operating environment, requirement for accurate alignment between the instrument and patient body part and various other reasons. The present disclosure provides methods and systems capable of detecting and tracking the system's operating environment and automatically making adjustments to the system thereby simplifying the workflow during system setup and/or during the procedure. In some embodiments, the autonomous processes and/or movement of the system may comprise alignment of an instrument drive mechanism (IDM) of the robotic endoscopic device to a patient-side mount, alignment of the robotic base (robot cart) relative to the patient or hospital suite equipment, recognition of other technologies/equipment and auto-configuration for compatibility, collision avoidance between the system and patient and/or other objects in the operating environment, self-adjusting the robotic arm or IDM based on real-time instrument buckling detection and monitoring of breathing/vitals of the patient, and other functions as described elsewhere herein.</p><p>[0044] The operating environment of the robotic endoscope system may comprise one or more objects. The robotic endoscope system may be capable of detecting the one or more objects in the operating environment, generating a 3D map with depth information, performing autonomous alignment of the instrument drive mechanism with respect to the patient bed or body part, and self-adjusting its placement and configuration to avoid collision with the one or more objects. The one or more objects may comprise, for example, system accessories (e.g., system monitor, a monitor pole), external monitors, patient bed, patient, imaging equipment (e.g., fluoroscopy c-arm), anesthesia cart and other equipment or subject (e.g., operator, surgeon) in the operating environment before or during surgical operation.</p><p>[0045] FIG. <b>1</b> schematically shows a robotic platform <b>100</b>. The platform may comprise a robotic endoscope system including one or more flexible articulatable surgical instruments <b>105</b>, and a support apparatus <b>110</b> such as a robotic manipulator (e.g., robotic arm) to drive, support, position or control the movements and/or operation of the robotic system. The robotic platform may further include peripheral devices and subsystems such as imaging systems that may assist and/or facilitate the navigation of the elongate member to the target site in the body of a subject <b>120</b>.</p><p>[0046] The robotic endoscope system is provided for performing surgical operations or diagnosis with improved performance at low cost. For example, the robotic endoscope system may comprise a steerable catheter that can be entirely disposable. As shown in FIG. <b>1</b>, the robotic endoscope system may comprise a steerable catheter assembly <b>105</b> and a robotic support system <b>110</b>, for supporting or carrying the steerable catheter assembly. The steerable catheter assembly can be an endoscope. In some embodiments, the steerable catheter assembly may be a single-use robotic endoscope. In some embodiments, the robotic endoscope system may comprise an instrument driving mechanism (IDM) <b>103</b> that is attached to the distal end of the robotic arm <b>107</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>105</b>. The mechanical interface may allow the steerable catheter assembly <b>105</b> to be releasably coupled to the instrument driving mechanism <b>103</b>. For instance, a handle portion <b>104</b> of the steerable catheter assembly can be attached to the instrument driving mechanism (IDM) via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool. The instrument driving mechanism may be used to control the elongate member or robotic catheter assembly in two or more degrees of freedom (e.g., articulation).</p><p>[0047] The robotic support system <b>110</b> may comprise a robotic arm <b>107</b> and a mobile base (e.g., robotic cart) <b>109</b>. The robotic arm <b>107</b> may initiate the positioning of the robotic catheter assembly or other robotic instrument. In some cases, a user interface, robotic control modules, and the robotic arm may be mounted to the mobile cart. The mobile cart may include various elements such as rechargeable power supply in electrical communication with an electric panel providing charging ports for portable electronic devices, converters, transformers and surge protectors for a plurality of AC and DC receptacles as power source for the on-board equipment including one or more computers storing application specific software for the user interface.</p><p>[0048] The robotic arm <b>107</b> may have redundant degrees of freedom allowing for its elbow to be algorithmically, or passively, moved into configurations that are convenient for an operator initiate the positioning of the robotic system or other robotic instrument. For example, the robotic arm may comprise a plurality of joints having redundant degrees of freedom such that the joints of the robotic arm can be driven through a range of differing configurations for a given end effector position (e.g., IDM position). The redundant degrees of freedom may beneficially allow the robotic arm to be self-adjusted to an optimal configuration to avoid collision with other object in the operating environment prior to or during a procedure. For example, the instrument drive mechanism may automatically align itself to a patient side mount (e.g., a support structure at the patient bed for holding and supporting the endoscope device in place) during setup procedure. During the setup procedure and the operation procedure, upon detection of motion of the patient side mount, the instrument drive mechanism (IDM) is able to auto-adjust accordingly to avoid collision while maintain the position of the IDM, eliminating any interruptions to the procedural workflow and avoiding misalignment. In another example, when the robotic arm is actuated during the setup procedure or surgical operation, the system may detect unwanted proximity between any portion of the robotic arm and other objects surrounding it (e.g., the monitor of the system) and the robotic arm may be automatically reconfigured, moved away from the monitor to avoid collision.</p><p>[0049] In some embodiments, in addition to the autonomous movement of the robotic arm such as automatically positioning the steerable catheter assembly <b>105</b> to an initial position (e.g., access point) to access the target tissue, the robot arm may be passively moved by an operator. In such case, an operator can push the arm in any position and the arm compliantly moves. The robotic arm can also be controlled in a compliant mode to improve human robot interaction. For example, the compliant motion control of the robot art may employ a collision avoidance strategy and the position-force control may be designed to save unnecessary energy consumption while reducing impact of possible collisions.</p><p>[0050] The steerable catheter assembly <b>105</b> may comprise a flexible elongate member that is coupled to a handle portion. The robotic endoscope system may comprise an anti-buckling device <b>101</b> for preventing the buckling of the elongate member during use.</p><p>[0051] FIG. <b>2</b> shows another example of a robotic catheter assembly with an anti-buckling device <b>201</b>. The steerable catheter assembly may comprise a handle portion <b>211</b> that may include components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>211</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly and the instrument driving mechanism <b>221</b>, and any other external system or devices. In another example, the handle portion <b>211</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>221</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>221</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0052] The steerable catheter assembly may comprise a flexible elongate member <b>213</b> (i.e., catheter) that is coupled to the handle portion <b>211</b>. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic endoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0053] The robotic endoscope can be releasably coupled to an instrument driving mechanism <b>221</b>. The instrument driving mechanism <b>221</b> may be mounted to the arm of the robotic support system or to any actuated support system as described above. The instrument driving mechanism may provide mechanical and electrical interface to the robotic endoscope. The mechanical interface may allow the robotic endoscope to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic endoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic endoscope may be coupled or released from the instrument driving mechanism manually without using a tool. In some embodiments, the instrument driving mechanism <b>221</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion <b>211</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the catheter.</p><p>[0054] The handle portion may be designed allowing the robotic endoscope to be disposable at reduced cost. For instance, classic manual and robotic endoscope may have a cable in the proximal end of the endoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as EM sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the endoscope. The provided robotic endoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic endoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0055] In some case, the handle portion may be housing or comprise components configured to process image data, provide power, or establish communication with other external devices. In some cases, the communication may be wireless communication. For example, the wireless communications may include Wi-Fi, radio communications, Bluetooth, IR communications, or other types of direct communications. Such wireless communication capability may allow the robotic bronchoscope function in a plug-and-play fashion and can be conveniently disposed after single use. In some cases, the handle portion may comprise circuitry elements such as power sources for powering the electronics (e.g., camera and LED light source) disposed within the robotic bronchoscope or catheter.</p><p>[0056] The handle portion may be designed in conjunction with the catheter such that cables or fibers can be eliminated. For instance, the catheter portion may employ a design having a single working channel allowing instruments to pass through the robotic bronchoscope, as well as low cost electronics such as a chip-on-tip camera, illumination sources such as light emitting diode (LED) and EM sensors located at optimal locations in accordance with the mechanical structure of the catheter. This may allow for a simplified design of the handle portion. For instance, by using LEDs for illumination, the termination at the handle portion can be based on electrical soldering or wire crimping alone. For example, the handle portion may include a proximal board where the camera cable, LED cable, and EM sensor cable terminate to while the proximal board connects to the interface of the handle portion and establishes the electrical connections to the instrument driving mechanism. As described above, the instrument driving mechanism is attached to the robot arm (robotic support system) and provide a mechanical and electrical interface to the handle portion. This may advantageously improve the assembly and implementation efficiency as well as simplify the manufacturing process and cost. In some cases, the handle portion along with the catheter may be disposed of after a single use.</p><p>[0057] In some embodiments, the steerable catheter assembly may have a substantially integral design that one or more components may be integral to the catheter thereby simplifying the assembly, manufacturing process while preserving the kinematic, dynamic performance of the steerable catheter. As shown in the example, the steerable catheter assembly may comprise an elongate member <b>213</b> or a probing portion that is brought into proximity to the tissue and/or area that is to be examined. The elongate member <b>213</b> may, in some cases, also be referred to as catheter. The catheter <b>213</b> may comprise internal structures such as a working channel allowing tools to be inserted through. As an example, the working channel may have a dimension such as diameter of around 2 mm to be compatible with standard tools. The working channel may have any other suitable dimensions based on the application.</p><p>[0058] The catheter <b>213</b> may be composed of suitable materials for desired flexibility or bending stiffness. In some cases, the materials of the catheter may be selected such that it may maintain structural support to the internal structures (e.g., working channel) as well as being substantially flexible (e.g., able to bend in various directions and orientations). For example, the catheter can be made of any suitable material such as urethane, vinyl (such as polyvinyl chloride), Nylon (such as vestamid, grillamid), pellethane, polyethylene, polypropylene, polycarbonate, polyester, silicon elastomer, acetate and so forth. In some cases, the materials may be polymer material, bio-compatible polymer material and the catheter may be sufficiently flexible to be advancing through a path with a small curvature without causing pain to a subject. In some cases, the catheter may comprise a sheath. The sheath may not be the same length of the catheter. The sheath may be shorter than the catheter to provide desired support. Alternatively, the catheter may be substantially a single-piece component.</p><p>[0059] In some case, the distal portion or tip of the catheter may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple segments having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments, adding additional supporting components or any combination of the above. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0060] The distal portion of the catheter may be steered by one or more pull wires. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such it can be bent by the pull wires. In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0061] As described above, the pull wires may be made of any suitable material such as stainless steel (e.g. SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0062] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter or the interstitials of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0063] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise AID converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0064] The illumination device may comprise one or more light sources positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0065] In some embodiments, the catheter may be designed to be flexible. When the flexible portions of catheter are inserted by extending mechanisms through endoscope into patients, one or more sections may bend or buckle.</p><p>[0066] The anti-buckling mechanism <b>201</b> may be coupled to the handle portion of the robotic endoscope to support the catheter. The anti-buckling mechanism is used for preventing buckling of the insertion shaft. The anti-buckling mechanism <b>201</b> may be a telescopic extending device with internal mechanism to achieve anti-buckling of catheter during the insertion and withdrawal. The anti-buckling mechanism may be detachably connected to the handle portion of the robotic bronchoscope at one end, and may be detachably connected to a support surface <b>203</b> at the other end. As shown in the example, the anti-buckling tube may be attached to a bracket on the instrument driving mechanism and may be removable and disposable after the procedure via quick release mechanism. In the examples illustrated in FIG. <b>2</b>, a support arm (e.g., ET tube mount support arm) may be supported by the robotic mobile cart that supports the endotracheal tube mount and provides a support surface for the distal end of the anti-buckling tube to press against as it is compressed. The support arm may be controlled to rotate, translate vertically up and down and/or may a boom arm that expands and contracts, such that it can be precisely positioned over the patients mouth and attached to the endotracheal tube mount. The support arm positioning may be synchronized with the movement of the robotic arm that it may track the location of the point of entry of the catheter.</p><p>[0067] The anti-buckling mechanism may require a relatively linear trajectory to be traveled. In some cases, such trajectory may be ensured via an alignment between the anti-buckling mechanism in a collapsed state and a patient-side connector. FIG. <b>1</b> shows an example of a patient side connector <b>121</b> and IDM <b>103</b>. For example, the patient-side connector may be fixed to a patient side mount <b>123</b> (e.g., attached to the patient bed). The alignment between the IDM and the patient side connector/mount may involve lining up a collapsed anti-buckling mechanism with the patient-side connector. The robotic arm may automatically move the IDM into a position such that the IDM is aligned to the patient-side connector. In some cases, the alignment may comprise generating a 3D depth map of the operating environment, moving the mobile cart into a desired position relative to the patient bed based on the depth map, and moving the IDM into a position and orientation in alignment with the patient-side connector based on a detection of the location/position of the patient-side connector.</p><p>[0068] FIG. <b>3</b> show examples of robotic endoscope (e.g., bronchoscopy) system capable of performing autonomous collision avoidance and self-alignment before and during operation <b>300</b>, <b>330</b>. The robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise a steerable catheter assembly <b>320</b> and a robotic support system <b>310</b>, for supporting or carrying the steerable catheter assembly. In some cases, the steerable catheter assembly may be a bronchoscope. The steerable catheter assembly can be the same as the endoscope device as described elsewhere herein. In some cases, the steerable catheter assembly may be a single-use robotic bronchoscope. In some embodiments, the robotic endoscope (e.g., bronchoscopy) system <b>300</b> may comprise an instrument driving mechanism (IDM) <b>313</b> that is attached to the end of the robotic arm <b>311</b> of the robotic support system. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>320</b>. The mechanical interface may allow the steerable catheter assembly <b>320</b> to be releasably coupled to the instrument driving mechanism. For instance, a handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0069] The steerable catheter assembly <b>320</b> may comprise a handle portion <b>323</b> that may include components configured to processing image data, provide power, or establish communication with other external devices. For instance, the handle portion <b>323</b> may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>320</b> and the instrument driving mechanism <b>313</b>, and any other external system or devices. In another example, the handle portion <b>323</b> may comprise circuitry elements such as power sources for powering the electronics (e.g. camera and LED lights) of the endoscope. In some cases, the handle portion may be in electrical communication with the instrument driving mechanism <b>313</b> via an electrical interface (e.g., printed circuit board) so that image/video data and/or sensor data can be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. Alternatively or in addition to, the instrument driving mechanism <b>313</b> may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0070] The steerable catheter assembly <b>320</b> may comprise a flexible elongate member <b>321</b> that is coupled to the handle portion. In some embodiments, the flexible elongate member may comprise a shaft, steerable tip and a steerable section. The steerable catheter assembly may be a single use robotic bronchoscope. In some cases, only the elongate member may be disposable. In some cases, at least a portion of the elongate member (e.g., shaft, steerable tip, etc) may be disposable. In some cases, the entire steerable catheter assembly <b>320</b> including the handle portion and the elongate member can be disposable. The flexible elongate member and the handle portion are designed such that the entire steerable catheter assembly can be disposed of at low cost.</p><p>[0071] In some embodiments, the provided bronchoscope system may also comprise accessories such as a user interface. As illustrated in the example system <b>330</b>, during operation, one or more components of the system such as a treatment interface module <b>331</b> (user console side) and/or a treatment control module <b>333</b> (patient and robot side) may be brought into the operating environment. The one or more components or accessories may be added or removed from the operating environment before or during a surgical operation. The robotic arm <b>311</b> may have redundant degrees of freedom such that the joints of the robotic arm can be driven into a range of differing configurations for a given end effector position. For example, upon detection of a treatment interface module <b>311</b>, the robotic arm <b>311</b> may automatically move into a different configuration to avoid collision with the treatment interface module while the distal end of the arm of the IDM <b>313</b> maintains a particular state (e.g., a given position or velocity of the end effector). Details about the collision avoidance is described later herein.</p><p>[0072] The treatment interface module may allow an operator or user to interact with the bronchoscope during surgical procedures. In some embodiments, the treatment control module <b>333</b> may be a hand-held controller. The treatment control module <b>333</b> may allow a user to control a velocity of the tip of the bronchoscope as described elsewhere herein. The treatment control module may, in some cases, comprise a proprietary user input device and one or more add-on elements removably coupled to an existing user device to improve user input experience. For instance, physical trackball or roller can replace or supplement the function of at least one of the virtual graphical element (e.g., navigational arrow displayed on touchpad) displayed on a graphical user interface (GUI) by giving it similar functionality to the graphical element which it replaces. Examples of user devices may include, but are not limited to, mobile devices, smartphones/cellphones, tablets, personal digital assistants (PDAs), laptop or notebook computers, desktop computers, media content players, and the like. Details about the user interface device and user console are described later herein.</p><p>[0073] The robotic endoscope platform herein may be able to detect one or more objects in the operating environment surrounding the robotic endoscope system. In the cases, the detection of the operating environment may comprise generating an obstacle map. An obstacle map may be a three-dimensional (3D) map describing positions of objects detected in the 3D space.</p><p>[0074] The 3D map of the operating environment may be constructed based on sensing data. In some cases, the sensing data is received from one or more vision sensors, including depth information for the environment. The vision sensor may comprise a camera, a video camera, a three-dimensional (3D) depth camera, a stereo camera, a depth camera, a Red Green Blue Depth (RGB-D) camera, a time-of-flight (TOF) camera, an infrared camera, a charge coupled device (CCD) image sensor, or a complementary metal oxide semiconductor (CMOS) image sensor. For example, the vision sensor can include only one camera (monocular vision sensor). Alternatively, the vision sensor can include two (binocular vision sensor) or more cameras. The vision sensors may be disposed on the robotic endoscope system such as the robotic cart, the monitor and the like. Alternatively or additionally, the vision sensors may not be disposed on the robotic endoscope system. For instance, the vision sensors may be disposed on the walls, ceilings or other places in the operating environment (e.g., room). In embodiments where multiple vision sensors are used, each sensor can be located on a different portion of the robotic endoscope system, and the disparity between the image data collected by each sensor can be used to provide depth information for the environment. Depth information can be used herein to refer to information regarding distances of one or more objects from the robotic endoscope system and/or sensor. In embodiments where a single vision sensor is used, depth information can be obtained by capturing image data for a plurality of different positions and orientations of the vision sensor, and then using suitable image analysis techniques (e.g., structure from motion) to reconstruct the depth information.</p><p>[0075] FIG. <b>4</b>A shows an example of constructing a 3D map of the operating environment <b>400</b> using optical sensor <b>401</b> such as a camera. As mentioned elsewhere herein, an operating environment of the system may generally comprise an environment external to the system (e.g., the room the system is in, within certain proximity, etc.) or the environment that the system can move or operate within. In some cases, the camera may be a plenoptic camera having a main lens and additional micro lens array (MLA). The plenoptic camera model may be used to calculate a depth map of the captured image data. In some cases, the image data captured by the camera may be grayscale image with depth information at each pixel coordinate (i.e., depth map). The camera may be calibrated such that intrinsic camera parameters such as focal length, focus distance, distance between the MLA and image sensor, pixel size and the like are obtained for improving the depth measurement accuracy. Other parameters such as distortion coefficients may also be calibrated to rectify the image for metric depth measurement.</p><p>[0076] In some cases, the image data may be received and processed by one or more processors of the robotic endoscope system. For example, pre-processing of the capture image data may be performed. In an embodiment, the pre-processing algorithm can include image processing algorithms, such as image smoothing, to mitigate the effect of sensor noise, or image histogram equalization to enhance the pixel intensity values. Next, optical approaches as described elsewhere herein may be employed to generate a depth map of the operating environment <b>400</b>. In some cases, computer vision (CV) techniques or computer vision systems may be used to process the sensing data to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. For example, the CV output data may be generated using passive methods that only require images. Passive methods may include, for example, object recognition, stereoscopy, monocular shape-from-motion, shape-from-shading, and Simultaneous Localization and Mapping (SLAM). Alternatively, active methods may be utilized which may require controlled light to be projected into the target scene and the active methods may include, for example structured light and Time-of-Flight (ToF). In some cases, computer vision techniques such as optical flow, computational stereo approaches, iterative method combined with predictive models, machine learning approaches, predictive filtering or any non-rigid registration methods may be used to generate the descriptions of the 3D scene.</p><p>[0077] In some cases, a fiducial marker <b>403</b> may be employed to align the IDM to a patient side mount. The fiducial marker may be placed on the patient bed or the patient side mount <b>405</b>. The fiducial marker may have a 2D shape or pattern. For example, the fiducial marker may be a 2D QR code, grids, or any asymmetric shape. By acquiring images of the 2D fiducial marker (e.g., from different angles), the location and orientation of the patient side mount that the fiducial marker is placed at in a camera frame can be determined (e.g., triangulation). Based on the known spatial relationship between the camera and the IDM, the orientation and location of the IDM with respect to the patient side mount can be calculated. Alternatively, the fiducial marker may be a 3D fiducial marker such that the marker is visible and discernable in a wide range of angels. For example, a 3D fiducial marker may be located within the view of the imaging system such that the fiducial marker is always discernable regardless the position of the optical sensor with respect to the patient bed or the marker. The fiducial marker(s) may have any suitable 2D/3D shape (non-isotropic) or pattern such that a projection of the fiducial mark(s) corresponding to a view/angle is discernable from that of another view/angle. Alternatively, the alignment of the IDM to the patient side mount may not require the fiducial marker. For instance, the patient side mount may be recognized using segmentation, and/or object recognition method as described elsewhere herein without the fiducial marker. In some cases, a 3D fiducial marker may be utilized to align the IDM to the patient side mount independent of using 3D point cloud. For example, sequence of image frames or video may be acquired containing the fiducial marker and may be processed to identify a spatial relationship (e.g., orientation, position) between the imaging device and patient side mount (i.e., fiducial marker). Based on a known geometric relationship between the IDM and the imaging device, the spatial relationship between the IDM and the patient side mount may be derived. Alternatively, the fiducial marker may be used in conjunction with a 3D depth map. For example, the object identity (e.g., patient side mount) may be identified by the fiducial marker and the depth data may be assigned to the object based on the 3D depth map. In some cases, the 3D depth map may include a 3D point cloud. Alternatively, the 3D depth map may be generated based on the optical image data. the 3D depth map may comprise at least an object (e.g., fiducial marker) with depth information obtained using optical method as described above.</p><p>[0078] In some cases, the imaging device may be used in conjunction with other types of sensors (e.g., proximity sensor, location sensor, positional sensor, etc.) to improve accuracy of the location information. For instance, the sensing data may further comprise sensor data from one or more proximity sensors. The proximity sensors can be any suitable type such as ultrasonic sensor (e.g., a wide angle sensor, an array sensor) or light detection and ranging (Lidar) sensor. Lidar can be used to obtain three-dimensional information of an environment by measuring distances to objects. The proximity sensors can also be disposed at the robotic endoscope system. The proximity sensors can be located near the vision sensors. Alternatively, the proximity sensors can be situated on a portion of the robotic endoscope system different from the portions used to carry the vision sensors.</p><p>[0079] In some cases, the 3D depth map may be generated using a single modality sensor data (e.g., image data, Lidar, proximity data, etc.). Alternatively, the 3D depth map may be generated using multi-modality data. For example, the image data and 3D point cloud generated by the Lidar system may be fused using Kalman filter or deep learning model to generate a 3D map. The 3D map may then be used for automatic alignment of the IDM, self-positioning of the robotic car, collision avoidance and various other functions as described elsewhere herein.</p><p>[0080] In some embodiments, the autonomous alignment of the IDM to a patient side mount may be based on positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the IDM and electromagnetic coils located on the patient side mount along with an electromagnetic tracking system. The position and orientation of the IDM and the patient side mount may be detected, and the difference may be used to generate a command to move robotic arm thereby achieving an autonomous alignment between the IDM and patient side mount.</p><p>[0081] As shown in FIG. <b>4</b>B, the system may comprise an EM field generator <b>415</b> that transmits an EM field in the environment. The EM field generator may be positioned next to the patient torso during procedure, such as on the bed <b>407</b>, on the robotic cart, or any other suitable place in the environment <b>410</b>. The system may comprise a first EM sensor <b>411</b> located at the IDM to measure position and orientation of the IDM, and a second EM sensor <b>413</b> located at the patient side mount to measure position and orientation of the patient side mount. FIG. <b>4</b>C shows an example of the patient side mount <b>431</b> and the associated EM sensors <b>430</b>. Referring back to FIG. <b>4</b>B, the EM field generator and the two sets of EM sensors <b>411</b>, <b>413</b> may be utilized by the system to locate the position and orientation of the IDM and the patient side mount in 3D space.</p><p>[0082] The position and orientation measured by the two sets of EM sensors may be expressed in the filed-generator frame. The EM sensor may be 6 DOF sensor (e.g., X, Y, Z, Roll, Pitch, Yaw) that is able to sense the EM signals generated by the EM field generator and measures the six degrees of freedom spatial data for the IDM and the patient-side-mount. Alternatively, a pair a 5 DOF EM sensors may be located at the IDM and/or the patient side mount to measure the position and orientation. For example, a single 5DOF sensor may generate signals in X, Y, Z, Pitch, and Yaw, without Roll. By placing 2 5DOF sensors inside a device such that they are fixed relative to each other and their center axes are not in parallel with each other, the pair of 5 DOF signals can be used to calculate roll information.</p><p>[0083] The spatial data about the IDM and the patient side mount may then be processed by the system to determine if the cart is properly positioned for auto-alignment and move the IDM to align with the patient-side-mount such as using a closed-loop-feedback (e.g., controlling robot arm movements to correct the IDM position/orientation based on EM sensor data) and/or other control method as described elsewhere herein.</p><p>[0084] FIG. <b>5</b> shows an example of autonomous alignment of the IDM (instrument drive mechanism) to the patient side mount. In the illustrated example <b>510</b>, the instrument drive mechanism may be initially positioned not in alignment with the patient side mount. During setup procedure, actuators of the plurality of links/joints of the robotic arm may be actuated and automatically align the IDM to the patient side mount <b>520</b>. In some cases, after alignment, the IDM may be moved to a proper position at a pre-determined distance <b>610</b> from the patient side mount <b>600</b> for loading an instrument such as the catheter assembly or endoscope device, as shown in FIG. <b>6</b>. For example, a flexible endoscope apparatus may be coupled to the IDM at the proximal end of the endoscope and coupled to the patient side mount via the connector at the distal end. In some cases, the pre-determined distance <b>610</b> may be generated based on a dimension of the endoscope device or empirical data. A user may be permitted to further manually adjust the position of the IDM as needed by switching the robotic arm into a passive mode. For example, once the passive model is enabled, the robotic arm can be placed into any configuration, position or orientation by a user applying force directly and will maintain the desired position and orientation.</p><p>[0085] As described above, the robotic arm may have redundant degrees of freedom. For instance, the robotic arm may have six, seven, eight, nine or more degrees of freedom (DOF) such that the IDM is able to be oriented in five or six degree of freedom (DOF) space. For example, the robotic arm end effector (e.g., IDM) that can be positioned with six degrees of freedom may in some cases have nine degrees of freedom (six end effector degrees of freedom-three for location, and three for orientation-plus three degrees of freedom to comply with the access site constraints), or ten or more degrees of freedom. Highly configurable robotic arm having more degrees of freedom than are needed for a given end effector position can beneficially sufficient degrees of freedom to allow a range of joint states for an end effector position in a workspace. During the procedure, if there is any motion of the patient side mount, the instrument drive mechanism may be able to auto-adjust accordingly to retain alignment with the patient side mount, eliminating any interruptions to the procedural workflow and avoiding misalignment.</p><p>[0086] In some embodiments, the 3D depth map generated by the platform may be used for automatic mobile/robotic cart placement. For example, the 3D depth map may comprise description about the operating environment such as identification of equipment, patient bed, human operator, patient and the like and such 3D depth map can be used to generate an optimal location of the mobile cart relative to the patient bed. As described above, computer vision (CV) techniques or computer vision systems may be used to extract high-level understanding of the operating environment, object detection, object classification, extraction of the scene depth and estimation of relative positions of objects, extraction of objects' orientation in space. The 3D map information and sensor data (e.g., proximity sensor, imaging sensor) may be used to detect whether the robotic cart is within an optimal zone with respect to the patient bed.</p><p>[0087] FIG. <b>7</b> shows an example of a self-propelled (e.g., via one or more propulsion units such as wheels, rotors, propellers) robotic cart autonomously placing itself with respect to a patient bed. In some embodiments, a propulsion unit may include a plurality of wheels that may permit the robotic cart to roll over an underlying surface. In some examples, two, three or four wheels may be provided which may permit the robotic cart to stand stably while not moving. In some instances, stabilization may occur with aid of one or more wheels or other stabilization platforms, such as gyroscopic platforms. The wheels may vary in size or be the same size. In some cases, the wheels can have a diameter of at least about 1 cm, 2 cm, 3 cm, 4 cm, 5 cm, 8 cm, 9 cm, 10 cm, 15 cm, 20 cm, 25 cm, 30 cm, 35 cm, 40 cm, 45 cm, 50 cm, 55 cm, 60 cm, 65 cm, 70 cm, 75 cm, 80 cm, 85 cm, 90 cm, 95 cm, 100 cm, 150 cm, or 200 cm. The wheels can have a smooth or treaded surface. The wheels may also permit the robotic cart to move laterally and/or rotate in place. The robotic cart may be capable of making any combination of translational or rotational movement. The propulsion unit may be driven with aid of one or more actuators. For example, a motor, engine, drive train, or any other component may be provided that may aid in driving the propulsion of the robotic cart.</p><p>[0088] Based on the 3D depth map, an optimal location of the robotic cart may be generated. The optimal location may be generated based on a dimension of the robotic cart, the dimension of the robotic arm (workspace), the dimension of the endoscope device and the 3D depth map. Real-time sensor data (e.g., proximity sensor) may be collected and may be used to determine whether the robotic cart is in the proper location relative to the patient bed. As shown in FIG. <b>8</b>, when the self-propelled robotic cart is detected not in a proper location with respect to the patient bed <b>810</b>, the robotic cart may automatically move to the proper location <b>820</b>. The proper location, the movement speed, moving acceleration, and the movement trajectory may be calculated by one or more processors of the platform based at least in part on the 3D depth map.</p><p>[0089] Alternatively or additionally, the system may inform the user of an non-optimal placement of the robotic cart and may prompt the user to intervene, For instance, message, warning or notification may be displayed on the screen along with recommendations for placing the robotic cart (e.g., specify that the cart should be closer to the bed) and/or displaying a 2D/3D depth map of the operating environment and the robotic system. For example, the robotic system may generate a preferred relative cart position and orientation, respective to the current cart position and display an animation to a user to guide the user changing the position of the robotic cart.</p><p>[0090] FIG. <b>9</b> shows an example process <b>900</b> for autonomous alignment of the robotic endoscope system. It should be noted that in the illustrated process though Lidar data (e.g., 3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. In the exemplary process, runtime sensor data (e.g., 3D point cloud) may be captured as input <b>901</b>. The runtime sensor data may include data captured by a Lidar (light detection and ranging). The Lidar may obtain three-dimensional information of the operating environment/scene by measuring distances to objects. For example, the emitting apparatus of a Lidar system may generate a sequence of light pulses emitted within short time durations such that the sequence of light pulses may be used to derive a distance measurement point. The Lidar system may provide three-dimensional (3D) imaging (e.g., 3D point cloud). In some cases, the 3D point cloud or the 3D images may be further processed by one or more processors of the robotic endoscope system for obstacles detection or collision avoidance <b>903</b>. Various suitable image processing method (e.g., image segmentation) may be utilized to recognize an object such as the patient bed. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the patient bed).</p><p>[0091] The positions and orientations of the Lidar sensor may be obtained based on kinematic mapping and such information along with the depth information of the bed is used to estimate relative orientation and position between the bed and the robotic endoscope system. Based on the relative position and orientation, a movement path for moving the robotic cart to an optimal placement with respect to the patient bed may be generated. The optimal placement may be generated by the system automatically without user intervention. In some cases, instead of or in addition to performing the robotic cart self-placement, the algorithm may inform a user an invalid location of the robotic cart relative to the patient bed and a property location of the robotic cart may be displayed to the user on a GUI. The user may be able to provide input via a GUI indicating a selection of the property placement of the robotic cart. Upon receiving the user confirmation, the system may generate the movement path for the robotic cart. Alternatively, a user may be guided to manually move the robotic cart to a desired place as described above.</p><p>[0092] As described above, any suitable method may be employed to process the real-time sensor data (e.g., 3D point cloud) for segmentation, object recognition and collision avoidance. One or more objects may be segmented in the workspace of the robotic arm. For example, deep learning techniques such as an automated pipeline engine may be provided for processing the lidar data. The pipeline engine may comprise multiple components or layers. The pipeline engine may be configured to preprocess continuous streams of raw Lidar data or batch data transmitted from a Lidar system. In some cases, data may be processed so it can be fed into machine learning analyses. In some cases, data may be processed to provide details at different understanding levels, which understanding may include, by way of non-limiting example, dimensions, weight, composition, identity, degree of collision risk, mobility, and so forth. In some case, the pipeline engine may comprise multiple components to perform different functions for extracting different levels of information from the 3D point cloud data. In some cases, the pipeline engine may further include basic data processing such as, data normalization, labeling data with metadata, tagging, data alignment, data segmentation, and various others. In some cases, the processing methodology is programmable through APIs by the developers constructing the pipeline.</p><p>[0093] In some embodiments, the pipeline engine may utilize machine learning techniques for processing data. In some embodiments, raw Lidar data may be supplied to a first layer of the pipeline engine which may employ a deep learning architecture to extract primitives, such as edges, corners, surfaces, of one or more target objects. In some cases, the deep learning architecture may be a convolutional neuron network (CNN). CNN systems commonly are composed of layers of different types: convolution, pooling, upscaling, and fully-connected neuron network. In some cases, an activation function such as rectified linear unit may be used in some of the layers. In a CNN system, there can be one or more layers for each type of operation. The input data of the CNN system may be the data to be analyzed such as 3D radar data. The simplest architecture of a convolutional neural networks starts with an input layer (e.g., images) followed by a sequence of convolutional layers and pooling layers, and ends with fully-connected layers. In some cases, the convolutional layers are followed by a layer of ReLU activation function. Other activation functions can also be used, for example the saturating hyperbolic tangent, identity, binary step, logistic, arcTan, softsign, parameteric rectified linear unit, exponential linear unit. softPlus, bent identity, softExponential, Sinusoid, Sinc, Gaussian, the sigmoid function and various others. The convolutional, pooling and ReLU layers may act as learnable features extractors, while the fully connected layers acts as a machine learning classifier.</p><p>[0094] In some cases, the convolutional layers and fully-connected layers may include parameters or weights. These parameters or weights can be learned in a training phase. The parameters may be trained with gradient descent so that the class scores that the CNN computes are consistent with the labels in the training set for each 3D point cloud image. The parameters may be obtained from a back propagation neural network training process that may or may not be performed using the same hardware as the production or application process.</p><p>[0095] A convolution layer may comprise one or more filters. These filters will activate when they see same specific structure in the input data. In some cases, the input data may be 3D images, and in the convolution layer one or more filter operations may be applied to the pixels of the image. A convolution layer may comprise a set of learnable filters that slide over the image spatially, computing dot products between the entries of the filter and the input image. The filter operations may be implemented as convolution of a kernel over the entire image. A kernel may comprise one or more parameters. Results of the filter operations may be summed together across channels to provide an output from the convolution layer to the next pooling layer. A convolution layer may perform high-dimension convolutions. For example, the three-dimensional feature maps or input 3D data are processed by a group of three-dimensional kernels in a convolution layer.</p><p>[0096] The output produced by the first layer of the pipeline engine may be supplied to a second layer which is configured to extract understanding of a target object such as shapes, materials, sub-surface structure and the like. In some cases, the second layer can also be implemented using a machine learning architecture.</p><p>[0097] The output produced by the second layer may then be supplied to a third layer of the pipeline engine which is configured for perform interpretations and decision makings, such as object recognition, separation, segmentation, collision avoidance, target dynamics (e.g., mobility), identity recognition, type classification and the like. In some cases, the dynamics or mobility of an object may be used for determining a collision avoidance scheme.</p><p>[0098] The pipeline engine described herein can be implemented by one or more processors. In some embodiments, the one or more processors may be a programmable processor (e.g., a central processing unit (CPU), a graphic processing unit (GPU), a general-purpose processing unit or a microcontroller), in the form of fine-grained spatial architectures such as a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), and/or one or more Advanced RISC Machine (ARM) processors. In some embodiments, the processor may be a processing unit of a computer system.</p><p>[0099] In some cases, to mitigate for noise effects, Bayesian estimation techniques (e.g. Kalman filtering) is applied to fit a point cloud to a rigid model of the bed for aligning the robotic cart to the bed. In some cases, if the target object (e.g., patient bed) is not detected <b>907</b>, the process may proceed with informing user that the robotic cart position is invalid (e.g., out of the proper region) <b>921</b>. For example, if the bed is not detected, a notification may be displayed on the GUI to inform the user that the cart position is invalid.</p><p>[0100] If a target object such as a patient bed is detected <b>907</b>, the method may proceed with model coordinate registration <b>909</b> by mapping the position and orientation of the patient bed model to the coordinates of the robotic system (e.g., via a pre-registration of the sensor and robotic system coordinate frames). Next, the method may comprise an algorithm to determine whether the system is in an acceptable workspace. As an example, the algorithm may include computing the position and orientation error <b>911</b> and compare it against a threshold to determine whether the system is in an acceptable workspace.</p><p>[0101] In some cases, the system may provide both an autonomous mode and a manual positioning mode. If autonomous mode once is enabled (e.g., selected by a user) <b>913</b>, the system may execute an active system positioning algorithm. The active system positioning algorithm may, for example, perform path planning <b>915</b> to generate a path from current position to a target position at a target orientation, with proper moving speed, acceleration, and the like. For instance, a robotic cart wheel path plan is generated, or updated, to eliminate the relative orientation error between the desired and the sensed orientation that is computed in the previous operation <b>911</b>. The path is executed <b>917</b> and the robotic wheel motion is controlled to move the robotic cart to the desired orientation and position.</p><p>[0102] If the system disables the active system positioning algorithm such as in a semi-autonomous mode <b>913</b>, the user may be informed of the state of validity of the system positioning relative to the bed e.g., acceptable or not acceptable position <b>919</b>, and the user may manually control the position of the robotic cart.</p><p>[0103] FIGS. <b>10</b>-<b>12</b> show examples of collision avoidance. As described above, the real-time 3D map of the operating environment may comprise obstacle information and relative location to one or more components of the robotic endoscope system. The system may determine a proximity threshold and may autonomously move one or more components of the system to avoid collision. For example, as shown in FIG. <b>10</b>, when the robotic arm is actuated during the procedure, the sensor may detect unwanted proximity between the arm and the monitor of the system <b>1000</b>. The proximity is detected via the analysis of sensor signals that may contain depth information. In response to the proximity, the user may be informed of the potential collision via a UI warning. Alternatively or additionally, the robotic arm may be automatically moved away from the monitor, or the monitor may be actuated away from the arm <b>1010</b>. For example, the monitor may be mounted to a robotic support that can be actuated to move the monitor in response to control signals. As illustrated in the example <b>1010</b>, upon detection of potential collision between the monitor and any portion of the robotic arm, both the robotic support for the monitor and the robotic arm may be actuated to move away from each other. Alternatively, as illustrated in FIG. <b>11</b>, upon detection of potential collision between the monitor and any portion of the robotic arm <b>1100</b>, the robotic arm may be actuated to move away from the monitor.</p><p>[0104] FIG. <b>11</b> shows an example when an operator moves a monitor into the workspace of the robotic arm, the system may detect a proximity to the monitor is approaching a threshold and may automatically change a configuration of the robotic arm. The redundancy of the robotic arm may beneficially allow for the change of configuration of the robotic arm while maintaining the position and orientation of the IDM. FIG. <b>12</b> shows an example that during the procedure, equipment such as the fluoroscopy C-arm <b>1207</b> is brought into the workspace. The system may detect the equipment <b>1207</b> and a proximity to the equipment, then automatically adjust a configuration of the robotic arm and/or the robotic cart to provide enough clearance for the C-arm to take fluoroscopy images of the patient and/or complete sweep(s) for tomosynthesis. The configuration of the robotic arm may change from a first configuration <b>1203</b> to a second configuration <b>1205</b> while the position and orientation of IDM <b>1201</b> remain the same.</p><p>[0105] FIG. <b>13</b> shows an example of a collision avoidance algorithm <b>1300</b>. It should be noted that in the illustrated process though Lidar data (3D point cloud) is used for the real-time detection of objects in the operating environment and obtaining depth information, any other suitable sensors (e.g., stereoscopic camera) and methods as described elsewhere herein can be utilized. The steps about capturing the 3D point cloud data and segmentation of object within a workspace to detect an object can be the same as those described in FIG. <b>9</b>. For example, the input data may comprise 3D point cloud data captured by Lidar device <b>1301</b> and deep learning techniques as described above may be employed to perform object segmentation, object recognition <b>1303</b> for collision avoidance. For instance, when an object (e.g., display device, monitor) is brought into the operating environment, a segmentation algorithm may be executed to detect and recognize the object. Depth information from the 3D point cloud may be used to assign distances to points within the segmented region (e.g., region of the monitor).</p><p>[0106] In some cases, signal filtering is performed to mitigate effects of sensing uncertainty <b>1305</b>. In some cases, when the object being detected can be represented by a precomputed model, Bayesian filtering methods, such as Kalman filtering, may be applied. For example, the precomputed model may include an estimation algorithm for an object, where the sensed point cloud of a monitor is an input to the estimation algorithm which estimates the state (position, orientation) of a monitoring device.</p><p>[0107] Next, position and orientation of the point cloud are mapped to arm base coordinate frame <b>1307</b>. Position of components of the robotic endoscope system such as the robotic arm and position of the points in the segmented monitor region may be registered in the same frame of reference for determining a minimum proximity between any portion of the monitor and any portion of the robotic arm (computing distance-to-contact) <b>1309</b>. The positions and orientations of the robotic arm may be obtained based on kinematic mapping.</p><p>[0108] In some cases, a proximity determination algorithm may be executed to determine a potential collision whether the minimum proximity violates a predetermined proximity threshold <b>1311</b>. In some cases, upon determine a violation of the predetermined proximity threshold, the system may activate an admittance controller to reconfigure a configuration of the robotic arm (e.g., by actuating one or more joints/links of the robotic arm) <b>1313</b>. For example, the admittance controller may execute an admittance control algorithm which defines a proximal-most point of collision as repulsive force and reconfigure the robotic arm to increase the (minimum) distance between the robotic arm and the monitor. An algorithm is executed to reposition the arm for the purpose of avoiding a collision prior to occurring. In some cases, the admittance control algorithm may map a sensed proximity between any portion of the robotic arm and an object to a virtual force applied on the robotic arm and calculate an output motion for the robotic arm as if an equivalent contact force was applied to the robotic arm (through contact). For instance, the admittance control algorithm comprises mapping a shortest distance between a link of an arm and obstacle to an input to a commanded task-space velocity of the arm link. The Geometric Jacobian of the arm may be used to map a task-space motion command to arm joint commands in order to achieve arm motion which increases the distance between the arm and obstacle.</p><p>[0109] In the event of the robotic system having more degrees-of-freedom than the task, i.e. actuation redundancy existing, the avoidance may be implemented via a redundancy resolution algorithm. For exempt the elbow of the arm may be repositioned to prevent a collision without affecting the position and orientation of the end-effector. Such algorithm may be implemented by defining a secondary redundancy-resolution task where the arm is repositioned such that the distance between the arm and the object (prior to collision) is increased. In some cases, when there is more than one degree-of-freedom in redundancy, the robot command can be chosen to actuate the joint, for example, as one where the norm of joint rates is minimized.</p><p>[0110] In some embodiments, the robotic endoscope system may be capable of autonomously adjusting position of the IDM relative to the patient side mount based on buckling detection. When the flexible endoscope is pushed at the proximal end, during insertion of the flexible device into the anatomy, the flexible endoscope may deform when navigating through turns and buckling. The deformation may happen during insertion, as the flexible device may take on a minimum-energy shape, which may be a \\\"hugging\\\" of the shaft against tissue. The buckling may happen when resistance force is encountered in the distal portion of the shaft.</p><p>[0111] During retraction of the flexible device, the distance of the shaft that is \\\"lost to buckling\\\" may become slack when the system actuation direction is reversed (e.g., backward moving or retraction). This phenomenon will result in a perceived dead-zone or system delay (a user input to command movement of the robotic endoscope tip does not map directly to robotic endoscope tip motion). When an endoscope is buckled, a retraction of the endoscope may result in robotic actuation with little endoscope tip translation.</p><p>[0112] The prolapse or kink may result in potential damage as it may expose the sharp edges of the kinked elongate device and complicate the surgical procedure. Moreover, a bent or kinked elongate device may render the system losing location/shape control of the device during both insertion and retraction and it may block the passage of an instrument. Furthermore, a device that prolapses or kinks may not be able to provide adequate reach towards the target anatomy for performing the intended task.</p><p>[0113] The robotic endoscope system herein may employ a responsive insertion and retraction velocity control of the flexible endoscope. The responsive velocity control method herein may automatically correct for motion differences between the endoscope tip and a velocity command (e.g., instrument driving mechanism (IDM) command). In some cases, the velocity control of the endoscope may be performed in conjunction with the collision avoidance algorithm as described above such that the robotic arm may autonomously reconfigure to avoid collision with other objects while the velocity of the endoscope tip is maintained and not influenced.</p><p>[0114] Unlike the conventional methods for buckling detection based on the difference in the position of the endoscope device (e.g., expected position and measure tip position), the methods and systems herein may automatically correct the buckling/deformation during insertion and retraction based on the velocity measured at an endoscope tip and a velocity control command. This beneficially avoids shape sensing or using extra imaging approaches to determine the shape or position of the endoscope device.</p><p>[0115] During insertion of an endoscope device, a velocity command may be received by the endoscope device. The velocity command may be provided by a user input. For example, a user may provide a control instruction via a control interface of the endoscope device indicating a desired/expected velocity of the endoscope tip. The control interface may include various devices such as touchscreen monitors, joysticks, keyboards and other interactive devices. A user may be able to navigate and/or control the motion of the robotic arm and the motion (e.g., tip velocity) of the catheter using a user input device. The user input device can have any type user interactive component, such as a button, mouse, joystick, trackball, touchpad, pen, image capturing device, motion capture device, microphone, touchscreen, hand-held wrist gimbals, exoskeletal gloves, or other user interaction system such as virtual reality systems, augmented reality systems and the like.</p><p>[0116] The user input for commanding a velocity of the endoscope tip may be received via the input device. For instance, a press on a joystick may be mapped to an analog value indicating a speed/velocity of the tip. For example, half press on the joystick may be mapped to 3 mm/s, full press may be mapped to 6 mm/s and no press may be mapped to 0 mm/s.</p><p>[0117] Based on the specific user input device, the user input may be processed to be converted to a desired/commanded velocity of the tip of the catheter/endoscope. Next, a tip velocity error is computed. The tip velocity error is the difference between the desired/commanded tip velocity and the velocity of the endoscope tip. In some embodiments, the velocity of the endoscope tip may be measured based on sensor data. In some cases, the sensor data may comprise position and orientation information of the distal tip of the endoscope.</p><p>[0118] In some cases, the sensor signals may be acquired by positioning sensors. For example, the sensor signals may be acquired by electromagnetic coils located on the distal end used with an electromagnetic tracking system to detect the position and orientation of the distal end of the endoscope. For example, positioning sensors such as electromagnetic (EM) sensors may be embedded at the distal tip of the catheter and an EM field generator may be positioned next to the patient torso during procedure. The EM field generator may locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. The endoscope tip position measured by the EM sensor p<sub>e</sub>=EM sensor (tip) position, may be expressed in field-generator frame.</p><p>[0119] Next, the linear velocity of the endoscope tip may be computed. The linear velocity may be computed using time derivative method such as backward Euler derivative. In some cases, the endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction. This projection may be required because the user provided velocity command is based on integrated position changes that occur in the direction of n. In some cases, a low pass filter may be applied to generate the filtered time derivative data.</p><p>[0120] The endoscope tip velocity may be computed as the filtered time derivative of tip position (e.g., measured by the EM sensor expressed in the filed-generator frame), projected in the heading direction of n using a projection matrix v<sub>n</sub>=nn<sup>T </sup>filt(dp<sub>e</sub>/dt).</p><p>[0121] Where n is a unit vector that indicates the endoscope tip heading direction, expressed in field-generator frame. There may be mechanical offset between the EM sensor and the scope tip and the mechanical offset may be calibrated for each endoscope device. dp<sub>e</sub>/dt represents the time derivative of the endoscope tip, nn<sup>T </sup>is the projection matrix that maps the aforementioned velocity to the heading direction of the endoscope tip (i.e. velocities that are not in the direction of the heading are ignored). The velocity v<sub>n </sub>may not be affected by articulation as the endoscope tip translation owing to articulation is orthogonal to n.</p><p>[0122] After the endoscope tip velocity is computed, the endoscope tip velocity error may be computed and may be further processed for safety checks. In some cases, the safety checks may comprise a plurality of checks. For example, the plurality of checks may comprise determining whether the endoscope tip has been stationary (e.g., tip velocity is about zero) while the handle portion of the endoscope (e.g., IDM) has an insertion distance is beyond a distance threshold. In another example, the plurality of checks may comprise determining whether the endoscope tip is retracting (e.g., negative tip velocity error) while the handle portion of the endoscope (e.g., IDM) is inserting, if yes, a prolapse may be occurring. In a further example, the plurality of checks may comprise determining whether the insertion force is beyond a force threshold. The term \\\"insertion distance\\\" as utilized herein may refer to the distance along the navigation path.</p><p>[0123] The method may comprise a closed loop control of the tip velocity to reduce the tip velocity error. The tip velocity of the endoscope may be controlled based on the tip velocity error computed at operation which is used as the feedback signal for commanding the motion of the IDM. The user velocity input may be mapped to the command to control the motion of IDM (e.g., velocity to move the IDM along the insertion axis). The command may be control signals to control the motors of the robotic arm thereby controlling a motion of the IDM (i.e., proximal end of the endoscope). The endoscope tip velocity may be calculated based on the robotic arm inverse kinematics. The endoscope tip velocity is then used to calculate the tip velocity in the heading direction by projection in the heading direction of n using a projection matrix as described above. In some cases, the feedback signal may be the projection of tip velocity processed by a low-pass filter.</p><p>[0124] Based on the control algorithm, a motion command is generated to actuate the robotic arm thereby affecting a tip velocity of the endoscope. The effect on the motion of the IDM (e.g., insertion velocity, insertion distance) and on the motion of the endoscope tip (e.g., tip velocity) may not perfectly match due to the tortuosity of the navigation path, the buckling and/or prolapse as described above.</p><p>[0125] During retraction of the medical device, a previously determined deformation-loss during insertion may be used as a feed-forward term to perform a backslash compensation for the retraction control. This reduces the deformation-loss incurred during the insertion prior to the tip motion during the retraction.</p><p>[0126] FIG. <b>14</b> illustrates an example of a flexible endoscope <b>1400</b>, in accordance with some embodiments of the present disclosure. As shown in FIG. <b>14</b>, the flexible endoscope <b>1400</b> may comprise a handle/proximal portion <b>1409</b> and a flexible elongate member to be inserted inside of a subject. The flexible elongate member can be the same as the one described above. In some embodiments, the flexible elongate member may comprise a proximal shaft (e.g., insertion shaft <b>1401</b>), steerable tip (e.g., tip <b>1405</b>), and a steerable section (active bending section <b>1403</b>). The active bending section, and the proximal shaft section can be the same as those described elsewhere herein. The endoscope <b>1400</b> may also be referred to as steerable catheter assembly as described elsewhere herein. In some cases, the endoscope <b>1400</b> may be a single-use robotic endoscope. In some cases, the entire catheter assembly may be disposable. In some cases, at least a portion of the catheter assembly may be disposable. In some cases, the entire endoscope may be released from an instrument driving mechanism and can be disposed of In some embodiment, the endoscope may contain varying levels of stiffness along the shaft, as to improve functional operation.</p><p>[0127] The endoscope or steerable catheter assembly <b>1400</b> may comprise a handle portion <b>1409</b> that may include one or more components configured to process image data, provide power, or establish communication with other external devices. For instance, the handle portion may include a circuitry and communication elements that enables electrical communication between the steerable catheter assembly <b>1400</b> and an instrument driving mechanism (not shown), and any other external system or devices. In another example, the handle portion <b>1409</b> may comprise circuitry elements such as power sources for powering the electronics (e.g., camera, electromagnetic sensor and LED lights) of the endoscope.</p><p>[0128] The one or more components located at the handle may be optimized such that expensive and complicated components may be allocated to the robotic support system, a hand-held controller or an instrument driving mechanism thereby reducing the cost and simplifying the design the disposable endoscope. The handle portion or proximal portion may provide an electrical and mechanical interface to allow for electrical communication and mechanical communication with the instrument driving mechanism. The instrument driving mechanism may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the catheter. The handle portion of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley/capstans assemblies are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0129] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0130] The electrical interface (e.g., printed circuit board) may allow image/video data and/or sensor data to be received by the communication module of the instrument driving mechanism and may be transmitted to other external devices/systems. In some cases, the electrical interface may establish electrical communication without cables or wires. For example, the interface may comprise pins soldered onto an electronics board such as a printed circuit board (PCB). For instance, receptacle connector (e.g., the female connector) is provided on the instrument driving mechanism as the mating interface. This may beneficially allow the endoscope to be quickly plugged into the instrument driving mechanism or robotic support without utilizing extra cables. Such type of electrical interface may also serve as a mechanical interface such that when the handle portion is plugged into the instrument driving mechanism, both mechanical and electrical coupling is established. Alternatively or in addition to, the instrument driving mechanism may provide a mechanical interface only. The handle portion may be in electrical communication with a modular wireless communication device or any other user device (e.g., portable/hand-held device or controller) for transmitting sensor data and/or receiving control signals.</p><p>[0131] In some cases, the handle portion <b>1409</b> may comprise one or more mechanical control modules such as lure <b>1411</b> for interfacing the irrigation system/aspiration system. In some cases, the handle portion may include lever/knob for articulation control. Alternatively, the articulation control may be located at a separate controller attached to the handle portion via the instrument driving mechanism.</p><p>[0132] The endoscope may be attached to a robotic support system or a hand-held controller via the instrument driving mechanism. The instrument driving mechanism may be provided by any suitable controller device (e.g., hand-held controller) that may or may not include a robotic system. The instrument driving mechanism may provide mechanical and electrical interface to the steerable catheter assembly <b>1400</b>. The mechanical interface may allow the steerable catheter assembly <b>1400</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the steerable catheter assembly can be attached to the instrument driving mechanism via quick install/release means, such as magnets, spring-loaded levels and the like. In some cases, the steerable catheter assembly may be coupled to or released from the instrument driving mechanism manually without using a tool.</p><p>[0133] In the illustrated example, the distal tip of the catheter or endoscope shaft is configured to be articulated/bent in two or more degrees of freedom to provide a desired camera view or control the direction of the endoscope. As illustrated in the example, imaging device (e.g., camera), position sensors (e.g., electromagnetic sensor) <b>1407</b> is located at the tip of the catheter or endoscope shaft <b>1405</b>. For example, line of sight of the camera may be controlled by controlling the articulation of the active bending section <b>1403</b>. In some instances, the angle of the camera may be adjustable such that the line of sight can be adjusted without or in addition to articulating the distal tip of the catheter or endoscope shaft. For example, the camera may be oriented at an angle (e.g., tilt) with respect to the axial direction of the tip of the endoscope with aid of an optimal component.</p><p>[0134] The distal tip <b>1405</b> may be a rigid component that allow for positioning sensors such as electromagnetic (EM) sensors, imaging devices (e.g., camera) and other electronic components (e.g., LED light source) being embedded at the distal tip.</p><p>[0135] In real-time EM tracking, the EM sensor comprising of one or more sensor coils embedded in one or more locations and orientations in the medical instrument (e.g., tip of the endoscopic tool) measures the variation in the EM field created by one or more static EM field generators positioned at a location close to a patient. The location information detected by the EM sensors is stored as EM data. The EM field generator (or transmitter), may be placed close to the patient to create a low intensity magnetic field that the embedded sensor may detect. The magnetic field induces small currents in the sensor coils of the EM sensor, which may be analyzed to determine the distance and angle between the EM sensor and the EM field generator. For example, the EM field generator may be positioned close to the patient torso during procedure to locate the EM sensor position in 3D space or may locate the EM sensor position and orientation in 5D or 6D space. This may provide a visual guide to an operator when driving the bronchoscope towards the target site.</p><p>[0136] The endoscope may have a unique design in the elongate member. In some cases, the active bending section <b>1403</b>, and the proximal shaft of the endoscope may consist of a single tube that incorporates a series of cuts (e.g., reliefs, slits, etc.) along its length to allow for improved flexibility, a desirable stiffness as well as the anti-prolapse feature (e.g., features to define a minimum bend radius).</p><p>[0137] As described above, the active bending section <b>1403</b> may be designed to allow for bending in two or more degrees of freedom (e.g., articulation). A greater bending degree such as 180 and 270 degrees (or other articulation parameters for clinical indications) can be achieved by the unique structure of the active bending section. In some cases, a variable minimum bend radius along the axial axis of the elongate member may be provided such that an active bending section may comprise two or more different minimum bend radii.</p><p>[0138] The articulation of the endoscope may be controlled by applying force to the distal end of the endoscope via one or multiple pull wires. The one or more pull wires may be attached to the distal end of the endoscope. In the case of multiple pull wires, pulling one wire at a time may change the orientation of the distal tip to pitch up, down, left, right or any direction needed. In some cases, the pull wires may be anchored at the distal tip of the endoscope, running through the bending section, and entering the handle where they are coupled to a driving component (e.g., pulley). This handle pulley may interact with an output shaft from the robotic system.</p><p>[0139] In some embodiments, the proximal end or portion of one or more pull wires may be operatively coupled to various mechanisms (e.g., gears, pulleys, capstans, etc.) in the handle portion of the catheter assembly. The pull wire may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire can also be made of natural or organic materials or fibers. The pull wire can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end/portion of one or more pull wires may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0140] The pull wires may be made of any suitable material such as stainless steel (e.g., SS316), metals, alloys, polymers, nylons or biocompatible material. Pull wires may be a wire, cable or a thread. In some embodiments, different pull wires may be made of different materials for varying the load bearing capabilities of the pull wires. In some embodiments, different sections of the pull wires may be made of different material to vary the stiffness and/or load bearing along the pull. In some embodiments, pull wires may be utilized for the transfer of electrical signals.</p><p>[0141] The proximal design may improve the reliability of the device without introducing extra cost allowing for a low-cost single-use endoscope. In another aspect of the invention, a single-use robotic endoscope is provided. The robotic endoscope may be a bronchoscope and can be the same as the steerable catheter assembly as described elsewhere herein. Traditional endoscopes can be complex in design and are usually designed to be re-used after procedures, which require thorough cleaning, dis-infection, or sterilization after each procedure. The existing endoscopes are often designed with complex structures to ensure the endoscopes can endure the cleaning, dis-infection, and sterilization processes. The provided robotic bronchoscope can be a single-use endoscope that may beneficially reduce cross-contamination between patients and infections. In some cases, the robotic bronchoscope may be delivered to the medical practitioner in a pre-sterilized package and are intended to be disposed of after a single-use.</p><p>[0142] As shown in FIG. <b>15</b>, a robotic bronchoscope <b>1510</b> may comprise a handle portion <b>1513</b> and a flexible elongate member <b>1511</b>. In some embodiments, the flexible elongate member <b>1111</b> may comprise a shaft, steerable tip, and a steerable/active bending section. The robotic bronchoscope <b>1510</b> can be the same as the steerable catheter assembly as described in FIG. <b>14</b>. The robotic bronchoscope may be a single-use robotic endoscope. In some cases, only the catheter may be disposable. In some cases, at least a portion of the catheter may be disposable. In some cases, the entire robotic bronchoscope may be released from the instrument driving mechanism and can be disposed of. In some cases, the bronchoscope may contain varying levels of stiffness along its shaft, as to improve functional operation. In some cases, a minimum bend radius along the shaft may vary.</p><p>[0143] The robotic bronchoscope can be releasably coupled to an instrument driving mechanism <b>1520</b>. The instrument driving mechanism <b>1520</b> may be mounted to the arm of the robotic support system or to any actuated support system as described elsewhere herein. The instrument driving mechanism may provide mechanical and electrical interface to the robotic bronchoscope <b>1510</b>. The mechanical interface may allow the robotic bronchoscope <b>1510</b> to be releasably coupled to the instrument driving mechanism. For instance, the handle portion of the robotic bronchoscope can be attached to the instrument driving mechanism via quick install/release means, such as magnets and spring-loaded levels. In some cases, the robotic bronchoscope may be coupled or released from the instrument driving mechanism manually without using a tool.</p><p>[0144] FIG. <b>16</b> shows an example of an instrument driving mechanism <b>1620</b> providing mechanical interface to the handle portion <b>1613</b> of the robotic bronchoscope. As shown in the example, the instrument driving mechanism <b>1620</b> may comprise a set of motors that are actuated to rotationally drive a set of pull wires of the flexible endoscope or catheter. The handle portion <b>1613</b> of the catheter assembly may be mounted onto the instrument drive mechanism so that its pulley assemblies or capstans are driven by the set of motors. The number of pulleys may vary based on the pull wire configurations. In some cases, one, two, three, four, or more pull wires may be utilized for articulating the flexible endoscope or catheter.</p><p>[0145] The handle portion may be designed allowing the robotic bronchoscope to be disposable at reduced cost. For instance, classic manual and robotic bronchoscopes may have a cable in the proximal end of the bronchoscope handle. The cable often includes illumination fibers, camera video cable, and other sensors fibers or cables such as electromagnetic (EM) sensors, or shape sensing fibers. Such complex cable can be expensive, adding to the cost of the bronchoscope. The provided robotic bronchoscope may have an optimized design such that simplified structures and components can be employed while preserving the mechanical and electrical functionalities. In some cases, the handle portion of the robotic bronchoscope may employ a cable-free design while providing a mechanical/electrical interface to the catheter.</p><p>[0146] FIG. <b>17</b> shows an example of a distal tip <b>1700</b> of an endoscope. In some cases, the distal portion or tip of the catheter <b>1700</b> may be substantially flexible such that it can be steered into one or more directions (e.g., pitch, yaw). The catheter may comprise a tip portion, bending section, and insertion shaft. In some embodiments, the catheter may have variable bending stiffness along the longitudinal axis direction. For instance, the catheter may comprise multiple sections having different bending stiffness (e.g., flexible, semi-rigid, and rigid). The bending stiffness may be varied by selecting materials with different stiffness/rigidity, varying structures in different segments (e.g., cuts, patterns), adding additional supporting components or any combination of the above. In some embodiments, the catheter may have variable minimum bend radius along the longitudinal axis direction. The selection of different minimum bend radius at different location long the catheter may beneficially provide anti-prolapse capability while still allow the catheter to reach hard-to-reach regions. In some cases, a proximal end of the catheter needs not be bent to a high degree thus the proximal portion of the catheter may be reinforced with additional mechanical structure (e.g., additional layers of materials) to achieve a greater bending stiffness. Such design may provide support and stability to the catheter. In some cases, the variable bending stiffness may be achieved by using different materials during extrusion of the catheter. This may advantageously allow for different stiffness levels along the shaft of the catheter in an extrusion manufacturing process without additional fastening or assembling of different materials.</p><p>[0147] The distal portion of the catheter may be steered by one or more pull wires <b>1705</b>. The distal portion of the catheter may be made of any suitable material such as co-polymers, polymers, metals or alloys such that it can be bent by the pull wires. In some embodiments, the proximal end or terminal end of one or more pull wires <b>1705</b> may be coupled to a driving mechanism (e.g., gears, pulleys, capstan etc.) via the anchoring mechanism as described above.</p><p>[0148] The pull wire <b>1705</b> may be a metallic wire, cable or thread, or it may be a polymeric wire, cable or thread. The pull wire <b>1705</b> can also be made of natural or organic materials or fibers. The pull wire <b>1705</b> can be any type of suitable wire, cable or thread capable of supporting various kinds of loads without deformation, significant deformation, or breakage. The distal end or portion of one or more pull wires <b>1705</b> may be anchored or integrated to the distal portion of the catheter, such that operation of the pull wires by the control unit may apply force or tension to the distal portion which may steer or articulate (e.g., up, down, pitch, yaw, or any direction in-between) at least the distal portion (e.g., flexible section) of the catheter.</p><p>[0149] The catheter may have a dimension so that one or more electronic components can be integrated to the catheter. For example, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm), and the diameter of the working channel may be around 2 mm such that one or more electronic components can be embedded into the wall of the catheter. However, it should be noted that based on different applications, the outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool dimensional or specific application.</p><p>[0150] The one or more electronic components may comprise an imaging device, illumination device or sensors. In some embodiments, the imaging device may be a video camera <b>1713</b>. The imaging device may comprise optical elements and image sensor for capturing image data. The image sensors may be configured to generate image data in response to wavelengths of light. A variety of image sensors may be employed for capturing image data such as complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD). The imaging device may be a low-cost camera. In some cases, the image sensor may be provided on a circuit board. The circuit board may be an imaging printed circuit board (PCB). The PCB may comprise a plurality of electronic elements for processing the image signal. For instance, the circuit for a CCD sensor may comprise A/D converters and amplifiers to amplify and convert the analog signal provided by the CCD sensor. Optionally, the image sensor may be integrated with amplifiers and converters to convert analog signal to digital signal such that a circuit board may not be required. In some cases, the output of the image sensor or the circuit board may be image data (digital signals) can be further processed by a camera circuit or processors of the camera. In some cases, the image sensor may comprise an array of optical sensors.</p><p>[0151] The illumination device may comprise one or more light sources <b>1711</b> positioned at the distal tip. The light source may be a light-emitting diode (LED), an organic LED (OLED), a quantum dot, or any other suitable light source. In some cases, the light source may be miniaturized LED for a compact design or Dual Tone Flash LED Lighting.</p><p>[0152] The imaging device and the illumination device may be integrated to the catheter. For example, the distal portion of the catheter may comprise suitable structures matching at least a dimension of the imaging device and the illumination device. The imaging device and the illumination device may be embedded into the catheter. FIG. <b>18</b> shows an example distal portion of the catheter with integrated imaging device and the illumination device. A camera may be located at the distal portion. The distal tip may have a structure to receive the camera, illumination device and/or the location sensor. For example, the camera may be embedded into a cavity <b>1810</b> at the distal tip of the catheter. The cavity <b>1810</b> may be integrally formed with the distal portion of the cavity and may have a dimension matching a length/width of the camera such that the camera may not move relative to the catheter. The camera may be adjacent to the working channel <b>1820</b> of the catheter to provide near field view of the tissue or the organs. In some cases, the attitude or orientation of the imaging device may be controlled by controlling a rotational movement (e.g., roll) of the catheter.</p><p>[0153] The power to the camera may be provided by a wired cable. In some cases, the cable wire may be in a wire bundle providing power to the camera as well as illumination elements or other circuitry at the distal tip of the catheter. The camera and/or light source may be supplied with power from a power source located at the handle portion via wires, copper wires, or via any other suitable means running through the length of the catheter. In some cases, real-time images or video of the tissue or organ may be transmitted to an external user interface or display wirelessly. The wireless communication may be WiFi, Bluetooth, RF communication or other forms of communication. In some cases, images or videos captured by the camera may be broadcasted to a plurality of devices or systems. In some cases, image and/or video data from the camera may be transmitted down the length of the catheter to the processors situated in the handle portion via wires, copper wires, or via any other suitable means. The image or video data may be transmitted via the wireless communication component in the handle portion to an external device/system. In some cases, the system may be designed such that no wires are visible or exposed to operators.</p><p>[0154] In conventional endoscopy, illumination light may be provided by fiber cables that transfer the light of a light source located at the proximal end of the endoscope, to the distal end of the robotic endoscope. In some embodiments of the disclosure, miniaturized LED lights may be employed and embedded into the distal portion of the catheter to reduce the design complexity. In some cases, the distal portion may comprise a structure <b>1430</b> having a dimension matching a dimension of the miniaturized LED light source. As shown in the illustrated example, two cavities <b>1430</b> may be integrally formed with the catheter to receive two LED light sources. For instance, the outer diameter of the distal tip may be around 4 to 4.4 millimeters (mm) and diameter of the working channel of the catheter may be around 2 mm such that two LED light sources may be embedded at the distal end. The outer diameter can be in any range smaller than 4 mm or greater than 4.4 mm, and the diameter of the working channel can be in any range according to the tool's dimensional or specific application. Any number of light sources may be included. The internal structure of the distal portion may be designed to fit any number of light sources.</p><p>[0155] In some cases, each of the LEDs may be connected to power wires which may run to the proximal handle. In some embodiment, the LEDs may be soldered to separated power wires that later bundle together to form a single strand. In some embodiments, the LEDs may be soldered to pull wires that supply power. In other embodiments, the LEDs may be crimped or connected directly to a single pair of power wires. In some cases, a protection layer such as a thin layer of biocompatible glue may be applied to the front surface of the LEDs to provide protection while allowing light emitted out. In some cases, an additional cover <b>1431</b> may be placed at the forwarding end face of the distal tip providing precise positioning of the LEDs as well as sufficient room for the glue. The cover <b>1831</b> may be composed of transparent material matching the refractive index of the glue so that the illumination light may not be obstructed.</p><p>[0156] While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"no\\\"><p><b>1</b>. (canceled)</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>2</b>. A method for controlling a robotic endoscope system, the method comprising: <BR />actuating a robotic support system to a desired location relative to a patient bed based at least in part on a 3D map of an environment surrounding the robotic endoscope system;<BR />autonomously aligning an instrument driving mechanism (IDM) to a component coupled to or as a part of the patient bed, wherein the IDM is releasably coupled to a distal end of a robotic arm of the robotic support system; and<BR />positioning the IDM at a pre-determined distance from the component coupled to or as a part of the patient bed.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>3</b>. The method of claim 2, wherein the IDM is autonomously aligned to the component based at least in part on a fiducial marker located on the component.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>4</b>. The method of claim 2, wherein the IDM is autonomously aligned to the component by generating a command to actuate one or more actuators of the robotic arm to move the IDM into a position or an orientation in alignment with the component.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>5</b>. The method of claim 2, further comprising aligning an anti-buckling mechanism to the component.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"5\\\"><p><b>6</b>. The method of claim 5, wherein the anti-buckling mechanism is in a collapsed state.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>7</b>. The method of claim 2, further comprising automatically adjusting a position or a distance of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>8</b>. The method of claim 2, further comprising detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p><b>9</b>. The method of claim 8, further comprising controlling a velocity of the tip of the flexible catheter to automatically correct a deformation caused by the buckling.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>10</b>. The method of claim 2, wherein the robotic arm is controlled in an autonomous mode and a compliant mode.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>11</b>. The method of claim 2, wherein the robotic support system operates in an autonomous mode or a semi-autonomous mode.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"yes\\\"><p><b>12</b>. A system for controlling a robotic endoscope system, the system comprising: <BR />a memory storing computer-executable instructions; one or more processors in communication with the robotic endoscope system and configured to execute the computer-executable instructions to:<BR />actuate a robotic support system to a desired location relative to a patient bed based at least in part on a 3D map of an environment surrounding the robotic endoscope system;<BR />autonomously align an instrument driving mechanism (IDM) to a component coupled to or as a part of the patient bed, wherein the IDM is releasably coupled to a distal end of a robotic arm of the robotic support system; and<BR />position the IDM at a pre-determined distance from the component coupled to or as a part of the patient bed,</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>13</b>. The system of claim 12, wherein the IDM is autonomously aligned to the component based at least in part on a fiducial marker located on the component.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>14</b>. The system of claim 12, wherein the IDM is autonomously aligned to the component by generating a command to actuate one or more actuators of the robotic arm to move the IDM into a position or an orientation in alignment with the component.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>15</b>. The system of claim 12, wherein the computer-executable instructions further comprise aligning an anti-buckling mechanism to the component.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"15\\\"><p><b>16</b>. The system of claim 15, wherein the anti-buckling mechanism is in a collapsed state.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>17</b>. The system of claim 12, wherein the computer-executable instructions further comprise automatically adjusting a position or a distance of the IDM relative to the component upon detection of a buckling event.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>18</b>. The system of claim 12, wherein the computer-executable instructions further comprise detecting a buckling of a flexible catheter coupled to the IDM while the flexible catheter is inserted into a body of a patient.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p><b>19</b>. The system of claim 18, wherein the computer-executable instructions further comprise controlling a velocity of the tip of the flexible catheter to automatically correct a deformation caused by the buckling.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>20</b>. The system of claim 12, wherein the robotic arm is controlled in an autonomous mode and a compliant mode.</p></Claim><Claim claimid=\\\"21\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>21</b>. The system of claim 12, wherein the robotic support system operates in an autonomous mode or a semi-autonomous mode.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/32\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B1/00097\",\n",
      "\n",
      "\"A61B1/0016\",\n",
      "\n",
      "\"A61B1/005\",\n",
      "\n",
      "\"A61B1/018\",\n",
      "\n",
      "\"A61B1/05\",\n",
      "\n",
      "\"A61B1/0676\",\n",
      "\n",
      "\"A61B1/0684\",\n",
      "\n",
      "\"A61B2034/107\",\n",
      "\n",
      "\"A61B2034/2051\",\n",
      "\n",
      "\"A61B2034/2055\",\n",
      "\n",
      "\"A61B2034/2074\",\n",
      "\n",
      "\"A61B2034/301\",\n",
      "\n",
      "\"A61B34/10\",\n",
      "\n",
      "\"A61B34/20\",\n",
      "\n",
      "\"A61B34/32\",\n",
      "\n",
      "\"A61B90/361\",\n",
      "\n",
      "\"A61B90/37\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"NOAH MEDICAL CORP\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20220524,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20240594287\",\n",
      "\n",
      "\"ad\": 20240304,\n",
      "\n",
      "\"pn\": \"US2024293195\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240304,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HUNG, CAROL KAYEE;SLAWINSKI, PIOTR ROBERT;THOMPSON, HENDRIK;AND OTHERS;SIGNING DATES FROM 20230712 TO 20230724;REEL/FRAME:066634/0324, Owner: NOAH MEDICAL CORPORATION, CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HUNG, CAROL KAYEE;SLAWINSKI, PIOTR ROBERT;THOMPSON, HENDRIK;AND OTHERS;SIGNING DATES FROM 20230712 TO 20230724;REEL/FRAME:066634/0324\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOAH MEDICAL CORPORATION, CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20240594287\",\n",
      "\n",
      "\"ad\": 20240304,\n",
      "\n",
      "\"pn\": \"US2024293195\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240528,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20240594287\",\n",
      "\n",
      "\"ad\": 20240304,\n",
      "\n",
      "\"pn\": \"US2024293195\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240528,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20240594287\",\n",
      "\n",
      "\"ad\": 20240304,\n",
      "\n",
      "\"pn\": \"US2024293195\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240905,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=100080311\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20210617360\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"TEMPORALLY BALANCED MULTI-MODE MASTER IMAGING SEQUENCE FOR ULTRASONIC CONTRAST IMAGING\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A particular sequence of ultrasound transmissions and corresponding echo receptions enables the production of Amplitude Modulated (AM) and Amplitude Modulated Phase Inverted (AMPI) signals that are temporally balanced. Temporal balancing significantly reduces tissue artifacts caused by movement of tissue during acquisition of the ultrasound echoes. Additionally, in combining the selected echo signals to produce the AM5 and AMPI signals, and optionally a Phase Inverted (PI) signal, each of the echo signals is equally weighted to facilitate an amplitude balance that avoids different echoes affecting the produced AM, AMPI, and PI signals differently.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO PRIOR APPLICATIONS</p><p>[0001] This application is the U.S. National Phase application under 35 U.S.C. § 371 of International Application No. PCT/EP2020/065504, filed on Jun. 4, 2020, which claims the benefit of U.S. Provisional Patent Application No. 62/859,936, filed on Jun. 11, 2019. These applications are hereby incorporated by reference herein.</p></relapp><shortsum><p>FIELD OF THE INVENTION</p><p>[0002] This invention relates to the field of contrast-enhanced ultrasound (CEUS) imaging, and in particular to a method and system that employs advanced pulse sequences that combine the advantages of multiple CEUS pulsing schemes.</p><p>BACKGROUND OF THE INVENTION</p><p>[0003] Ultrasound images are created by transmitting ultrasonic pulses at varying amplitudes and frequencies, receiving the echoes corresponding to each transmitted pulse, and processing the echoes to create the image. Often, an ultrasonic contrast medium is injected in a blood vessel of the subject to enhance viewing of blood flow through the blood vessels and perfused tissue. The ultrasonic contrast medium typically contains microbubbles that substantially increase the strength of signals emanating from blood and, therefore, preferentially enhance those signals vs signals received from tissue not perfused by blood.</p><p>[0004] However, ultrasound signals that provide a strong acoustic pressure can destroy the microbubbles in the blood vessels. Accordingly, the amplitude of the ultrasound signal is controlled to remain below a given limit to minimize microbubble destruction; correspondingly, the amplitude of the microbubble echoes is fairly small. Consequently, these low-amplitude microbubble echoes are often obscured by the larger echoes from the surrounding tissues.</p><p>[0005] Recognizing the benefits to be gained by enabling an ultrasound image to display blood flow in vessels as well as tissue perfusion, techniques have been developed to distinguish microbubble echoes from non-perfused tissue echoes based on the characteristics of the echoes. Tissue echoes generally exhibit a linear response, whereas microbubble echoes exhibit a primarily nonlinear response, and distinguishing microbubble echoes from tissue echoes is conventionally accomplished by canceling linear signals (‘tissue cancelation’) in the received echo signals.</p><p>[0006] Three techniques (or sub-modes) are commonly available for tissue cancelation: pulse inversion (PI), amplitude modulation (AM), and amplitude modulation pulse inversion (AMPI). In conventional contrast imaging ultrasound systems, each of the three sub-modes PI, AM, and AMPI, individually or in combination are used to create ultrasound images because each of these sub-modes has its own set of advantages and disadvantages with respect to resolution, bubble sensitivity, penetration, artifacts, and so on.</p><p>[0007] USPA 2005/0256404 discloses sequences of two pulses that can provide each of the PI, AM, and AMPI signals, and is incorporated by reference herein.</p><p>[0008] Pulse Inversion (PI) is illustrated in FIG. <b>1</b>A. Two pulses P<b>1</b>, P<b>2</b> are transmitted, each at an opposite phase. The received echo signals E<b>1</b>, E<b>2</b>, are provided to a unity (+1) gain amplifier <b>110</b> and combined at adder <b>120</b>. At the adder <b>120</b>, the opposing-phase signals cancel each other, removing the linear components of the signal due to non-perfused tissue echoes. The remainder signal PI is representative of the nonlinear components due to contrast microbubble echoes from blood vessels and blood-perfused tissue.</p><p>[0009] Amplitude Modulation (AM) is illustrated in FIG. <b>1</b>B. Two pulses are transmitted, one pulse P<b>1</b> at half amplitude, and one pulse P<b>2</b> at full amplitude, each at the same phase. The half amplitude echo E<b>1</b> is doubled via the +2 gain amplifier <b>112</b>, while the full amplitude echo E<b>2</b> is inverted at the negative (-1) unity gain amplifier <b>114</b>. When these signals are summed <b>120</b>, the resulting amplitude of the linear signals is zero, and the remainder signal AM is another representation of the nonlinear components (microbubble echoes).</p><p>[0010] Amplitude Modulation Pulse Inversion (AMPI), illustrated in FIG. <b>1</b>C, combines the AM and PI sub-modes, by setting the phase of the full aperture pulse P<b>2</b> to be opposite the phase of the half aperture pulse P<b>1</b> in the above AM sequence. In this embodiment, the negative (-1) unity gain amplifier <b>114</b> is replaced by a positive (+1) unity gain amplifier <b>110</b>, and the signals are combined <b>120</b>. The resulting amplitude of the opposing phase linear signals is zero, and the remaining signal AMPI is another representation of the nonlinear components (microbubble echoes).</p><p>[0011] USPA 2005/0256404 also discloses that the half amplitude pulses may be obtained by activating half of the ultrasound transducer elements. The transducer elements may be sequentially numbered, and, in an example embodiment, all of the odd numbered transducer elements are activated to produce half amplitude pulse P<b>1</b>(<i>o</i>), while all of the even numbered transducer elements are activated to produce half amplitude pulse P<b>2</b>(<i>e</i>), as illustrated in FIG. <b>1</b>D. One of skill in the art will recognize that the half amplitude pulses may be obtained in a variety of sequences, such as \\\"enable the first N/2 transducer elements, then the remaining N/2 transmitters\\\"; or, \\\"repeatedly enable every other set of K transducer elements of the N transducer elements, where N/K is an even integer\\\" (e.g. N=18, K=3: set 1={1, 2, 3, 7, 8, 9, 13, 14, 15}; set 2={4, 5, 6, 10, 11, 12, 16, 17, 18}); or, \\\"enable a random set of N/2 transducer elements, then enable the remainder N/2 transducer elements\\\"; etc. For ease of reference and understanding, the terms \\\"odd\\\" (P<b>1</b>(<i>o</i>)) and \\\"even\\\" (P<b>2</b>(<i>e</i>)) with respect to pulses of half amplitude are used hereinafter to signify alternate sets of half the transducer elements, regardless of how these sets are selected.</p><p>[0012] Also illustrated in FIG. <b>1</b>D are two full amplitude pulses P<b>3</b> and P<b>4</b> of opposing phase. As illustrated, this sequence of four pulses are sufficient to provide each of the PI, AM, and AMPI signals. Because two half amplitude echo signals E<b>1</b>(<i>o</i>), E<b>1</b>(<i>e</i>) due to pulses P<b>1</b>(<i>o</i>) and P<b>2</b>(<i>e</i>) are produced, there is no need to double the received half amplitude signals via the +2 gain amplifier <b>112</b> as in the examples of FIGS. <b>1</b>B and <b>1</b>C.</p><p>[0013] The use of four pulses to provide each of the PI, AM, and AMPI signals reduces the time required to acquire the three sets of signals compared to acquiring PI, AM, and AMPI independently, but does not necessarily improve the efficacy of reducing the amount of non-perfused tissue contained in the corresponding images, especially in the presence of tissue motion.</p><p>SUMMARY OF THE INVENTION</p><p>[0014] It would be advantageous to provide a system and method that improves the quality of contrast-enhanced ultrasonic images by reducing the amount of non-perfused tissue (subsequently referred to as \\\"tissue clutter\\\") that appears in the ultrasound images, particularly tissue artifacts caused by tissue motion.</p><p>[0015] To better address one or more of these concerns, in an embodiment of this invention, a particular sequence of ultrasound transmissions and corresponding echo receptions enable the production of AM and AMPI signals that are temporally balanced. Temporal balancing significantly reduces tissue artifacts caused by movement of tissue during acquisition of the ultrasound echoes. Additionally, in combining the selected echo signals to produce the PI, AM, and AMPI signals, each of the echo signals is equally weighted to facilitate an amplitude balance that can produce ideal AM, and AMPI summations.</p><p>[0016] In an example embodiment, the sequence of transmit pulses comprises: (+0.5o, +1, +0.5e, -1, +0.5o), wherein the +/- sign indicates the phase of the transmission, the numerals indicate the amplitude, wherein o/e indicates complementary half aperture transmissions.</p><p>[0017] To produce the PI signal the second (+1) and fourth (-1) echoes are summed.</p><p>[0018] To produce a temporally balanced AM signal, the second echo (+1) is subtracted from the sum of the first (+0.5o) and third echoes (+0.5e).</p><p>[0019] To produce a temporally balanced AMPI signal, the third (+0.5e), fourth (-1), and fifth (-0.5o) signals are summed.</p><p>[0020] Images based on these PI, AM, and AMPI signals, individually or in combination, are displayed to a user. The combinations may be based on the signal-to-noise ratio (SNR) of one or more of the signals, as well as the spectral response of one or more of the signals, to further enhance the display of blood flow and blood perfusion in the patient.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0021] The invention is explained in further detail, and by way of example, with reference to the accompanying drawings wherein:</p><p>[0022] FIGS. <b>1</b>A-<b>1</b>D illustrate the transmission and reception of ultrasound pulses to produce PI, AM, and AMPI signals.</p><p>[0023] FIGS. <b>2</b>A-<b>2</b>C illustrate the effects of tissue motion on a prior art sequence of ultrasound pulses.</p><p>[0024] FIG. <b>3</b> illustrates a sequence of ultrasound pulses that enables the production AM and AMPI signals with reduced motion-induced artifacts, and PI signals.</p><p>[0025] FIGS. <b>4</b>A-<b>4</b>C illustrate a combination of echo processors that provide the AM and AMPI signals with reduced motion-induced artifacts, and the PI signals from the echoes of the sequence of FIG. <b>3</b>.</p><p>[0026] FIGS. <b>5</b>A-<b>5</b>D illustrate ultrasound images with and without AM and AMPI temporally balanced echo signals.</p><p>[0027] FIG. <b>6</b> illustrates an example block diagram of an ultrasound system.</p><p>[0028] FIG. <b>7</b> illustrates an example flow diagram for creating a pulse sequence that produces temporally balanced AM and AMPI signals.</p><p>[0029] FIG. <b>8</b> illustrates a block diagram showing an example processor according to embodiments of the invention.</p></shortdesdrw><p>[0030] Throughout the drawings, the same reference numerals indicate similar or corresponding features or functions. The drawings are included for illustrative purposes and are not intended to limit the scope of the invention.</p><p>DETAILED DESCRIPTION</p><p>[0031] In the following description, for purposes of explanation rather than limitation, specific details are set forth such as the particular architecture, interfaces, techniques, etc., in order to provide a thorough understanding of the concepts of the invention. However, it will be apparent to those skilled in the art that the present invention may be practiced in other embodiments, which depart from these specific details. In like manner, the text of this description is directed to the example embodiments as illustrated in the Figures, and is not intended to limit the claimed invention beyond the limits expressly included in the claims. For purposes of simplicity and clarity, detailed descriptions of well-known devices, circuits, and methods are omitted so as not to obscure the description of the present invention with unnecessary detail.</p><p>[0032] As detailed above, the prior art systems that employ PI, AM, and AMPI sub-modes for distinguishing tissue from blood flow and blood perfusion rely on the transmission of a sequence of pulses that are in some way complementary such that certain combinations of the echo signals from these pulses result in a cancellation of the linear echo signals from a stationary object (tissue), while preserving the non-linear signals due emanating from microbubbles in vessels or perfused tissue. These sub-modes (PI, AM, and AMPI) are premised on the assumption that the echoes from the stationary object are consistent throughout the duration of the sequence of pulses.</p><p>[0033] However, it is known that routine anatomic processes, such as the respiration cycle, cardiac cycle, and so on, as well as small movements introduced by the operator holding the ultrasound transducer will often cause the (relatively) stationary object to move. In some situations, such movement can be minimized, for example by asking the patient to hold his/her breath, but in other situations such motion is beyond the patient's control. This tissue motion introduces a non-linear component to the otherwise linear echo signals from stationary tissue. These non-linear echo signals are not cancelled-out by the conventional PI, AM, or AMPI sub-modes, which causes a partial appearance of a residual tissue component (clutter) in the ultrasound image ('motion-induced tissue artifacts). These artifacts diminish the clarity of the images documenting blood flow and tissue perfusion, and in some instances also diminish the diagnostic information that can be extracted from such images.</p><p>[0034] Heretofore, the particular order of pulses in the sequence has been considered irrelevant to the resultant ultrasound image (\\\"the pulses in [an ultrasound] sequence can be transmitted in any order\\\" (USPA 2005/0124895, which is incorporated by reference herein, [0021]); however, the inventors have determined that the order of the sequence of pulses in each sub-mode can have a significant effect on the magnitude of the appearance of motion-induced tissue artifacts. In the interest of ease of understanding, the examples provided herein are primarily directed to the AM and AMPI sub-modes, and one of skill in the art would recognize that the same principles could be applied to the PI sub-mode, as detailed further below. The reason that the order of pulses in an ultrasound transmission affects the magnitude of the motion-induced artifacts is illustrated in FIGS. <b>2</b>A-<b>2</b>C, using the prior art pulse sequence of FIG. <b>1</b>D.</p><p>[0035] FIG. <b>2</b>A illustrates an example idealized reception of echo signals corresponding to the transmission of two half amplitude pulses (P<b>1</b>, P<b>2</b> of FIG. <b>1</b>D) followed by two full amplitude pulses (P<b>3</b>, P<b>4</b>), as these pulses are reflected from a stationary object at times T<b>1</b>, T<b>2</b>, T<b>3</b>, and T<b>4</b>. As illustrated, the AM signal from these echo signals amounts to E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>. Ideally, in the absence of any non-linear echoes, this sum is zero; accordingly, any residual signal corresponds to non-linear echoes; which are primarily the microbubble echoes of interest.</p><p>[0036] In FIG. <b>2</b>B, tissue motion is illustrated by the dotted line <b>210</b>. For illustrative effect, the echoes received at times T<b>1</b>, T<b>2</b>, T<b>3</b>, T<b>4</b> are superimposed on the motion line <b>210</b>. The echo signal E<b>2</b>(<i>e</i>) differs from echo signal E<b>1</b>(<i>o</i>) by a motion-induced difference d<b>1</b>, and echo signal E<b>3</b> differs from echo signal E<b>2</b>(<i>e</i>) by a motion-induced difference d<b>2</b>. Likewise, E<b>4</b> will differ from E<b>3</b>. As noted above, the echoes are superimposed on the motion line <b>210</b> for illustrative effect. The differences d<b>1</b> and d<b>2</b> represent a difference in the received echo signals due to the motion of the tissue, which is assumed to be related to the distances that the tissue has moved. That is, when the tissue moves continuously in one direction, the echo signals are affected monotonically (i.e. continuously increase or continually decrease). The differences d<b>1</b>, d<b>2</b> represent the parameters of the echo that are affected by the movement, not the distance that the tissue moves, per se. For ease of reference, these parameters are hereinafter referred to as the magnitude of the echo, and may refer to the magnitude of the amplitude of the echo, the magnitude of the frequency change of the echo, and so on, depending upon the technology used to process the echo signals. In like manner, the relative difference between the magnitudes of the echo signals may merely be referred to as being ‘smaller’ or ‘larger’.</p><p>[0037] In this example, motion introduces a reduction in the received signal strength over the time period corresponding to the transmitted pulse sequence. Therefore, each subsequent echo is illustrated as being lower along the line of motion <b>210</b> relative to its preceding pulse. However, in other cases, each subsequent echo may be higher along the line of motion <b>210</b> relative to its preceding pulse.</p><p>[0038] In this simple illustrative example, relative to the center of the three echoes E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), and E<b>3</b> at T<b>2</b>, E<b>1</b>(<i>o</i>) is larger than E<b>2</b>(<i>e</i>) by an amount d<b>1</b>, and E<b>3</b> is smaller than E<b>2</b>(<i>e</i>) by an amount d<b>2</b>. Therefore, as illustrated in the summing arrangement of FIG. <b>2</b>B, E<b>1</b>(<i>o</i>)+d<b>1</b>, E<b>2</b>(<i>e</i>), and E<b>3</b>-d<b>2</b> will appear at their respective inputs. The resultant AM signal will therefore be (E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>)+(d<b>1</b>+d<b>2</b>). The first term (E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>) is the same as the AM signal in the absence of motion, as illustrated in FIG. <b>2</b>A; therefore, the motion-induced effect on the AM signal amounts to (d<b>1</b>+d<b>2</b>) when the AM signal is produced by the P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b> pulse sequence.</p><p>[0039] In like manner, the AMPI signal produced by the combination of E<b>1</b>(<i>o</i>), E<b>2</b>(<b>3</b>), E<b>4</b> will be offset by an even larger amount, because the difference between E<b>2</b>(<i>e</i>) and E<b>4</b> in this AMPI case is larger than the distance d<b>2</b> between E<b>2</b>(<i>e</i>) and E<b>3</b> in the AM case, above.</p><p>[0040] Consider an alternative as illustrated in FIG. <b>2</b>C, wherein the order of pulses to: P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>), are changed with respect to FIG. <b>2</b>B. This change produces echo signals E<b>1</b>(<i>o</i>), E<b>3</b>, E<b>2</b>(<i>e</i>), in that order. Using the same explanation as in FIG. <b>2</b>B, relative to the center of these pulses along the line of motion <b>210</b> at T<b>2</b>, the received echo E<b>1</b>(<i>o</i>) is larger than E<b>3</b> by an amount d<b>1</b>, and the received echo E<b>2</b>(<i>e</i>) is smaller than E<b>3</b> by an amount d<b>2</b>. Correspondingly, the inputs to the summing arrangement will be expressed as E<b>1</b>(<i>o</i>)+d<b>1</b>, E<b>3</b>, E<b>2</b>(<i>e</i>)-d<b>2</b>. The output AM signal using this reordered sequence will therefore be (E<b>1</b>(<i>o</i>)-E<b>3</b>+E<b>2</b>(<i>e</i>))+(d<b>1</b>-d<b>2</b>). The term (E<b>1</b>(<i>o</i>)-E<b>3</b>+E<b>2</b>(<i>e</i>)) is equivalent to the AM signal in the absence of motion, as illustrated in FIG. <b>2</b>A. Accordingly, the difference caused by motion using the sequence P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>) is (d<b>1</b>-d<b>2</b>), as compared to the difference (d<b>1</b>+d<b>2</b>) caused by the same motion using the sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>.</p><p>[0041] It is significant to note that if the tissue motion has a relatively constant velocity, as is typical, for example, when the patient is inhaling or exhaling, the motion-induced differences d<b>1</b> and d<b>2</b> will be similar, and thus the difference (d<b>1</b>-d<b>2</b>) in the output AM signal due to motion using the sequence P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>) will generally be substantially less than the differences (d<b>1</b>+d<b>2</b>) in the output AM signal due to motion using the prior art sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>. This substantial improvement in tissue cancellation is clearly evident in the images shown in FIGS. <b>5</b>A, <b>5</b>B, as detailed further below.</p><p>[0042] To explain which order of pulse sequences will be more or less effective in reducing motion-induced artifacts, the concept of \\\"temporal balance\\\" is introduced. As discussed above, the underlying principle behind tissue cancelation in contrast-enhanced ultrasound images is the use of two complementary sets of signals to cancel the echoes of each other as much as possible. With regard to motion-induced artifacts, the time that each pulse is transmitted, or the time that each echo is received, must be taken into account, because the magnitude of the echo signals vary with time, as detailed above with regard to FIGS. <b>2</b>A-<b>2</b>C.</p><p>[0043] In FIG. <b>2</b>C, using the pulse sequence P<b>1</b>(<i>o</i>), P<b>2</b>, P<b>3</b>(<i>e</i>), P<b>4</b>, and the combination E<b>1</b>(<i>o</i>), E<b>2</b>, E<b>2</b>(<i>e</i>) to provide the AM signal, the P<b>1</b>(<i>o</i>) and P<b>3</b>(<i>e</i>) signals form one complement, and the P<b>2</b> signal forms the second complement. The motion-induced artifact of the AM signal is reduced compared to FIG. <b>2</b>B because the echoes of the first complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) occur on either side of the other complement (E<b>3</b>), providing a ‘balanced’ application of these complementary signals. Practically, this means that the summation of E<b>1</b>(<i>o</i>) and E<b>2</b>(<i>e</i>) produces a signal with a similar average displacement as E<b>3</b>. Contrarily, in FIG. <b>2</b>B, both of the echoes of the first complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) occur before the other complement (E<b>3</b>), so the effective displacement of the summed signals is different than E<b>3</b>.</p><p>[0044] In FIG. <b>2</b>C, the time of occurrence of the complement echo (E<b>3</b>) is T<b>2</b>. The times of occurrences of the signals in the other complement echoes (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) are T<b>1</b> and T<b>3</b>. When the signals in the complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) are combined, the time of occurrence of the entire complement of both signals is, effectively, their average time, T<b>2</b>. Because the effective time of occurrence of each of the sets of complementary signals is the same (i.e. at time T<b>2</b>), the set of signals E<b>1</b>(<i>o</i>), E<b>3</b>, E<b>2</b>(<i>e</i>) is said to be temporally balanced.</p><p>[0045] Conversely, in FIG. <b>2</b>B, the signals of the first set of complement E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>) occur at times T<b>1</b> and T<b>2</b>, with an effective time of occurrence of this set halfway between T<b>1</b> and T<b>2</b>, which is designated T<b>1</b>.<b>5</b> for convenience. The time of occurrence of the second set of complement E<b>3</b> is at T<b>3</b>, which is distant from T<b>1</b>.<b>5</b>. Accordingly, the sequence E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>3</b> is said to be temporally unbalanced by 1.5 time units (T<b>3</b>-T<b>1</b>.<b>5</b>=1.5 time units). The greater the difference between the effective time of occurrence of each complement, the greater the magnitude of the motion-induced artifacts. For example, the AMPI signal produced by the combination of E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>4</b> of FIG. <b>2</b>B has effective times of occurrences of each complement of T<b>1</b>.<b>5</b> (the midpoint between E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) and T<b>4</b>, for a difference of 2.5 time units (T<b>4</b>-T<b>1</b>.<b>5</b>=2.5 time units). This confirms the statements above that the AMPI signal with effective times of T<b>1</b>.<b>5</b> and T<b>4</b> using the sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>, P<b>4</b> will exhibit larger motion-induced artifacts than the AM signal that has effective times of T<b>1</b>.<b>5</b> and T<b>3</b> using that sequence.</p><p>[0046] However, the signals E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>4</b> of FIG. <b>2</b>C that are used to produce the AMPI signal are also not temporally balanced. The temporal center of E<b>1</b>(<i>o</i>) and E<b>2</b>(<i>e</i>) is at T<b>2</b>, whereas the temporal center of E<b>4</b> is at T<b>4</b>, which produces a temporal imbalance of 2 time units. This unbalance is less than the unbalance (2.5 time units) of the AMPI signal using the sequence in FIG. <b>2</b>B, but still not sufficient to substantially reduce the motion-induced tissue artifacts in the AMPI signal.</p><p>[0047] It is significant to note that a four pulse sequence comprising two half-amplitude pulses and two full-amplitude pulses of opposite phase, as commonly used in prior art systems, cannot be arranged in any order to provide a temporal balance of both the AM and AMPI signals. It is also significant to note that a temporal balance of a PI signal using unity gain amplifiers requires at least four pulses comprising two at each phase, in order to create a temporal balance point between each pair of pulses at each phase. For example, a sequence of full pulses at {phase1, phase2, phase2, phase1) results in an effective time for the phase2 signals at the half-way point between the two pulses at phase2, which is also the effective time for the phase1 signals: half-way between the two pulses at phase1. Two consecutive full pulses in a sequence (i.e. phase2 in the above sequence) rarely, if ever, occur in prior art systems because redundant transmissions would conventionally be considered inefficient.</p><p>[0048] FIG. <b>3</b> illustrates an example five pulse sequence that provides temporal balance for both AM and AMPI signals, and enables production of PI signals according to standard practice to those skilled in the art. This sequence is formed with three half-amplitude pulses P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), P<b>5</b>(<i>o</i>) separated from each other by two full-amplitude pulses P<b>2</b> and P<b>4</b>. P<b>2</b> and P<b>4</b> have a phase opposite to each other while P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), P<b>5</b>(<i>o</i>) are in phase with each other and with P<b>2</b>. In an equivalent embodiment, P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), and P<b>5</b>(<i>o</i>) could be in phase with P<b>1</b>. For ease of illustration and understanding, the follow symbols are hereinafter defined. \\\"o\\\"=half-amplitude odd; \\\"e\\\"=half-amplitude even; \\\"+\\\"=full amplitude, phase <b>1</b>; and \\\"-\\\"=full amplitude, phase <b>2</b>. Accordingly, the sequence of FIG. <b>3</b> can be referred to as sequence (o, +, e, -, o), and the prior art sequence of FIG. <b>1</b>D as sequence (o, e, +, -).</p><p>[0049] FIGS. <b>4</b>A-<b>4</b>C illustrate the echo combining units for providing PI, AM, and AMPI ultrasound signals using unity-gain amplifiers <b>110</b>, <b>114</b> and summers <b>120</b>.</p><p>[0050] FIG. <b>4</b>A illustrates the configuration for providing a PI signal based on the sum of full-amplitude, opposite phase, signals E<b>2</b> and E<b>4</b>. As noted above, the balance of the PI signal is not addressed with respect to this example; the temporal imbalance of the PI signal is two time-units (T<b>2</b>-T<b>4</b>) i.e. the center of signal E<b>2</b> is T<b>2</b> and the center of signal E<b>4</b> is T<b>4</b>.</p><p>[0051] FIG. <b>4</b>B illustrates the configuration for providing an AM signal based on the sum of two half-amplitude echoes E<b>1</b>(<i>o</i>), E<b>3</b>(<i>e</i>) and a negative full-amplitude echo -E<b>2</b>. The effective time of occurrence of the set E<b>1</b> (<i>o</i>), E<b>3</b> (<i>e</i>) is T<b>2</b> (the signals E<b>1</b>(<i>o</i>) and E<b>3</b> (<i>e</i>) are evenly displaced around a center point T<b>2</b>), and the effective time of occurrence of E<b>2</b> is also T<b>2</b> (the center of E<b>2</b> is time T<b>2</b>), thus providing a temporally balanced AM signal.</p><p>[0052] FIG. <b>4</b>C illustrates the configuration for providing an AMPI signal based on the sum of two half-amplitude echoes E<b>3</b>(<i>e</i>), E<b>5</b>(<i>o</i>) and a full-amplitude echo of opposite phase E<b>4</b>. The effective time of occurrence of the set E<b>3</b>(<i>e</i>) and E<b>5</b>(<i>o</i>) is T<b>4</b> (the signals E<b>3</b>(<i>e</i>) and E<b>5</b>(<i>o</i>) are evenly displaced around a center point T<b>4</b>), and the effective time of occurrence of E<b>4</b> is also T<b>4</b> (the center of E<b>4</b> is time T<b>4</b>), thus providing a temporally balanced AMPI signal.</p><p>[0053] As noted above, if the tissue motion is at a constant velocity, and the pulses are equally spaced, this sequence will substantially reduce motion-induced tissue artifacts in the AM and AMPI based images.</p><p>[0054] The PI signal is temporally unbalanced (T<b>2</b>, T<b>4</b>; unbalance of 2 time units), and will exhibit motion-induced artifacts. In embodiments of this invention, the AM or AMPI images, or both, may be compared to the PI images or combined with the PI images to identify and reduce motion-induced tissue artifacts from the PI images.</p><p>[0055] FIGS. <b>5</b>A-<b>5</b>D illustrate the substantial improvement that can be achieved in ultrasound imaging by providing temporally balanced pulse sequences to produce temporally balanced AM and AMPI signals.</p><p>[0056] FIG. <b>5</b>A illustrates an ultrasound image obtained by the prior art transmission sequence of (o, e, +, -) (FIG. <b>1</b>D), and using the o, e, and + echo signals to provide an AM signal (FIG. <b>1</b>D).</p><p>[0057] FIG. <b>5</b>B illustrates an ultrasound image obtained by the example sequence of this invention: (o, +, e, -, o), and using the first three echo signals (o, +, e) to provide a temporally balanced AM signal (FIG. <b>4</b>B).</p><p>[0058] A comparison of FIGS. <b>5</b>A and <b>5</b>B shows an example improvement at region <b>520</b> in FIG. <b>5</b>B compared to region <b>510</b> in FIG. <b>5</b>A. As can be seen, the ultrasound image of FIG. <b>5</b>A produced using the prior art sequence of (o, e, +, -) introduces substantial \\\"tissue clutter\\\" at <b>510</b>, due primarily to movement of the tissue during the acquisition of the echo signals (motion-induced tissue artifacts). By providing a transmission sequence (o, +, e, -, o) that enables reduction or elimination of motion-induced tissue artifacts by providing temporally balanced echo signals (o, +, e) in accordance with this invention, the region <b>520</b> exhibits substantially less tissue clutter compared to the region <b>510</b>.</p><p>[0059] FIG. <b>5</b>C illustrates an ultrasound image obtained by the prior art transmission sequence of (o, e, +, -), and using the o, e, and - echo signals to provide an AMPI signal (FIG. <b>1</b>D). As can be seen, and as discussed above, because of the larger temporal unbalance (T<b>1</b>.<b>5</b>-T<b>4</b>) of the echo signals used to produce the AMPI image of FIG. <b>5</b>C compared to the temporal unbalance (T<b>1</b>.<b>5</b>-T<b>3</b>) of the echo signals used to produce the AM image of FIG. <b>5</b>A, the degree of tissue clutter at region <b>530</b> in the AMPI image of FIG. <b>5</b>C is recognizably greater than the tissue clutter at region <b>510</b> produced in the AM image of FIG. <b>5</b>A.</p><p>[0060] FIG. <b>5</b>D illustrates an ultrasound image obtained by the example sequence of this invention (o, +, e, -, o) and using the last three e, -, and o echo signals to provide a temporally balanced AMPI signal. As can be seen, the amount of tissue clutter at <b>540</b> in FIG. <b>5</b>D is substantially less than the amount of tissue clutter at <b>530</b> in FIG. <b>5</b>C.</p><p>[0061] FIG. <b>6</b> illustrates an example block diagram of an ultrasound system <b>600</b> in accordance with an aspect of this invention.</p><p>[0062] A scanhead <b>610</b> includes a plurality of transducer elements <b>615</b> and a controller <b>630</b>. The plurality of transducer elements <b>615</b> transmit and receive ultrasound signals. The controller <b>630</b> determines whether signals are provided to the transducer elements for transmission or signals are received from the transducer elements, via a switch <b>620</b>. The transducer elements are typically configured as a matrix, with each transducer being sequentially numbered; in this manner, the transducer elements can provide half-amplitude signals by enabling either the odd or even numbered transducer elements as described above.</p><p>[0063] When the switch <b>620</b> is in the transmit state, the transmitter <b>650</b> provides a sequence of temporally balanced pulses <b>655</b> to the transducer elements. The transmitter <b>650</b> also notifies the controller <b>630</b> which set of transducer elements (all, odd, even) should be enabled for each pulse in the sequence, and the controller <b>630</b> controls the transducer elements <b>615</b> accordingly.</p><p>[0064] In the receive mode, the received echoes are directed to the beam former <b>640</b> by the switch <b>620</b>, then processed by the tissue canceller <b>660</b> to provide PI, AM, and AMPI signals. The echo signals are processed in the canceller <b>660</b> in the conventional manner, except that, because the transmitted pulses are temporally balanced, the results of the processing of the echoes for tissue cancellation via the AM or AMPI sub-mode at the canceller <b>660</b> will be substantially improved compared to the results of AM or AMPI processing of non-temporally balanced echoes.</p><p>[0065] The resultant PI, AM, and AMPI signals from the tissue canceller <b>660</b> are provided to an image processor that selectively creates images based on each of the PI, AM, and AMPI signals, or a combination of two or more of these signals. As noted above, each of these tissue-canceled PI, AM, and AMPI sub-modes has particular advantages and disadvantages. For example, each sub-mode PI, AM, and AMPI contains a unique frequency-dependent response from the microbubbles and tissue. Accordingly, in some embodiments a blending at each pixel, or each region can be determined by the strongest properties over several frequency bands. In some embodiments, the regions and frequency bands of each sub-mode containing highest microbubble signal-to-noise may be used to blend the pixels into a final image.</p><p>[0066] As noted above, the sub-mode data could be used to identify regions of tissue clutter in particular images, and the images could be masked or blended to suppresses unwanted tissue artifacts.</p><p>[0067] In like manner, changes in the bubble spectral response over depth/time could be used to change the proportion of blending, in situations where one sub-mode is dominant in an arterial phase and another sub-mode in a later phase. That is, for example, the AM output signal may exhibit less clutter during the arterial phase, and the AMPI output may exhibit less clutter during the portal venous phase, and a composite image may be formed by selectively blending the AM and AMPI outputs based on phase.</p><p>[0068] The images produced by the image processor <b>670</b> are communicated to a display device <b>680</b>. At the display, a variety of combinations of images can be displayed, including for example, displaying the images of each sub-mode PI, AM, and AMPI simultaneously to the clinician so that the different information contained in each sub-mode can be assessed. In like manner, images based on combinations of the sub-mode signals may be selectively displayed, with or without a simultaneous display of the underlying sub-mode images.</p><p>[0069] As noted above, the pulse sequence of FIG. <b>3</b> is only an example of a pulse sequence that provides temporally balanced AM and AMPI signals. FIG. <b>7</b> illustrates an example flow diagram <b>700</b> for producing a temporally balanced ultrasound pulse sequence(s) limited by a fixed number of pulses in the sequence. The sequence provides temporally balanced AM and AMPI signals, and, as detailed further below, may be extended to also provide temporally balanced PI signals. One of skill in the art will recognize that alternative processes may be used as well.</p><p>[0070] At <b>710</b> the number of different pulse forms (e.g. half-odd, half-even, full-phase1, full-phase2, or others) that are available for transmission are identified, and the number of pulses forming the sequence (typically five or more) is selected. Based on the available pulse forms and the number of pulses forming the pulse sequence, the possible sequences of these K pulse forms taken N at a time, with duplication, are determined, and filtered to eliminate any pulse sequence that does not enable PI, AM, and AMPI sub-modes (such as all pulses of the same phase, no half-amplitude pulses, etc.), at <b>715</b>.</p><p>[0071] The loop <b>720</b>-<b>785</b> processes each feasible sequence to determine whether a temporally balanced AM signal and a temporally balanced AMPI signal can be formed from this sequence.</p><p>[0072] At <b>720</b> the loop begins and incrementally tests each of the potential sequences identified at <b>715</b>, until a temporally balanced sequence is found (at <b>765</b>). Alternatively, all of the potential sequences may be tested to create a set of temporally balanced sequences. The sequences in this set may be assessed to select a preferred sequence, based on some other criteria, such as where in the sequence the temporal balance points lie.</p><p>[0073] At <b>725</b>, the set(s) of signals in the sequence that can be used to provide an AM signal are identified. The sequence may contain alternative arrangements (sets) of echoes to provide an AM signal. In the loop <b>730</b>-<b>780</b>, each of the sets of echoes are processed to determine if the set is temporally balanced. At <b>730</b>, the loop starts and incrementally selects each set of potential sequences for producing the AM signal identified at <b>725</b>. At <b>735</b>, the temporal centers of each of the sets of complementary signals (e.g. (half-odd, half-even), (full-phase1)) are determined, and compared at <b>740</b>. If this set of signals in the sequence cannot provide an AM temporal balance (‘No’ at <b>740</b>), the set is unsuitable, and the next AM set, if any, is processed.</p><p>[0074] If, at <b>740</b>, the sequence provides a temporally balanced AM signal, the sets of signals in the sequence that provide an AMPI signal are identified, at <b>745</b>, and the loop <b>750</b>-<b>775</b> determines whether each AMPI set also provides a temporally balanced AMPI signal. The temporal centers of the complementary sets of signals (e.g. (half-odd, half-even), (full-phase2)) are determined at <b>755</b>, and compared at <b>760</b>. If the temporal centers are the same, this sequence is suitable for providing both the temporally balanced AM signal and the temporally balanced AMPI signal, and this sequence and the sets of AM and AMPI pulse signals are selected, at <b>765</b>.</p><p>[0075] Having found a pulse sequence that provides temporally balanced AM and AMPI signals, further processing is unnecessary, and the process is terminated at <b>770</b>. Due to the fact that the possible sequences at <b>715</b> must be able to provide a PI signal, this selected sequence is assured to also provide a PI signal.</p><p>[0076] If the set of signals cannot provide a temporally balanced AMPI signal, at <b>760</b>, the next AMPI set, if any, is processed in the loop <b>750</b>-<b>775</b>. After the all of the AMPI sets for this sequence are determined not to provide a temporally balanced AMPI signal, the next set of AM signals, if any, are processed in the loop <b>730</b>-<b>780</b>. If the sequence is not able to provide temporally balanced AM and AMPI signals, the next sequence is assessed in the loop <b>720</b>-<b>785</b>. If no sequence can be found to provide temporally balanced AM and AMPI signals, the process is terminated without selecting a sequence, at <b>770</b>.</p><p>[0077] One of skill in the art will recognize that the flow diagram of FIG. <b>7</b> may be modified to find a sequence that provides temporally balanced PI, AM, and AMPI signals.</p><p>[0078] If the sequence provides AM balance at <b>740</b>, and AMPI balance at <b>760</b>, the process could be modified to subsequently determine if this sequence can also provide a balanced PI signal, using the same technique of identifying the sets of pulses that can be used to provide the PI signal, then assessing whether one of the sets provides a temporally balance PI signal. For example, by increasing N, from five pulses in the sequence to eight pulses, the modified process will identify a sequence (o, +, e, -, o, -e, +) that will provide temporally balanced PI, AM, and AMPI signals. The PI signal {+}, {-}, {-}, {+} from the 2<sup>nd</sup>, 4<sup>th</sup>, 6<sup>th</sup>, and 8<sup>th </sup>pulses is balanced at T<b>5</b>; the AM signal {o}, {+}, {e} from the 1<sup>st</sup>, 2<sup>nd </sup>and 3<sup>rd </sup>pulse is balanced at T<b>2</b>; and the AMPI signal {e}, {-}, {o} from the 3<sup>rd</sup>, 4<sup>th </sup>and 5<sup>th </sup>pulse is balanced at T<b>4</b>. Optionally, another AMPI signal {o}, {-}, {e} may be obtained from the 5<sup>th</sup>, 6<sup>th </sup>and 7<sup>th </sup>pulses, balanced at T<b>6</b>.</p><p>[0079] Although an increase in the size of the sequence consumes more time per sample, in certain situations, the reduction or elimination of motion-induced artifacts in each of the sub-modes may be worth the extra time. Additionally, because the AMPI signal can be provided by the e, o signals (T<b>4</b>) or the o, -, e signals (T<b>6</b>), both signals may be produced and combined to potentially provide an improved AMPI signal.</p><p>[0080] One of skill in the art will recognize that the flow diagram of FIG. <b>7</b> may also be used to provide different temporally balanced signals. For example, replacing \\\"AMPI\\\" in the blocks of FIG. <b>7</b> with \\\"PI\\\" will produce a sequence that provides temporally balanced AM and PI signals; replacing \\\"AM\\\" with \\\"PI\\\" in FIG. <b>7</b> will produce a sequence that provides temporally balanced PI and AMPI signals. In like manner, if the temporally unbalance signal (e.g. PI) is not expected to be used, the test at block <b>725</b> can omit the requirement that the sequence is able to produce this temporally unbalanced signal.</p><p>[0081] One of skill in the art will also recognize that the principles of this invention may be applied to any set of signals that are subject to motion-induced artifacts and use complementary sets of signals to cancel an underlying unwanted signal. That is, the flow diagram of FIG. <b>7</b>, and the example extensions, may be applied if different sub-modes and/or different pulse types are found to cancel the underlying unwanted signal.</p><p>[0082] While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description are to be considered illustrative or exemplary and not restrictive; the invention is not limited to the disclosed embodiments.</p><p>[0083] For example, it is possible to operate the invention in an embodiment wherein the order of the sequence is reversed, the phases are reversed, and so on, provided that the resultant sequence remains temporally balanced. That is, for the purposes of understanding this invention as disclosed in the claims, the use of the terms ‘positive’ and ‘negative’, ‘odd’ and ‘even’, are not absolute, but are relative to each other.</p><p>[0084] FIG. <b>8</b> is a block diagram illustrating an example processor <b>800</b> according to embodiments of the disclosure. Processor <b>800</b> may be used to implement one or more processors described herein, for example, any or all of the processing elements shown in FIG. <b>6</b>. Processor <b>800</b> may be any suitable processor type including, but not limited to, a microprocessor, a microcontroller, a digital signal processor (DSP), a field programmable array (FPGA) where the FPGA has been programmed to form a processor, a graphical processing unit (GPU), an application specific circuit (ASIC) where the ASIC has been designed to form a processor, or a combination thereof.</p><p>[0085] The processor <b>800</b> may include one or more cores <b>802</b>. The core <b>802</b> may include one or more arithmetic logic units (ALU) <b>804</b>. In some embodiments, the core <b>802</b> may include a floating point logic unit (FPLU) <b>806</b> and/or a digital signal processing unit (DSPU) <b>808</b> in addition to or instead of the ALU <b>804</b>.</p><p>[0086] The processor <b>800</b> may include one or more registers <b>812</b> communicatively coupled to the core <b>802</b>. The registers <b>812</b> may be implemented using dedicated logic gate circuits (e.g., flip-flops) and/or any memory technology. In some embodiments the registers <b>812</b> may be implemented using static memory. The register may provide data, instructions and addresses to the core <b>802</b>.</p><p>[0087] In some embodiments, processor <b>800</b> may include one or more levels of cache memory <b>810</b> communicatively coupled to the core <b>802</b>. The cache memory <b>810</b> may provide computer-readable instructions to the core <b>802</b> for execution. The cache memory <b>810</b> may provide data for processing by the core <b>802</b>. In some embodiments, the computer-readable instructions may have been provided to the cache memory <b>810</b> by a local memory, for example, local memory attached to the external bus <b>816</b>. The cache memory <b>810</b> may be implemented with any suitable cache memory type, for example, metal-oxide semiconductor (MOS) memory such as static random access memory (SRAM), dynamic random access memory (DRAM), and/or any other suitable memory technology.</p><p>[0088] The processor <b>800</b> may include a controller <b>814</b>, which may control input to the processor <b>800</b> from other processors and/or components included in a system (e.g., component BBB shown in FIG. B) and/or outputs from the processor <b>800</b> to other processors and/or components included in the system (e.g., component CCC shown in FIG. C). Controller <b>814</b> may control the data paths in the ALU <b>804</b>, FPLU <b>806</b> and/or DSPU <b>808</b>. Controller <b>814</b> may be implemented as one or more state machines, data paths and/or dedicated control logic. The gates of controller <b>814</b> may be implemented as standalone gates, FPGA, ASIC or any other suitable technology.</p><p>[0089] The registers <b>812</b> and the cache <b>810</b> may communicate with controller <b>814</b> and core <b>802</b> via internal connections <b>820</b>A, <b>820</b>B, <b>820</b>C and <b>820</b>D. Internal connections may be implemented as a bus, multiplexor, crossbar switch, and/or any other suitable connection technology.</p><p>[0090] Inputs and outputs for the processor <b>800</b> may be provided via a bus <b>816</b>, which may include one or more conductive lines. The bus <b>816</b> may be communicatively coupled to one or more components of processor <b>800</b>, for example the controller <b>814</b>, cache <b>810</b>, and/or register <b>812</b>. The bus <b>816</b> may be coupled to one or more components of the system, such as components BBB and CCC mentioned previously.</p><p>[0091] The bus <b>816</b> may be coupled to one or more external memories. The external memories may include Read Only Memory (ROM) <b>832</b>. ROM <b>832</b> may be a masked ROM, Electronically Programmable Read Only Memory (EPROM) or any other suitable technology. The external memory may include Random Access Memory (RAM) <b>833</b>. RAM <b>833</b> may be a static RAM, battery backed up static RAM, Dynamic RAM (DRAM) or any other suitable technology. The external memory may include Electrically Erasable Programmable Read Only Memory (EEPROM) <b>835</b>. The external memory may include Flash memory <b>834</b>. The External memory may include a magnetic storage device such as disc <b>836</b>. In some embodiments, the external memories may be included in a system, such as ultrasound imaging system <b>600</b> shown in FIG. <b>6</b>.</p><p>[0092] Although the processor <b>800</b> is shown as distinct from the external bus <b>816</b> and memories <b>832</b>, <b>834</b>, <b>833</b>, <b>835</b> and disc <b>836</b> some or all of these items may be part of the \\\"processor\\\" <b>800</b> in alternative embodiments. It should be understood that the term processor, processor system, computer, computer system controller or controller system may refer to the processor <b>800</b> alone or to the processor along with some or all of the elements <b>816</b>, <b>832</b>, <b>834</b>, <b>833</b>, <b>835</b> and <b>836</b>.</p><p>[0093] Other variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing the claimed invention, from a study of the drawings, the disclosure, and the appended claims. In the claims, the word \\\"comprising\\\" does not exclude other elements or steps, and the indefinite article \\\"a\\\" or \\\"an\\\" does not exclude a plurality. A single processor or other unit may fulfill the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. Reference numerals and symbols that appear in the claim are for ease of understanding only, and represent example embodiments; they are not intended to limit the scope of the claims. A computer program may be stored/distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. An ultrasound method for distinguishing non-perfused tissue from at least one of blood flow or blood perfusion comprising: <BR />transmitting a sequence of ultrasound signals including a first complement and a second complement to a patient via a plurality of ultrasound transducer elements;<BR />receiving a sequence of echo signals corresponding to the sequence of ultrasound signals;<BR />combining a first selected set of echo signals to produce an Amplitude Modulated (AM) signal;<BR />combining a second selected set of echo signals to produce an Amplitude Modulated Phase Inversion (AMPI) signal;<BR />creating at least one image based on at least one of the AM signal or the AMPI signal;<BR />displaying the at least one image;<BR />wherein at least one of an order of at least one of the first selected set of echo signals or the second selected set of echo signals is temporally balanced by having echo signals of the first complement in the first selected set or the second selected set occur on each side of an echo signal of the second complement in the first selected set or the second selected set, or center times of the first selected set of echo signals or the second selected set of echo signals are temporally balanced by having center times of echo signals of the first complement in the first selected set or the second selected set evenly displaced around a center time of an echo signal of the second complement in the first selected set or the second selected set;<BR />wherein a characteristic of each of the echo signals is set to be equal in each combining to produce the AM and AMPI signals.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of claim 1, <BR />wherein the sequence of ultrasound signals comprises a sequence of a first signal, a second signal, a third signal, a fourth signal, and a fifth signal;<BR />wherein the first signal and the fifth signal correspond to a transmission through a first half of the plurality of ultrasound transducer elements at a first phase;<BR />wherein the second signal corresponds to a transmission through the plurality of transducer elements at the first phase;<BR />wherein the third signal corresponds to a transmission through a second half of the plurality of ultrasound transducer elements at the first phase; and<BR />wherein the fourth signal corresponds to a transmission through the plurality of transducer elements at a second phase.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>3. The method of claim 2, <BR />wherein the first signal corresponds to a first echo signal,<BR />wherein the second signal corresponds to a second echo signal,<BR />wherein the third signal corresponds to a third echo signal.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>4. The method of claim 2, wherein the AMPI signal comprises a sum of the third signal, the fourth signal, and the fifth signal.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p>5. The method of claim 2, wherein the method comprises summing the second signal and the fourth signal to provide a Phase Inverted (PI) signal, and wherein the at least one image is also based on the PI signal.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>6. The method of claim 1, wherein the AMPI signal produced by combining the second selected set of echo signals is temporally balanced.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>7. The method of claim 1, wherein the at least one image is further based on a spectral response of at least one of the AM signal or the AMPI signal.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The method of claim 1, wherein the at least one image is further based on a signal-to-noise ratio of at least one of the AM signal or the AMPI signal.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>9. The method of claim 1, wherein the at least one image comprises at least two images, and wherein the method comprises simultaneously displaying the at least two images.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>10. The method of claim 1, wherein the at least one image comprises a combination image that is based on both the AM and AMPI signals.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>11. The method of claim 1, <BR />wherein each of the selected first set of echo signals and the selected second set of echo signals comprises a first set of echo signals and a second set of echo signals;<BR />wherein a form of the first set of echo signals and the second set of echo signals are complementary;<BR />wherein the first set of echo signals has a first center time;<BR />wherein the second set of echo signals has a second center time; and<BR />wherein the first center time is equal to the second center time.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>12. The method of claim 1, further comprising injecting contrast-enhancing microbubbles in a vessel of the patient; wherein the at least one image displays a flow of the microbubbles through the patient.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>13. An ultrasound system, comprising: <BR />a plurality of transducer elements configured to transmit a sequence of ultrasound pulses and to receive a sequence of echo signals in response to the sequence of ultrasound pulses; and<BR />a processing circuit configured to generate the sequence of ultrasound pulses and to process the sequence of echo signals as claimed in claim 1.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"yes\\\"><p>14. A non-transitory computer-readable medium that includes a program that, when executed by a processing system, causes the processing system to: <BR />transmit a sequence of ultrasound signals including a first complement and a second complement to a patient via a plurality of ultrasound transducer elements;<BR />receive a sequence of echo signals corresponding to the sequence of ultrasound signals;<BR />combine a first selected set of echo signals to produce an Amplitude Modulated (AM) signal;<BR />combine a second selected set of echo signals to produce an Amplitude Modulated Phase Inversion (AMPI) signal;<BR />create at least one image based on at least one of the AM signal or the AMPI signal;<BR />display the at least one image;<BR />wherein at least one of an order of each of the first selected set of echo signals and the second selected set of echo signals is temporally balanced by having echo signals of the first complement in the first selected set or the second selected set occur on each side of an echo signal of the second complement in the first selected set or the second selected set, or center times of the first selected set of echo signals and the second selected set of echo signals are temporally balanced by having center times of echo signals of the first complement in the first selected set or the second selected set evenly displaced around a center time of an echo signal of the second complement in the first selected set or the second selected set;<BR />wherein a characteristic of each of the echo signals is set to be equal in each combining to produce the AM and AMPI signals; and<BR />wherein the at least one image minimizes an appearance of motion-induced tissue artifacts in the images.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>15. The medium of claim 14, wherein: <BR />the sequence of ultrasound signals comprises a sequence of a first signal, a second signal, a third signal, a fourth signal, and a fifth signal;<BR />the first signal and the fifth signal correspond to a transmission through a first half of the plurality of ultrasound transducer elements at a first phase;<BR />the second signal corresponds to a transmission through the plurality of transducer elements at the first phase;<BR />the third signal corresponds to a transmission through a second half of the plurality of ultrasound transducer elements at the first phase; and<BR />the fourth signal corresponds to a transmission through the plurality of transducer elements at a second phase.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B8/00\",\n",
      "\n",
      "\"A61B8/06\",\n",
      "\n",
      "\"A61B8/08\",\n",
      "\n",
      "\"A61B8/14\",\n",
      "\n",
      "\"G01S15/89\",\n",
      "\n",
      "\"G01S7/52\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B8/06\",\n",
      "\n",
      "\"A61B8/14\",\n",
      "\n",
      "\"A61B8/463\",\n",
      "\n",
      "\"A61B8/5207\",\n",
      "\n",
      "\"A61B8/5276\",\n",
      "\n",
      "\"G01S15/8915\",\n",
      "\n",
      "\"G01S7/5202\",\n",
      "\n",
      "\"G01S7/52026\",\n",
      "\n",
      "\"G01S7/52039\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"PHILIPS KONINKLL NV\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190611,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US12213838\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20250204,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US12213838\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20400604,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82505014\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20210617360\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"TEMPORALLY BALANCED MULTI-MODE MASTER IMAGING SEQUENCE FOR ULTRASONIC CONTRAST IMAGING\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A particular sequence of ultrasound transmissions and corresponding echo receptions enables the production of Amplitude Modulated (AM) and Amplitude Modulated Phase Inverted (AMPI) signals that are temporally balanced. Temporal balancing significantly reduces tissue artifacts caused by movement of tissue during acquisition of the ultrasound echoes. Additionally, in combining the selected echo signals to produce the AM5 and AMPI signals, and optionally a Phase Inverted (PI) signal, each of the echo signals is equally weighted to facilitate an amplitude balance that avoids different echoes affecting the produced AM, AMPI, and PI signals differently.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<shortsum><p>FIELD OF THE INVENTION</p><p>[0001] This invention relates to the field of contrast-enhanced ultrasound (CEUS) imaging, and in particular to a method and system that employs advanced pulse sequences that combine the advantages of multiple CEUS pulsing schemes.</p><p>BACKGROUND OF THE INVENTION</p><p>[0002] Ultrasound images are created by transmitting ultrasonic pulses at varying amplitudes and frequencies, receiving the echoes corresponding to each transmitted pulse, and processing the echoes to create the image. Often, an ultrasonic contrast medium is injected in a blood vessel of the subject to enhance viewing of blood flow through the blood vessels and perfused tissue. The ultrasonic contrast medium typically contains microbubbles that substantially increase the strength of signals emanating from blood and, therefore, preferentially enhance those signals vs signals received from tissue not perfused by blood.</p><p>[0003] However, ultrasound signals that provide a strong acoustic pressure can destroy the microbubbles in the blood vessels. Accordingly, the amplitude of the ultrasound signal is controlled to remain below a given limit to minimize microbubble destruction; correspondingly, the amplitude of the microbubble echoes is fairly small. Consequently, these low-amplitude microbubble echoes are often obscured by the larger echoes from the surrounding tissues.</p><p>[0004] Recognizing the benefits to be gained by enabling an ultrasound image to display blood flow in vessels as well as tissue perfusion, techniques have been developed to distinguish microbubble echoes from non-perfused tissue echoes based on the characteristics of the echoes. Tissue echoes generally exhibit a linear response, whereas microbubble echoes exhibit a primarily nonlinear response, and distinguishing microbubble echoes from tissue echoes is conventionally accomplished by canceling linear signals (‘tissue cancelation’) in the received echo signals.</p><p>[0005] Three techniques (or sub-modes) are commonly available for tissue cancelation: pulse inversion (PI), amplitude modulation (AM), and amplitude modulation pulse inversion (AMPI). In conventional contrast imaging ultrasound systems, each of the three sub-modes PI, AM, and AMPI, individually or in combination are used to create ultrasound images because each of these sub-modes has its own set of advantages and disadvantages with respect to resolution, bubble sensitivity, penetration, artifacts, and so on.</p><p>[0006] USPA 2005/0256404 discloses sequences of two pulses that can provide each of the PI, AM, and AMPI signals, and is incorporated by reference herein.</p><p>[0007] Pulse Inversion (PI) is illustrated in FIG. 1A. Two pulses P<b>1</b>, P<b>2</b> are transmitted, each at an opposite phase. The received echo signals E<b>1</b>, E<b>2</b>, are provided to a unity (+1) gain amplifier <b>110</b> and combined at adder <b>120</b>. At the adder <b>120</b>, the opposing-phase signals cancel each other, removing the linear components of the signal due to non-perfused tissue echoes. The remainder signal PI is representative of the nonlinear components due to contrast microbubble echoes from blood vessels and blood-perfused tissue.</p><p>[0008] Amplitude Modulation (AM) is illustrated in FIG. 1B. Two pulses are transmitted, one pulse P<b>1</b> at half amplitude, and one pulse P<b>2</b> at full amplitude, each at the same phase. The half amplitude echo E<b>1</b> is doubled via the +2 gain amplifier <b>112</b>, while the full amplitude echo E<b>2</b> is inverted at the negative (-1) unity gain amplifier <b>114</b>. When these signals are summed <b>120</b>, the resulting amplitude of the linear signals is zero, and the remainder signal AM is another representation of the nonlinear components (microbubble echoes).</p><p>[0009] Amplitude Modulation Pulse Inversion (AMPI), illustrated in FIG. 1C, combines the AM and PI sub-modes, by setting the phase of the full aperture pulse P<b>2</b> to be opposite the phase of the half aperture pulse P<b>1</b> in the above AM sequence. In this embodiment, the negative (-1) unity gain amplifier <b>114</b> is replaced by a positive (+1) unity gain amplifier <b>110</b>, and the signals are combined <b>120</b>. The resulting amplitude of the opposing phase linear signals is zero, and the remaining signal AMPI is another representation of the nonlinear components (microbubble echoes).</p><p>[0010] USPA 2005/0256404 also discloses that the half amplitude pulses may be obtained by activating half of the ultrasound transducer elements. The transducer elements may be sequentially numbered, and, in an example embodiment, all of the odd numbered transducer elements are activated to produce half amplitude pulse P<b>1</b>(<i>o</i>), while all of the even numbered transducer elements are activated to produce half amplitude pulse P<b>2</b>(<i>e</i>), as illustrated in FIG. 1D. One of skill in the art will recognize that the half amplitude pulses may be obtained in a variety of sequences, such as \\\"enable the first N/2 transducer elements, then the remaining N/2 transmitters\\\"; or, \\\"repeatedly enable every other set of K transducer elements of the N transducer elements, where N/K is an even integer\\\" (e.g. N=18, K=3: set 1={1, 2, 3, 7, 8, 9, 13, 14, 15}; set 2={4, 5, 6, 10, 11, 12, 16, 17, 18}); or, \\\"enable a random set of N/2 transducer elements, then enable the remainder N/2 transducer elements\\\"; etc. For ease of reference and understanding, the terms \\\"odd\\\" (P<b>1</b>(<i>o</i>)) and \\\"even\\\" (P<b>2</b>(<i>e</i>)) with respect to pulses of half amplitude are used hereinafter to signify alternate sets of half the transducer elements, regardless of how these sets are selected.</p><p>[0011] Also illustrated in FIG. 1D are two full amplitude pulses P<b>3</b> and P<b>4</b> of opposing phase. As illustrated, this sequence of four pulses are sufficient to provide each of the PI, AM, and AMPI signals. Because two half amplitude echo signals E<b>1</b>(<i>o</i>), E<b>1</b>(<i>e</i>) due to pulses P<b>1</b>(<i>o</i>) and P<b>2</b>(<i>e</i>) are produced, there is no need to double the received half amplitude signals via the +2 gain amplifier <b>112</b> as in the examples of FIGS. 1B and 1C.</p><p>[0012] The use of four pulses to provide each of the PI, AM, and AMPI signals reduces the time required to acquire the three sets of signals compared to acquiring PI, AM, and AMPI independently, but does not necessarily improve the efficacy of reducing the amount of non-perfused tissue contained in the corresponding images, especially in the presence of tissue motion.</p><p>SUMMARY OF THE INVENTION</p><p>[0013] It would be advantageous to provide a system and method that improves the quality of contrast-enhanced ultrasonic images by reducing the amount of non-perfused tissue (subsequently referred to as \\\"tissue clutter\\\") that appears in the ultrasound images, particularly tissue artifacts caused by tissue motion.</p><p>[0014] To better address one or more of these concerns, in an embodiment of this invention, a particular sequence of ultrasound transmissions and corresponding echo receptions enable the production of AM and AMPI signals that are temporally balanced. Temporal balancing significantly reduces tissue artifacts caused by movement of tissue during acquisition of the ultrasound echoes. Additionally, in combining the selected echo signals to produce the PI, AM, and AMPI signals, each of the echo signals is equally weighted to facilitate an amplitude balance that can produce ideal AM, and AMPI summations.</p><p>[0015] In an example embodiment, the sequence of transmit pulses comprises: (+0.5o, +1, +0.5e, -1, +0.5o), wherein the +/- sign indicates the phase of the transmission, the numerals indicate the amplitude, wherein o/e indicates complementary half aperture transmissions.</p><p>[0016] To produce the PI signal the second (+1) and fourth (-1) echoes are summed.</p><p>[0017] To produce a temporally balanced AM signal, the second echo (+1) is subtracted from the sum of the first (+0.5o) and third echoes (+0.5e).</p><p>[0018] To produce a temporally balanced AMPI signal, the third (+0.5e), fourth (-1), and fifth (-0.5o) signals are summed.</p><p>[0019] Images based on these PI, AM, and AMPI signals, individually or in combination, are displayed to a user. The combinations may be based on the signal-to-noise ratio (SNR) of one or more of the signals, as well as the spectral response of one or more of the signals, to further enhance the display of blood flow and blood perfusion in the patient.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0020] The invention is explained in further detail, and by way of example, with reference to the accompanying drawings wherein:</p><p>[0021] FIGS. 1A-1D illustrate the transmission and reception of ultrasound pulses to produce PI, AM, and AMPI signals.</p><p>[0022] FIGS. 2A-2C illustrate the effects of tissue motion on a prior art sequence of ultrasound pulses.</p><p>[0023] FIG. 3 illustrates a sequence of ultrasound pulses that enables the production AM and AMPI signals with reduced motion-induced artifacts, and PI signals.</p><p>[0024] FIGS. 4A-4C illustrate a combination of echo processors that provide the AM and AMPI signals with reduced motion-induced artifacts, and the PI signals from the echoes of the sequence of FIG. 3.</p><p>[0025] FIGS. 5A-5D illustrate ultrasound images with and without AM and AMPI temporally balanced echo signals.</p><p>[0026] FIG. 6 illustrates an example block diagram of an ultrasound system.</p><p>[0027] FIG. 7 illustrates an example flow diagram for creating a pulse sequence that produces temporally balanced AM and AMPI signals.</p></shortdesdrw><p>[0028] Throughout the drawings, the same reference numerals indicate similar or corresponding features or functions. The drawings are included for illustrative purposes and are not intended to limit the scope of the invention.</p><p>DETAILED DESCRIPTION</p><p>[0029] In the following description, for purposes of explanation rather than limitation, specific details are set forth such as the particular architecture, interfaces, techniques, etc., in order to provide a thorough understanding of the concepts of the invention. However, it will be apparent to those skilled in the art that the present invention may be practiced in other embodiments, which depart from these specific details. In like manner, the text of this description is directed to the example embodiments as illustrated in the Figures, and is not intended to limit the claimed invention beyond the limits expressly included in the claims. For purposes of simplicity and clarity, detailed descriptions of well-known devices, circuits, and methods are omitted so as not to obscure the description of the present invention with unnecessary detail.</p><p>[0030] As detailed above, the prior art systems that employ PI, AM, and AMPI sub-modes for distinguishing tissue from blood flow and blood perfusion rely on the transmission of a sequence of pulses that are in some way complementary such that certain combinations of the echo signals from these pulses result in a cancellation of the linear echo signals from a stationary object (tissue), while preserving the non-linear signals due emanating from microbubbles in vessels or perfused tissue. These sub-modes (PI, AM, and AMPI) are premised on the assumption that the echoes from the stationary object are consistent throughout the duration of the sequence of pulses.</p><p>[0031] However, it is known that routine anatomic processes, such as the respiration cycle, cardiac cycle, and so on, as well as small movements introduced by the operator holding the ultrasound transducer will often cause the (relatively) stationary object to move. In some situations, such movement can be minimized, for example by asking the patient to hold his/her breath, but in other situations such motion is beyond the patient's control. This tissue motion introduces a non-linear component to the otherwise linear echo signals from stationary tissue. These non-linear echo signals are not cancelled-out by the conventional PI, AM, or AMPI sub-modes, which causes a partial appearance of a residual tissue component (clutter) in the ultrasound image ('motion-induced tissue artifacts). These artifacts diminish the clarity of the images documenting blood flow and tissue perfusion, and in some instances also diminish the diagnostic information that can be extracted from such images.</p><p>[0032] Heretofore, the particular order of pulses in the sequence has been considered irrelevant to the resultant ultrasound image (\\\"the pulses in [an ultrasound] sequence can be transmitted in any order\\\" (USPA 2005/0124895, which is incorporated by reference herein, [0021]); however, the inventors have determined that the order of the sequence of pulses in each sub-mode can have a significant effect on the magnitude of the appearance of motion-induced tissue artifacts. In the interest of ease of understanding, the examples provided herein are primarily directed to the AM and AMPI sub-modes, and one of skill in the art would recognize that the same principles could be applied to the PI sub-mode, as detailed further below. The reason that the order of pulses in an ultrasound transmission affects the magnitude of the motion-induced artifacts is illustrated in FIGS. 2A-2C, using the prior art pulse sequence of FIG. 1D.</p><p>[0033] FIG. 2A illustrates an example idealized reception of echo signals corresponding to the transmission of two half amplitude pulses (P<b>1</b>, P<b>2</b> of FIG. 1D) followed by two full amplitude pulses (P<b>3</b>, P<b>4</b>), as these pulses are reflected from a stationary object at times T<b>1</b>, T<b>2</b>, T<b>3</b>, and T<b>4</b>. As illustrated, the AM signal from these echo signals amounts to E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>. Ideally, in the absence of any non-linear echoes, this sum is zero; accordingly, any residual signal corresponds to non-linear echoes; which are primarily the microbubble echoes of interest.</p><p>[0034] In FIG. 2B, tissue motion is illustrated by the dotted line <b>210</b>. For illustrative effect, the echoes received at times T<b>1</b>, T<b>2</b>, T<b>3</b>, T<b>4</b> are superimposed on the motion line <b>210</b>. The echo signal E<b>2</b>(<i>e</i>) differs from echo signal E<b>1</b>(<i>o</i>) by a motion-induced difference d<b>1</b>, and echo signal E<b>3</b> differs from echo signal E<b>2</b>(<i>e</i>) by a motion-induced difference d<b>2</b>. Likewise, E<b>4</b> will differ from E<b>3</b>. As noted above, the echoes are superimposed on the motion line <b>210</b> for illustrative effect. The differences d<b>1</b> and d<b>2</b> represent a difference in the received echo signals due to the motion of the tissue, which is assumed to be related to the distances that the tissue has moved. That is, when the tissue moves continuously in one direction, the echo signals are affected monotonically (i.e. continuously increase or continually decrease). The differences d<b>1</b>, d<b>2</b> represent the parameters of the echo that are affected by the movement, not the distance that the tissue moves, per se. For ease of reference, these parameters are hereinafter referred to as the magnitude of the echo, and may refer to the magnitude of the amplitude of the echo, the magnitude of the frequency change of the echo, and so on, depending upon the technology used to process the echo signals. In like manner, the relative difference between the magnitudes of the echo signals may merely be referred to as being ‘smaller’ or ‘larger’.</p><p>[0035] In this example, motion introduces a reduction in the received signal strength over the time period corresponding to the transmitted pulse sequence. Therefore, each subsequent echo is illustrated as being lower along the line of motion <b>210</b> relative to its preceding pulse. However, in other cases, each subsequent echo may be higher along the line of motion <b>210</b> relative to its preceding pulse.</p><p>[0036] In this simple illustrative example, relative to the center of the three echoes E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), and E<b>3</b> at T<b>2</b>, E<b>1</b>(<i>o</i>) is larger than E<b>2</b>(<i>e</i>) by an amount d<b>1</b>, and E<b>3</b> is smaller than E<b>2</b>(<i>e</i>) by an amount d<b>2</b>. Therefore, as illustrated in the summing arrangement of FIG. 2B, E<b>1</b>(<i>o</i>)+d<b>1</b>, E<b>2</b>(<i>e</i>), and E<b>3</b>-d<b>2</b> will appear at their respective inputs. The resultant AM signal will therefore be (E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>)+(d<b>1</b>+d<b>2</b>). The first term (E<b>1</b>(<i>o</i>)+E<b>2</b>(<i>e</i>)-E<b>3</b>) is the same as the AM signal in the absence of motion, as illustrated in FIG. 2A; therefore, the motion-induced effect on the AM signal amounts to (d<b>1</b>+d<b>2</b>) when the AM signal is produced by the P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b> pulse sequence.</p><p>[0037] In like manner, the AMPI signal produced by the combination of E<b>1</b>(<i>o</i>), E<b>2</b>(<b>3</b>), E<b>4</b> will be offset by an even larger amount, because the difference between E<b>2</b>(<i>e</i>) and E<b>4</b> in this AMPI case is larger than the distance d<b>2</b> between E<b>2</b>(<i>e</i>) and E<b>3</b> in the AM case, above.</p><p>[0038] Consider an alternative as illustrated in FIG. 2C, wherein the order of pulses to: P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>), are changed with respect to FIG. 2B. This change produces echo signals E<b>1</b>(<i>o</i>), E<b>3</b>, E<b>2</b>(<i>e</i>), in that order. Using the same explanation as in FIG. 2B, relative to the center of these pulses along the line of motion <b>210</b> at T<b>2</b>, the received echo E<b>1</b>(<i>o</i>) is larger than E<b>3</b> by an amount d<b>1</b>, and the received echo E<b>2</b>(<i>e</i>) is smaller than E<b>3</b> by an amount d<b>2</b>. Correspondingly, the inputs to the summing arrangement will be expressed as E<b>1</b>(<i>o</i>)+d<b>1</b>, E<b>3</b>, E<b>2</b>(<i>e</i>)-d<b>2</b>. The output AM signal using this reordered sequence will therefore be (E<b>1</b>(<i>o</i>)-E<b>3</b>+E<b>2</b>(<i>e</i>))+(d<b>1</b>-d<b>2</b>). The term (E<b>1</b>(<i>o</i>)-E<b>3</b>+E<b>2</b>(<i>e</i>)) is equivalent to the AM signal in the absence of motion, as illustrated in FIG. 2A. Accordingly, the difference caused by motion using the sequence P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>) is (d<b>1</b>-d<b>2</b>), as compared to the difference (d<b>1</b>+d<b>2</b>) caused by the same motion using the sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>.</p><p>[0039] It is significant to note that if the tissue motion has a relatively constant velocity, as is typical, for example, when the patient is inhaling or exhaling, the motion-induced differences d<b>1</b> and d<b>2</b> will be similar, and thus the difference (d<b>1</b>-d<b>2</b>) in the output AM signal due to motion using the sequence P<b>1</b>(<i>o</i>), P<b>3</b>, P<b>2</b>(<i>e</i>) will generally be substantially less than the differences (d<b>1</b>+d<b>2</b>) in the output AM signal due to motion using the prior art sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>. This substantial improvement in tissue cancellation is clearly evident in the images shown in FIGS. 5A, 5B, as detailed further below.</p><p>[0040] To explain which order of pulse sequences will be more or less effective in reducing motion-induced artifacts, the concept of \\\"temporal balance\\\" is introduced. As discussed above, the underlying principle behind tissue cancelation in contrast-enhanced ultrasound images is the use of two complementary sets of signals to cancel the echoes of each other as much as possible. With regard to motion-induced artifacts, the time that each pulse is transmitted, or the time that each echo is received, must be taken into account, because the magnitude of the echo signals vary with time, as detailed above with regard to FIGS. 2A-2C.</p><p>[0041] In FIG. 2C, using the pulse sequence P<b>1</b>(<i>o</i>), P<b>2</b>, P<b>3</b>(<i>e</i>), P<b>4</b>, and the combination E<b>1</b>(<i>o</i>), E<b>2</b>, E<b>2</b>(<i>e</i>) to provide the AM signal, the P<b>1</b>(<i>o</i>) and P<b>3</b>(<i>e</i>) signals form one complement, and the P<b>2</b> signal forms the second complement. The motion-induced artifact of the AM signal is reduced compared to FIG. 2B because the echoes of the first complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) occur on either side of the other complement (E<b>3</b>), providing a ‘balanced’ application of these complementary signals. Practically, this means that the summation of E<b>1</b>(<i>o</i>) and E<b>2</b>(<i>e</i>) produces a signal with a similar average displacement as E<b>3</b>. Contrarily, in FIG. 2B, both of the echoes of the first complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) occur before the other complement (E<b>3</b>), so the effective displacement of the summed signals is different than E<b>3</b>.</p><p>[0042] In FIG. 2C, the time of occurrence of the complement echo (E<b>3</b>) is T<b>2</b>. The times of occurrences of the signals in the other complement echoes (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) are T<b>1</b> and T<b>3</b>. When the signals in the complement (E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) are combined, the time of occurrence of the entire complement of both signals is, effectively, their average time, T<b>2</b>. Because the effective time of occurrence of each of the sets of complementary signals is the same (i.e. at time T<b>2</b>), the set of signals E<b>1</b>(<i>o</i>), E<b>3</b>, E<b>2</b>(<i>e</i>) is said to be temporally balanced.</p><p>[0043] Conversely, in FIG. 2B, the signals of the first set of complement E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>) occur at times T<b>1</b> and T<b>2</b>, with an effective time of occurrence of this set halfway between T<b>1</b> and T<b>2</b>, which is designated T<b>1</b>.<b>5</b> for convenience. The time of occurrence of the second set of complement E<b>3</b> is at T<b>3</b>, which is distant from T<b>1</b>.<b>5</b>. Accordingly, the sequence E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>3</b> is said to be temporally unbalanced by 1.5 time units (T<b>3</b>-T<b>1</b>.<b>5</b>=1.5 time units). The greater the difference between the effective time of occurrence of each complement, the greater the magnitude of the motion-induced artifacts. For example, the AMPI signal produced by the combination of E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>4</b> of FIG. 2B has effective times of occurrences of each complement of T<b>1</b>.<b>5</b> (the midpoint between E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>)) and T<b>4</b>, for a difference of 2.5 time units (T<b>4</b>-T<b>1</b>.<b>5</b>=2.5 time units). This confirms the statements above that the AMPI signal with effective times of T<b>1</b>.<b>5</b> and T<b>4</b> using the sequence P<b>1</b>(<i>o</i>), P<b>2</b>(<i>e</i>), P<b>3</b>, P<b>4</b> will exhibit larger motion-induced artifacts than the AM signal that has effective times of T<b>1</b>.<b>5</b> and T<b>3</b> using that sequence.</p><p>[0044] However, the signals E<b>1</b>(<i>o</i>), E<b>2</b>(<i>e</i>), E<b>4</b> of FIG. 2C that are used to produce the AMPI signal are also not temporally balanced. The temporal center of E<b>1</b>(<i>o</i>) and E<b>2</b>(<i>e</i>) is at T<b>2</b>, whereas the temporal center of E<b>4</b> is at T<b>4</b>, which produces a temporal imbalance of 2 time units. This unbalance is less than the unbalance (2.5 time units) of the AMPI signal using the sequence in FIG. 2B, but still not sufficient to substantially reduce the motion-induced tissue artifacts in the AMPI signal.</p><p>[0045] It is significant to note that a four pulse sequence comprising two half-amplitude pulses and two full-amplitude pulses of opposite phase, as commonly used in prior art systems, cannot be arranged in any order to provide a temporal balance of both the AM and AMPI signals. It is also significant to note that a temporal balance of a PI signal using unity gain amplifiers requires at least four pulses comprising two at each phase, in order to create a temporal balance point between each pair of pulses at each phase. For example, a sequence of full pulses at {phase1, phase2, phase2, phase1) results in an effective time for the phase2 signals at the half-way point between the two pulses at phase2, which is also the effective time for the phase1 signals: half-way between the two pulses at phase1. Two consecutive full pulses in a sequence (i.e. phase2 in the above sequence) rarely, if ever, occur in prior art systems because redundant transmissions would conventionally be considered inefficient.</p><p>[0046] FIG. 3 illustrates an example five pulse sequence that provides temporal balance for both AM and AMPI signals, and enables production of PI signals according to standard practice to those skilled in the art. This sequence is formed with three half-amplitude pulses P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), P<b>5</b>(<i>o</i>) separated from each other by two full-amplitude pulses P<b>2</b> and P<b>4</b>. P<b>2</b> and P<b>4</b> have a phase opposite to each other while P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), P<b>5</b>(<i>o</i>) are in phase with each other and with P<b>2</b>. In an equivalent embodiment, P<b>1</b>(<i>o</i>), P<b>3</b>(<i>e</i>), and P<b>5</b>(<i>o</i>) could be in phase with P<b>1</b>. For ease of illustration and understanding, the follow symbols are hereinafter defined. \\\"o\\\"=half-amplitude odd; \\\"e\\\"=half-amplitude even; \\\"+\\\"=full amplitude, phase <b>1</b>; and \\\"-\\\"=full amplitude, phase <b>2</b>. Accordingly, the sequence of FIG. 3 can be referred to as sequence (o, +, e, -, o), and the prior art sequence of FIG. 1D as sequence (o, e, +, -).</p><p>[0047] FIGS. 4A-4C illustrate the echo combining units for providing PI, AM, and AMPI ultrasound signals using unity-gain amplifiers <b>110</b>, <b>114</b> and summers <b>120</b>.</p><p>[0048] FIG. 4A illustrates the configuration for providing a PI signal based on the sum of full-amplitude, opposite phase, signals E<b>2</b> and E<b>4</b>. As noted above, the balance of the PI signal is not addressed with respect to this example; the temporal imbalance of the PI signal is two time-units (T<b>2</b>-T<b>4</b>) i.e. the center of signal E<b>2</b> is T<b>2</b> and the center of signal E<b>4</b> is T<b>4</b>.</p><p>[0049] FIG. 4B illustrates the configuration for providing an AM signal based on the sum of two half-amplitude echoes E<b>1</b>(<i>o</i>), E<b>3</b>(<i>e</i>) and a negative full-amplitude echo -E<b>2</b>. The effective time of occurrence of the set E<b>1</b> (<i>o</i>), E<b>3</b> (<i>e</i>) is T<b>2</b> (the signals E<b>1</b>(<i>o</i>) and E<b>3</b> (<i>e</i>) are evenly displaced around a center point T<b>2</b>), and the effective time of occurrence of E<b>2</b> is also T<b>2</b> (the center of E<b>2</b> is time T<b>2</b>), thus providing a temporally balanced AM signal.</p><p>[0050] FIG. 4C illustrates the configuration for providing an AMPI signal based on the sum of two half-amplitude echoes E<b>3</b>(<i>e</i>), E<b>5</b>(<i>o</i>) and a full-amplitude echo of opposite phase E<b>4</b>. The effective time of occurrence of the set E<b>3</b>(<i>e</i>) and E<b>5</b>(<i>o</i>) is T<b>4</b> (the signals E<b>3</b>(<i>e</i>) and E<b>5</b>(<i>o</i>) are evenly displaced around a center point T<b>4</b>), and the effective time of occurrence of E<b>4</b> is also T<b>4</b> (the center of E<b>4</b> is time T<b>4</b>), thus providing a temporally balanced AMPI signal.</p><p>[0051] As noted above, if the tissue motion is at a constant velocity, and the pulses are equally spaced, this sequence will substantially reduce motion-induced tissue artifacts in the AM and AMPI based images.</p><p>[0052] The PI signal is temporally unbalanced (T<b>2</b>, T<b>4</b>; unbalance of 2 time units), and will exhibit motion-induced artifacts. In embodiments of this invention, the AM or AMPI images, or both, may be compared to the PI images or combined with the PI images to identify and reduce motion-induced tissue artifacts from the PI images.</p><p>[0053] FIGS. 5A-5D illustrate the substantial improvement that can be achieved in ultrasound imaging by providing temporally balanced pulse sequences to produce temporally balanced AM and AMPI signals.</p><p>[0054] FIG. 5A illustrates an ultrasound image obtained by the prior art transmission sequence of (o, e, +, -) (FIG. 1D), and using the o, e, and + echo signals to provide an AM signal (FIG. 1D).</p><p>[0055] FIG. 5B illustrates an ultrasound image obtained by the example sequence of this invention: (o, +, e, -, o), and using the first three echo signals (o, +, e) to provide a temporally balanced AM signal (FIG. 4B).</p><p>[0056] A comparison of FIGS. 5A and 5B shows an example improvement at region <b>520</b> in FIG. 5B compared to region <b>510</b> in FIG. 5A. As can be seen, the ultrasound image of FIG. 5A produced using the prior art sequence of (o, e, +, -) introduces substantial \\\"tissue clutter\\\" at <b>510</b>, due primarily to movement of the tissue during the acquisition of the echo signals (motion-induced tissue artifacts). By providing a transmission sequence (o, +, e, -, o) that enables reduction or elimination of motion-induced tissue artifacts by providing temporally balanced echo signals (o, +, e) in accordance with this invention, the region <b>520</b> exhibits substantially less tissue clutter compared to the region <b>510</b>.</p><p>[0057] FIG. 5C illustrates an ultrasound image obtained by the prior art transmission sequence of (o, e, +, -), and using the o, e, and - echo signals to provide an AMPI signal (FIG. 1D). As can be seen, and as discussed above, because of the larger temporal unbalance (T<b>1</b>.<b>5</b>-T<b>4</b>) of the echo signals used to produce the AMPI image of FIG. 5C compared to the temporal unbalance (T<b>1</b>.<b>5</b>-T<b>3</b>) of the echo signals used to produce the AM image of FIG. 5A, the degree of tissue clutter at region <b>530</b> in the AMPI image of FIG. 5C is recognizably greater than the tissue clutter at region <b>510</b> produced in the AM image of FIG. 5A.</p><p>[0058] FIG. 5D illustrates an ultrasound image obtained by the example sequence of this invention (o, +, e, -, o) and using the last three e, -, and o echo signals to provide a temporally balanced AMPI signal. As can be seen, the amount of tissue clutter at <b>540</b> in FIG. 5D is substantially less than the amount of tissue clutter at <b>530</b> in FIG. 5C.</p><p>[0059] FIG. 6 illustrates an example block diagram of an ultrasound system <b>600</b> in accordance with an aspect of this invention.</p><p>[0060] A scanhead <b>610</b> includes a plurality of transducer elements <b>615</b> and a controller <b>630</b>. The plurality of transducer elements <b>615</b> transmit and receive ultrasound signals. The controller <b>630</b> determines whether signals are provided to the transducer elements for transmission or signals are received from the transducer elements, via a switch <b>620</b>. The transducer elements are typically configured as a matrix, with each transducer being sequentially numbered; in this manner, the transducer elements can provide half-amplitude signals by enabling either the odd or even numbered transducer elements as described above.</p><p>[0061] When the switch <b>620</b> is in the transmit state, the transmitter <b>650</b> provides a sequence of temporally balanced pulses <b>655</b> to the transducer elements. The transmitter <b>650</b> also notifies the controller <b>630</b> which set of transducer elements (all, odd, even) should be enabled for each pulse in the sequence, and the controller <b>630</b> controls the transducer elements <b>615</b> accordingly.</p><p>[0062] In the receive mode, the received echoes are directed to the beam former <b>640</b> by the switch <b>620</b>, then processed by the tissue canceller <b>660</b> to provide PI, AM, and AMPI signals. The echo signals are processed in the canceller <b>660</b> in the conventional manner, except that, because the transmitted pulses are temporally balanced, the results of the processing of the echoes for tissue cancellation via the AM or AMPI sub-mode at the canceller <b>660</b> will be substantially improved compared to the results of AM or AMPI processing of non-temporally balanced echoes.</p><p>[0063] The resultant PI, AM, and AMPI signals from the tissue canceller <b>660</b> are provided to an image processor that selectively creates images based on each of the PI, AM, and AMPI signals, or a combination of two or more of these signals. As noted above, each of these tissue-canceled PI, AM, and AMPI sub-modes has particular advantages and disadvantages. For example, each sub-mode PI, AM, and AMPI contains a unique frequency-dependent response from the microbubbles and tissue. Accordingly, in some embodiments a blending at each pixel, or each region can be determined by the strongest properties over several frequency bands. In some embodiments, the regions and frequency bands of each sub-mode containing highest microbubble signal-to-noise may be used to blend the pixels into a final image.</p><p>[0064] As noted above, the sub-mode data could be used to identify regions of tissue clutter in particular images, and the images could be masked or blended to suppresses unwanted tissue artifacts.</p><p>[0065] In like manner, changes in the bubble spectral response over depth/time could be used to change the proportion of blending, in situations where one sub-mode is dominant in an arterial phase and another sub-mode in a later phase. That is, for example, the AM output signal may exhibit less clutter during the arterial phase, and the AMPI output may exhibit less clutter during the portal venous phase, and a composite image may be formed by selectively blending the AM and AMPI outputs based on phase.</p><p>[0066] The images produced by the image processor <b>670</b> are communicated to a display device <b>680</b>. At the display, a variety of combinations of images can be displayed, including for example, displaying the images of each sub-mode PI, AM, and AMPI simultaneously to the clinician so that the different information contained in each sub-mode can be assessed. In like manner, images based on combinations of the sub-mode signals may be selectively displayed, with or without a simultaneous display of the underlying sub-mode images.</p><p>[0067] As noted above, the pulse sequence of FIG. 3 is only an example of a pulse sequence that provides temporally balanced AM and AMPI signals. FIG. 7 illustrates an example flow diagram <b>700</b> for producing a temporally balanced ultrasound pulse sequence(s) limited by a fixed number of pulses in the sequence. The sequence provides temporally balanced AM and AMPI signals, and, as detailed further below, may be extended to also provide temporally balanced PI signals. One of skill in the art will recognize that alternative processes may be used as well.</p><p>[0068] At <b>710</b> the number of different pulse forms (e.g. half-odd, half-even, full-phase1, full-phase2, or others) that are available for transmission are identified, and the number of pulses forming the sequence (typically five or more) is selected. Based on the available pulse forms and the number of pulses forming the pulse sequence, the possible sequences of these K pulse forms taken N at a time, with duplication, are determined, and filtered to eliminate any pulse sequence that does not enable PI, AM, and AMPI sub-modes (such as all pulses of the same phase, no half-amplitude pulses, etc.), at <b>715</b>.</p><p>[0069] The loop <b>720</b>-<b>785</b> processes each feasible sequence to determine whether a temporally balanced AM signal and a temporally balanced AMPI signal can be formed from this sequence.</p><p>[0070] At <b>720</b> the loop begins and incrementally tests each of the potential sequences identified at <b>715</b>, until a temporally balanced sequence is found (at <b>765</b>). Alternatively, all of the potential sequences may be tested to create a set of temporally balanced sequences. The sequences in this set may be assessed to select a preferred sequence, based on some other criteria, such as where in the sequence the temporal balance points lie.</p><p>[0071] At <b>725</b>, the set(s) of signals in the sequence that can be used to provide an AM signal are identified. The sequence may contain alternative arrangements (sets) of echoes to provide an AM signal. In the loop <b>730</b>-<b>780</b>, each of the sets of echoes are processed to determine if the set is temporally balanced. At <b>730</b>, the loop starts and incrementally selects each set of potential sequences for producing the AM signal identified at <b>725</b>. At <b>735</b>, the temporal centers of each of the sets of complementary signals (e.g. (half-odd, half-even), (full-phase1)) are determined, and compared at <b>740</b>. If this set of signals in the sequence cannot provide an AM temporal balance (‘No’ at <b>740</b>), the set is unsuitable, and the next AM set, if any, is processed.</p><p>[0072] If, at <b>740</b>, the sequence provides a temporally balanced AM signal, the sets of signals in the sequence that provide an AMPI signal are identified, at <b>745</b>, and the loop <b>750</b>-<b>775</b> determines whether each AMPI set also provides a temporally balanced AMPI signal. The temporal centers of the complementary sets of signals (e.g. (half-odd, half-even), (full-phase2)) are determined at <b>755</b>, and compared at <b>760</b>. If the temporal centers are the same, this sequence is suitable for providing both the temporally balanced AM signal and the temporally balanced AMPI signal, and this sequence and the sets of AM and AMPI pulse signals are selected, at <b>765</b>.</p><p>[0073] Having found a pulse sequence that provides temporally balanced AM and AMPI signals, further processing is unnecessary, and the process is terminated at <b>770</b>. Due to the fact that the possible sequences at <b>715</b> must be able to provide a PI signal, this selected sequence is assured to also provide a PI signal.</p><p>[0074] If the set of signals cannot provide a temporally balanced AMPI signal, at <b>760</b>, the next AMPI set, if any, is processed in the loop <b>750</b>-<b>775</b>. After the all of the AMPI sets for this sequence are determined not to provide a temporally balanced AMPI signal, the next set of AM signals, if any, are processed in the loop <b>730</b>-<b>780</b>. If the sequence is not able to provide temporally balanced AM and AMPI signals, the next sequence is assessed in the loop <b>720</b>-<b>785</b>. If no sequence can be found to provide temporally balanced AM and AMPI signals, the process is terminated without selecting a sequence, at <b>770</b>.</p><p>[0075] One of skill in the art will recognize that the flow diagram of FIG. 7 may be modified to find a sequence that provides temporally balanced PI, AM, and AMPI signals.</p><p>[0076] If the sequence provides AM balance at <b>740</b>, and AMPI balance at <b>760</b>, the process could be modified to subsequently determine if this sequence can also provide a balanced PI signal, using the same technique of identifying the sets of pulses that can be used to provide the PI signal, then assessing whether one of the sets provides a temporally balance PI signal. For example, by increasing N, from five pulses in the sequence to eight pulses, the modified process will identify a sequence (o, +, e, -, o, -e, +) that will provide temporally balanced PI, AM, and AMPI signals. The PI signal {+}, {-}, {-}, {+} from the 2<sup>nd</sup>, 4<sup>th</sup>, 6<sup>th</sup>, and 8<sup>th </sup>pulses is balanced at T<b>5</b>; the AM signal {o}, {+}, {e} from the 1<sup>st</sup>, 2<sup>nd </sup>and 3<sup>rd </sup>pulse is balanced at T<b>2</b>; and the AMPI signal {e}, {-}, {o} from the 3<sup>rd</sup>, 4<sup>th </sup>and 5<sup>th </sup>pulse is balanced at T<b>4</b>. Optionally, another AMPI signal {o}, {-}, {e} may be obtained from the 5<sup>th</sup>, 6<sup>th </sup>and 7<sup>th </sup>pulses, balanced at T<b>6</b>.</p><p>[0077] Although an increase in the size of the sequence consumes more time per sample, in certain situations, the reduction or elimination of motion-induced artifacts in each of the sub-modes may be worth the extra time. Additionally, because the AMPI signal can be provided by the e, o signals (T<b>4</b>) or the o, -, e signals (T<b>6</b>), both signals may be produced and combined to potentially provide an improved AMPI signal.</p><p>[0078] One of skill in the art will recognize that the flow diagram of FIG. 7 may also be used to provide different temporally balanced signals. For example, replacing \\\"AMPI\\\" in the blocks of FIG. 7 with \\\"PI\\\" will produce a sequence that provides temporally balanced AM and PI signals; replacing \\\"AM\\\" with \\\"PI\\\" in FIG. 7 will produce a sequence that provides temporally balanced PI and AMPI signals. In like manner, if the temporally unbalance signal (e.g. PI) is not expected to be used, the test at block <b>725</b> can omit the requirement that the sequence is able to produce this temporally unbalanced signal.</p><p>[0079] One of skill in the art will also recognize that the principles of this invention may be applied to any set of signals that are subject to motion-induced artifacts and use complementary sets of signals to cancel an underlying unwanted signal. That is, the flow diagram of FIG. 7, and the example extensions, may be applied if different sub-modes and/or different pulse types are found to cancel the underlying unwanted signal.</p><p>[0080] While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description are to be considered illustrative or exemplary and not restrictive; the invention is not limited to the disclosed embodiments.</p><p>[0081] For example, it is possible to operate the invention in an embodiment wherein the order of the sequence is reversed, the phases are reversed, and so on, provided that the resultant sequence remains temporally balanced. That is, for the purposes of understanding this invention as disclosed in the claims, the use of the terms ‘positive’ and ‘negative’, ‘odd’ and ‘even’, are not absolute, but are relative to each other.</p><p>[0082] FIG. 8 is a block diagram illustrating an example processor <b>800</b> according to embodiments of the disclosure. Processor <b>800</b> may be used to implement one or more processors described herein, for example, any or all of the processing elements shown in FIG. 6. Processor <b>800</b> may be any suitable processor type including, but not limited to, a microprocessor, a microcontroller, a digital signal processor (DSP), a field programmable array (FPGA) where the FPGA has been programmed to form a processor, a graphical processing unit (GPU), an application specific circuit (ASIC) where the ASIC has been designed to form a processor, or a combination thereof.</p><p>[0083] The processor <b>800</b> may include one or more cores <b>802</b>. The core <b>802</b> may include one or more arithmetic logic units (ALU) <b>804</b>. In some embodiments, the core <b>802</b> may include a floating point logic unit (FPLU) <b>806</b> and/or a digital signal processing unit (DSPU) <b>808</b> in addition to or instead of the ALU <b>804</b>.</p><p>[0084] The processor <b>800</b> may include one or more registers <b>812</b> communicatively coupled to the core <b>802</b>. The registers <b>812</b> may be implemented using dedicated logic gate circuits (e.g., flip-flops) and/or any memory technology. In some embodiments the registers <b>812</b> may be implemented using static memory. The register may provide data, instructions and addresses to the core <b>802</b>.</p><p>[0085] In some embodiments, processor <b>800</b> may include one or more levels of cache memory <b>810</b> communicatively coupled to the core <b>802</b>. The cache memory <b>810</b> may provide computer-readable instructions to the core <b>802</b> for execution. The cache memory <b>810</b> may provide data for processing by the core <b>802</b>. In some embodiments, the computer-readable instructions may have been provided to the cache memory <b>810</b> by a local memory, for example, local memory attached to the external bus <b>816</b>. The cache memory <b>810</b> may be implemented with any suitable cache memory type, for example, metal-oxide semiconductor (MOS) memory such as static random access memory (SRAM), dynamic random access memory (DRAM), and/or any other suitable memory technology.</p><p>[0086] The processor <b>800</b> may include a controller <b>814</b>, which may control input to the processor <b>800</b> from other processors and/or components included in a system (e.g., component BBB shown in FIG. B) and/or outputs from the processor <b>800</b> to other processors and/or components included in the system (e.g., component CCC shown in FIG. C). Controller <b>814</b> may control the data paths in the ALU <b>804</b>, FPLU <b>806</b> and/or DSPU <b>808</b>. Controller <b>814</b> may be implemented as one or more state machines, data paths and/or dedicated control logic. The gates of controller <b>814</b> may be implemented as standalone gates, FPGA, ASIC or any other suitable technology.</p><p>[0087] The registers <b>812</b> and the cache <b>810</b> may communicate with controller <b>814</b> and core <b>802</b> via internal connections <b>820</b>A, <b>820</b>B, <b>820</b>C and <b>820</b>D. Internal connections may be implemented as a bus, multiplexor, crossbar switch, and/or any other suitable connection technology.</p><p>[0088] Inputs and outputs for the processor <b>800</b> may be provided via a bus <b>816</b>, which may include one or more conductive lines. The bus <b>816</b> may be communicatively coupled to one or more components of processor <b>800</b>, for example the controller <b>814</b>, cache <b>810</b>, and/or register <b>812</b>. The bus <b>816</b> may be coupled to one or more components of the system, such as components BBB and CCC mentioned previously.</p><p>[0089] The bus <b>816</b> may be coupled to one or more external memories. The external memories may include Read Only Memory (ROM) <b>832</b>. ROM <b>832</b> may be a masked ROM, Electronically Programmable Read Only Memory (EPROM) or any other suitable technology. The external memory may include Random Access Memory (RAM) <b>833</b>. RAM <b>833</b> may be a static RAM, battery backed up static RAM, Dynamic RAM (DRAM) or any other suitable technology. The external memory may include Electrically Erasable Programmable Read Only Memory (EEPROM) <b>835</b>. The external memory may include Flash memory <b>834</b>. The External memory may include a magnetic storage device such as disc <b>836</b>. In some embodiments, the external memories may be included in a system, such as ultrasound imaging system <b>600</b> shown in FIG. 6.</p><p>[0090] Although the processor <b>800</b> is shown as distinct from the external bus <b>816</b> and memories <b>832</b>, <b>834</b>, <b>833</b>, <b>835</b> and disc <b>836</b> some or all of these items may be part of the \\\"processor\\\" <b>800</b> in alternative embodiments. It should be understood that the term processor, processor system, computer, computer system controller or controller system may refer to the processor <b>800</b> alone or to the processor along with some or all of the elements <b>816</b>, <b>832</b>, <b>834</b>, <b>833</b>, <b>835</b> and <b>836</b>.</p><p>[0091] Other variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing the claimed invention, from a study of the drawings, the disclosure, and the appended claims. In the claims, the word \\\"comprising\\\" does not exclude other elements or steps, and the indefinite article \\\"a\\\" or \\\"an\\\" does not exclude a plurality. A single processor or other unit may fulfill the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. Reference numerals and symbols that appear in the claim are for ease of understanding only, and represent example embodiments; they are not intended to limit the scope of the claims. A computer program may be stored/distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. An ultrasound method for distinguishing non-perfused tissue from blood flow and/or blood perfusion comprising: <BR />transmitting a sequence of ultrasound signals (P<b>1</b>(<i>o</i>), P<b>2</b>, P<b>3</b>(<i>e</i>), P<b>4</b>, P<b>5</b>(<i>o</i>)) to a patient via a plurality of ultrasound transducer elements;<BR />receiving a sequence of echo signals corresponding to the sequence of ultrasound images;<BR />combining a first selected set of echo signals to produce an Amplitude Modulated signal;<BR />combining a second selected set of echo signals to produce an Amplitude Modulated Phase Inversion signal;<BR />creating at least one image based on at least one of the AM signal and the AMPI signal;<BR />displaying the at least one image;<BR />wherein each of the first selected set of echo signals and the second selected set of echo signals is temporally balanced;<BR />wherein each of the echo signals is equally weighted in each combining to produce the AM and AMPI signals.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The method of claim 1, <BR />wherein the sequence of ultrasound signals comprises a sequence of a first, second, third, fourth, and fifth signals;<BR />wherein the first and fifth signals corresponds to a transmission through a first half of the plurality of ultrasound transducer elements at a first phase;<BR />wherein the second signal corresponds to a transmission through the plurality of transducer elements at the first phase;<BR />wherein the third signal corresponds to a transmission through a second half of the plurality of ultrasound transducer elements at the first phase,<BR />wherein the second half is a complement of the first half; and <BR />wherein the fourth signal corresponds to a transmission through the plurality of transducer elements at a second phase,<BR />wherein the second phase is a complement of the first phase.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>3</b>. The method of claim 2, <BR />wherein the first signal corresponds to the echo signal E<b>1</b>(<i>o</i>),<BR />wherein the second signal corresponds to the echo signal E<b>2</b>,<BR />wherein the third signal corresponds to the echo signal E<b>3</b>(<i>o</i>).</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>4</b>. The method of claim 2, wherein the AMPI signal comprises a sum of the third, fourth, and fifth signals.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>5</b>. The method of claim 2, wherein the method comprises summing the second and fourth signal to provide a Phase Inverted (PI) signal, and wherein the at least one image is also based on the PI signal.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>6</b>. The method of claim 1, wherein the PI signal is temporally balanced.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>7</b>. The method of claim 1, wherein the at least one image is further based on a spectral response of at least one of the AM and AMPI signals.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>8</b>. The method of claim 1, wherein the at least one image is further based on a signal-to-noise ratio of at least one of the AM and AMPI signals.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>9</b>. The method of claim 1, wherein the at least one image comprises at least two images, and wherein the method comprises simultaneously displaying the at least two images.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>10</b>. The method of claim 1, wherein the at least one image comprises a combination image that is based on both the AM and AMPI signals.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>11</b>. The method of claim 1, <BR />wherein each of the selected first set of echo signals and the selected second set of echo signals comprises a first set of signals and a second set of signals;<BR />wherein a form of the first set of signals and the second set of signals are complementary;<BR />wherein the first set of signals has a first temporal center;<BR />wherein the second set of echo signals has a second temporal center; and<BR />wherein the first temporal center is equal to the second temporal center.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>12</b>. The method of claim 1, further comprising injecting contrast-enhancing microbubbles in a vessel of the patient; wherein the at least one image displays a flow of the microbubbles through the patient.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>13</b>. An ultrasound system comprising: <BR />a plurality of transducer elements that transmit a sequence of ultrasound pulses (P<b>1</b>(<i>o</i>), P<b>2</b>, P<b>3</b>(<i>e</i>), P<b>4</b>, P<b>5</b>(<i>o</i>)) and receive a sequence of echo signals in response to the sequence of ultrasound pulses; and<BR />a processing circuit that generates the sequence of ultrasound pulses and processes the sequence of echo signal as claimed in claim 1.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"yes\\\"><p><b>14</b>. A non-transitory computer-readable medium that includes a program that, when executed by a processing system, causes the processing system to: <BR />transmit a sequence of ultrasound signals to a patient via a plurality of ultrasound transducer elements;<BR />receive a sequence of echo signals corresponding to the sequence of ultrasound images;<BR />combine a first selected set of echo signals to produce an Amplitude Modulated signal;<BR />combine a second selected set of echo signals to produce an Amplitude Modulated Phase Inversion (AMPI) signal;<BR />create at least one image based on at least one of the AM signal and the AMPI signal;<BR />display the at least one image;<BR />wherein each of the first and second selected sets of echo signals is temporally balanced;<BR />wherein each of the echo signals is equally weighted in each combining to produce the AM and AMPI signals; and<BR />wherein the at least one image minimizes an appearance of motion-induced tissue artifacts in the images.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>15</b>. The medium of claim 14, wherein: <BR />the sequence of ultrasound signals comprises a sequence of a first, second, third, fourth, and fifth signals;<BR />the first and fifth signals corresponds to a transmission through a first half of the plurality of ultrasound transducer elements at a first phase;<BR />the second signal corresponds to a transmission through the plurality of transducer elements at the first phase;<BR />the third signal corresponds to a transmission through a second half of the plurality of ultrasound transducer elements at the first phase, wherein the second half is a complement of the first half; and<BR />the fourth signal corresponds to a transmission through the plurality of transducer elements at a second phase, the second phase being a complement of the first phase.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B8/00\",\n",
      "\n",
      "\"A61B8/06\",\n",
      "\n",
      "\"A61B8/08\",\n",
      "\n",
      "\"A61B8/14\",\n",
      "\n",
      "\"G01S15/89\",\n",
      "\n",
      "\"G01S7/52\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B8/06\",\n",
      "\n",
      "\"A61B8/085\",\n",
      "\n",
      "\"A61B8/0891\",\n",
      "\n",
      "\"A61B8/14\",\n",
      "\n",
      "\"A61B8/463\",\n",
      "\n",
      "\"A61B8/481\",\n",
      "\n",
      "\"A61B8/5207\",\n",
      "\n",
      "\"A61B8/5276\",\n",
      "\n",
      "\"G01S15/8915\",\n",
      "\n",
      "\"G01S7/5202\",\n",
      "\n",
      "\"G01S7/52026\",\n",
      "\n",
      "\"G01S7/52039\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"PHILIPS KONINKLL NV\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190611,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20211208,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LOUPAS, THANASIS;SHEERAN, PAUL;TREMBLAY-DARVEAU, CHARLES;AND OTHERS;SIGNING DATES FROM 20200604 TO 20200618;REEL/FRAME:058331/0923, Owner: KONINKLIJKE PHILIPS N.V., NETHERLANDS, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LOUPAS, THANASIS;SHEERAN, PAUL;TREMBLAY-DARVEAU, CHARLES;AND OTHERS;SIGNING DATES FROM 20200604 TO 20200618;REEL/FRAME:058331/0923\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"KONINKLIJKE PHILIPS N.V., NETHERLANDS\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220504,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220504,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220714,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231010,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231010,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231010,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240119,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240119,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240214,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FRM2\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Final Rejection Counted, Not Yet Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240214,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: FINAL REJECTION COUNTED, NOT YET MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"FINAL REJECTION COUNTED, NOT YET MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240216,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FRM1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Final Rejection Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240216,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: FINAL REJECTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"FINAL REJECTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240424,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response after Final Action Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240426,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"ADV1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Advisory Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240426,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: ADVISORY ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ADVISORY ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240501,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240501,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240528,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240528,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240528,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240904,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240904,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240913,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240913,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240917,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"ZAAB\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"NOTICE OF ALLOWANCE MAILED\",\n",
      "\n",
      "\"details\": \"Text: ORIGINAL CODE: MN/=.\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ORIGINAL CODE: MN/=.\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241217,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPV1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Verified\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241217,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PAT1\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"Patented Case\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20210617360\",\n",
      "\n",
      "\"ad\": 20200604,\n",
      "\n",
      "\"pn\": \"US2022218312\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STCF\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT GRANT\",\n",
      "\n",
      "\"details\": \"Text: PATENTED CASE\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PATENTED CASE\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82505014\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20230501913\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"ULTRASOUND GUIDED FEMORAL ARTERY ACCESS SYSTEM (UFAAS) AND METHOD TO ENHANCE BRAIN PERFUSION DURING NEUROPROTECTIVE CPR\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A handheld ultrasound guiding system for the transcutaneous delivery of a needle into blood vessel that can be used to then deliver a guidewire, the battery powered light-weight device consisting of a phased array ultrasound generator, a transducer probe, a screen, computer processor that is mechanically coupled to a needle holder mounted at an angle to the ultrasound probe, the needle is mounted such that it can be advanced into a target blood vessel that is being visualized real-time by the ultrasound probe.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATION</p><p>[0001] This application claims benefit of U.S. Provisional Patent Application No. 63/382,231, entitled, \\\"ULTRASOUND GUIDED FEMORAL ARTERY ACCESS SYSTEM (UFAAS) AND METHOD TO ENHANCE BRAIN PERFUSION DURING NEUROPROTECTIVE CPR\\\", filed Nov. 3, 2022, the contents of which are hereby incorporated by reference in their entirety.</p></relapp><shortsum><p>BACKGROUND OF THE INVENTION</p><p>[0002] Ultrasound is widely used to identify vascular structures, including arteries and veins for various medical tests and/or procedures, and in particular, procedures such as Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA) in which a catheter is inserted into an artery. Conventionally an ultrasound probe containing one or more sensors may be positioned against a patient to produce an ultrasound image that may be viewed on a screen. There are a wide range of sizes and shapes for the ultrasound probes, which are generally hand-held, and the ultrasound machine, which is used to generate and generally display the ultrasound image. One such machine, the Butterfly (US201461981491P) is a pocket-size hand-held that can be used to easily image the femoral artery. When ultrasound-guided vascular access is used, a probe is often held by the operator in one hand and the needle is held in the other hand. Once the needle is inserted into the vessel, typically the operator may either let go of the needle while finding and then inserting a guide wire through the needle or the operator may let go of the ultrasound probe to stabilize the needle. Both approaches can work but can result in small movement of the needle which results in loss of access and the inability to thread wire and ultimately cannulate the vessel. Such approaches may be difficult or impractical when pulsations of the veins and/or arteries occur and/or when time is of the essence this approach can result in worse clinical outcomes. For example, ultrasound visualization to implant a REBOA catheter, an operator can see real-time images of both the femoral artery and immediately adjacent vein, but vibrations can result in movement of the small insertion needle that is used to initially cannulate the artery. This can reduce the success rate of cannulation and thus reduce the benefit of REBOA. Therefore, improvements in ultrasound techniques for transcutaneous vascular access are desired.</p><p>SUMMARY OF THE CLAIMS</p><p>[0003] A handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure may include a body. The system may include an ultrasound probe coupled with a first end of the body. The system may include a display device coupled with a second end of the body. The display device may be configured to present an image generated using the ultrasound probe.</p><p>[0004] In some embodiments, the system may include a needle-positioning member coupled with the body. The needle-positioning member may define a needle channel. The needle channel may be alignable with an imaging axis of the ultrasound probe. The needle-positioning member may be detachably coupled with the body. An angle of the needle-positioning member may be adjustable relative to the body. The needle-positioning member may be retractable into the body. The display device and the ultrasound probe may be battery powered. The display device may be detachably coupled with the body. A total length of the body may be between 4 inches and 15 inches.</p><p>[0005] Some embodiments of the present technology may encompass methods of performing cardiovascular resuscitation. The methods may include elevating an individual's head, shoulders, and heart relative to the individual's lower body while the individual's lower body remains generally aligned with a horizontal plane. The methods may include compressing the individual's chest while the individual's head, shoulders, and heart are elevated. The methods may include regulating an intrathoracic pressure of the individual while the individual's head, shoulders, and heart are elevated relative to the lower body. The methods may include performing REBOA on the individual.</p><p>[0006] In some embodiments, performing REBOA may include positioning an intra-aortic balloon in the individual's aorta using an ultrasound-guided cannulation device and inflating the intra-aortic balloon. The methods may include performing active compression decompression cardiopulmonary resuscitation on the individual while the individual is in a supine position in general alignment with the horizontal plane prior to elevating the individual's head, shoulders, and heart. The methods may include actively decompressing the individual's chest between each chest compression. Regulating the intrathoracic pressure may include interfacing an impedance threshold device with the individual's airway. Elevating the individual's head, shoulders, and heart may include elevating the individuals' head to a height of between 10 cm and 30 cm above the horizontal plane and elevating the individual's heart to a height of between 5 cm to 15 cm above the horizontal plane. REBOA may be performed on the individual while the individual's head, shoulders, and heart are elevated.</p><p>[0007] Some embodiments of the present technology may encompass methods for cannulating a vascular access structure. The methods may include maneuvering an ultrasound probe coupled with a body of a cannulation device over an individual's body. The methods may include locating a vascular access structure of the individual based on an image presented on a display device coupled with the body of the cannulation device. The image may be generated based on signals from the ultrasound probe. The methods may include inserting a medical implement into the vascular access structure.</p><p>[0008] In some embodiments, the medical implement may include a hollow needle. Inserting the medical implement into the vascular access structure may include aligning the hollow needle with the vascular access structure using a needle-positioning member of the body that holds the hollow needle into alignment with an imaging axis of the ultrasound probe. The methods may include advancing a guidewire through a hollow bore of the hollow needle and into the vascular access structure under ultrasound visualization. The methods may include retracting the hollow needle into a sheath prior to advancing the guidewire. The methods may include adjusting an orientation of the display device relative to the body.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0009] A further understanding of the nature and advantages of the disclosed technology may be realized by reference to the remaining portions of the specification and the drawings.</p><p>[0010] FIG. <b>1</b> illustrates a handheld cannulation device according to embodiments of the present technology.</p><p>[0011] FIG. <b>2</b> illustrates a handheld cannulation device according to embodiments of the present technology.</p><p>[0012] FIG. <b>3</b> illustrates operations of a method for performing Neuroprotective CPR in conjunction with REBOA according to embodiments of the present technology.</p><p>[0013] FIG. <b>4</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0014] FIG. <b>5</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0015] FIG. <b>6</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0016] FIG. <b>7</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0017] FIG. <b>8</b> illustrates a cumulative 24-hour survival rate during a pig study.</p></shortdesdrw><p>[0018] Several of the figures are included as schematics. It is to be understood that the figures are for illustrative purposes, and are not to be considered of scale or proportion unless specifically stated to be of scale or proportion. Additionally, as schematics, the figures are provided to aid comprehension and may not include all aspects or information compared to realistic representations, and may include exaggerated material for illustrative purposes.</p><p>[0019] In the appended figures, similar components and/or features may have the same reference label. Further, various components of the same type may be distinguished by following the reference label by a letter that distinguishes among the similar components. If only the first reference label is used in the specification, the description is applicable to any one of the similar components having the same first reference label irrespective of the letter.</p><p>DETAILED DESCRIPTION OF THE INVENTION</p><p>[0020] The subject matter of embodiments of the present invention is described here with specificity to meet statutory requirements, but this description is not necessarily intended to limit the scope of the claims. The claimed subject matter may be embodied in other ways, may include different elements or steps, and may be used in conjunction with other existing or future technologies. This description should not be interpreted as implying any particular order or arrangement among or between various steps or elements except when the order of individual steps or arrangement of elements is explicitly described.</p><p>[0021] Outcomes with conventional cardiopulmonary resuscitation (CPR) are generally dismal, especially in out-of-hospital cardiac arrest patients. For example, in 2021 and 2022 only 7.4 percent of out-of-hospital cardiac arrest patients in the USA survived with favorable brain function. This is often due to lack of timely treatment and/or treatment using techniques that do not generate sufficient levels of cerebral perfusion pressure to adequately deliver oxygenated blood to the brain. Embodiments of the present invention are directed to devices and methods to improve outcomes after cardiac arrest when CPR is needed. In particular, embodiments may enable new methods that markedly improve brain perfusion during CPR, such as by performing Neuroprotective CPR, (defined herein as the combination of active compression and decompression of the chest, an impedance threshold device, and controlled gradual elevation of the head and thorax), in combination with ultrasound-guided delivery and deployment of a Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA) catheter. This sequential method of resuscitation provides a novel means to achieve markedly higher brain blood flow during CPR. In particular, while Neuroprotective CPR results in near normal cerebral perfusion pressures, higher brain blood flow, and a higher likelihood of survival after cardiac arrest, additional hemodynamic support can be particularly of benefit, especially during prolonged resuscitation efforts that last more than 20 minutes. The REBOA occludes the aorta to limit arterial blood flow distal to the occlusion balloon, and restricts the effective circulating vascular volume to a region above the occlusion, thereby increasing compression and decompression phase arterial pressures proximal to the occlusion. This results in unexpectedly higher cerebral perfusion pressures and coronary perfusion when used in conjunction with Neuroprotective CPR.</p><p>[0022] Performing Neuroprotective CPR in conjunction with REBOA may require specialized equipment and devices, including an ultrasound-guided femoral artery cannulation device. During REBOA, the femoral artery is often used as a catheterization access point. For example, femoral artery catheterization is used during coronary angiography, for arterial blood pressure monitoring, and for blood sampling. Access to the femoral artery is generally performed in a ‘blinded’ manner, by feeling for a pulse and then inserting a needle into the femoral artery, at times the pulse may be absent, difficult to feel, or difficult to identify, especially on patients exhibiting low blood pressure, during cardiac arrest, and/or during the performance of CPR. Additionally, in the setting of a cardiac arrest, when it may be of benefit to insert a REBOA catheter, motion associated with delivery of CPR can result pulsations of both the femoral artery and adjacent vein that may make it difficult to successfully cannulate the femoral artery. The ability to access the femoral artery may be further compromised when treating patients in out-of-hospital environments, as conventional imaging devices to visualize the artery may not be available, even in ambulances and/or smaller medical facilities. Moreover, oftentimes, non-hospital staff (e.g., paramedics, clinicians, etc.) are not properly trained in how to access the femoral artery for delivery of the occlusion device. Such issues make it even more difficult to implement REBOA in out-of-hospital treatment settings.</p><p>[0023] To further improve outcomes, a means to selectively and rapidly insert an aortic balloon occlusion catheter may be utilized in conjunction with Neuroprotective CPR. For example, embodiments of the present technology may include femoral artery catheterization devices that use ultrasound imaging to visualize and locate the femoral artery. The devices may include a needle holder and insertion system to assist in advancing a needle into the femoral artery and stabilizing the needle thereafter with on-going real time direct ultrasound visualization. By integrating an ultrasound probe and needle holder into a single device, the femoral artery catheterization devices and cannulation methods described herein may provide a safer, quicker, and more effective way to cannulate the femoral artery and other arteries and veins during Neuroprotective CPR, or as part of any other medical procedure. Such devices may help properly cannulate the femoral artery, even in the presence of vibrations or other movements from the performance of CPR that may result in movement of the small insertion needle that is used to initially cannulate the artery, thereby improving the success rate of cannulation. While disclosed primarily in the context of REBOA cannulation during Neuroprotective CPR, it will be appreciated that the cannulation techniques disclosed herein are not so limited. The cannulation devices and techniques described herein may be used to cannulate the femoral artery, and other vascular access structures (e.g., brachial artery, radial artery, carotid artery, jugular vein, and femoral vein) for any number of medical procedures. As such, it will be appreciated that the catheterization devices that use ultrasound imaging, including the cannula devices and techniques described herein, are not intended to be limited solely for use with CPR, but may be used with various other procedures as well.</p><p>[0024] In some embodiments, the Neuroprotective CPR+REBOA may be performed using existing CPR devices and methods that combine active compression and decompression of the chest, an impedance threshold device, and a means and structure for controlled gradual elevation of the head and thorax. Such CPR devices and methods may include those described herein, as well as those described in U.S. application Ser. No. 17/668,266, U.S. application Ser. No. 17/335,922, U.S. application Ser. No. 17/222,476, U.S. application Ser. No. 16/955,382, U.S. application Ser. No. 16/201,339, U.S. Pat. Nos. 11,259,988, 11,246,794, 11,096,861, 11,020,314, 10,667,987, 10,406,069, 10,406,068, 10,350,137, 9,801,782, 9,750,661, 9,707,152, 10,092,481, and 10,245,209, the complete disclosures of which have previously been incorporated by reference for all intents and purposes.</p><p>[0025] Turning now to FIG. <b>1</b>, one embodiment of an ultrasound-guided cannulation device <b>100</b> is illustrated. The cannulation device <b>100</b> may be in the form of a handheld device that may enable an operator to use ultrasound to visually locate and subsequently cannulate (or otherwise access) a desired vascular access structure. The cannulation device <b>100</b> may include an elongate body <b>102</b> that may be sized and shaped to be graspable using a single hand. In some embodiments, the body <b>102</b> may have a substantially constant cross-section along a length of the body <b>102</b>, while in other embodiments the body <b>102</b> may have a variable cross-section along the length. For example, as illustrated, the body <b>102</b> tapers from a patient interface end <b>104</b> to a display end <b>106</b>. Other profiles (e.g., stepped, angled, curved, etc.) may be used in various embodiments.</p><p>[0026] The body <b>102</b> may include all necessary components to generate and view an ultrasound image. For example, the body <b>102</b> may house a battery or other power source, one or more processors, circuits, ultrasonic transducers, display screens, etc. In other words, the entire ultrasound device including the screen, probe, power supply, and hardware and software are a single unit. This may enable an operator to view an ultrasound image and the cannulation device <b>100</b> simultaneously, which may help facilitate improved cannulation of a vascular access structure as will be described below. The patient interface end <b>104</b> may be or include a probe tip that includes one or more ultrasonic transducers that may transmit high frequency sound waves (e.g., &gt;20 kHz) to the body and record reflected or echo waves. An image processor disposed within the body <b>102</b> may use the recorded waves to develop 2-D real-time images. The image processor may also be disposed in other parts of the device. These images may be displayed, in real-time, using a display screen <b>108</b> that is integrated into the display end <b>106</b> of the body <b>102</b>. For example, in a particular embodiment, the display screen <b>108</b> may be positioned on an opposite end surface as the probe and/or ultrasonic transducer. In some embodiments, the display screen <b>108</b> may include a touchscreen and/or may be electrically coupled with one or more buttons and/or other user interface inputs that enable an operator to control operation of the cannulation device <b>100</b>. A smallest lateral dimension (e.g., length or width) of the display screen <b>108</b> may range from between about 0.5 inches and 3 inches in some embodiments, although larger display screens <b>100</b> are possible in various embodiments. In some embodiments images may be capable of being recorded, stored, and/or transmitted to a remote receiver.</p><p>[0027] The cannulation device <b>100</b> may further include a means to stabilize a hollow bore needle and/or syringe with a needle to cannulate a vascular access structure. Such a design of the cannulation device <b>100</b> may enable the ultrasound imager and needle to be held in one hand. For example, as illustrated in FIG. <b>1</b>, a needle-positioning member <b>110</b> is coupled with the body <b>102</b>, such as at a medial portion of the body <b>102</b> between the patient interface end <b>104</b> and the display end <b>106</b>. The needle-positioning member <b>110</b> may be permanently or removably coupled with the body <b>102</b> in various embodiments. For example, a fastener and/or quick connect/disconnect mechanism (such as a clip) may be used to couple the needle-positioning member <b>110</b> with the body <b>102</b>, which may enable the needle-positioning member <b>110</b> to be removed and replaced (e.g., may be disposable), while the body <b>102</b> (and ultrasound components) may be reused. In some embodiments, the needle-positioning member <b>110</b> may extend laterally outward from the body <b>102</b> at an angle, such as less than or about 90 degrees relative to a longitudinal axis of the body <b>102</b>, with a distal end of the needle-positioning member <b>110</b> pointing substantially away from the display end <b>106</b>. The distal end of the needle-positioning member <b>110</b> may include a needle holder <b>112</b> that may securely receive a cannulation needle <b>114</b> during the insertion process. For example, a tip of the cannulation needle <b>114</b> may be angled toward a point under the probe tip in some embodiments, such that when the probe tip is aligned with a target vascular access structure, the tip of the cannulation needle <b>114</b> is similarly aligned with the target vascular access structure. In some embodiments, the cannulation needle <b>114</b> may be stored in a retracted position and advanced from the protective sheath only when the cannulation needle <b>114</b> is being deployed.</p><p>[0028] In some embodiments, the needle-positioning member <b>110</b> may include an adjustment mechanism <b>116</b>, such as (but not limited to) a ball joint, that may enable an angle of the needle holder <b>112</b> (and associated cannulation needle <b>114</b>) to be adjusted relative to the body <b>102</b> and upper portion of the needle-positioning member <b>110</b>. This may enable the angle of the cannulation needle <b>114</b> to be adjusted such that the cannulation needle <b>114</b> can be inserted over a range of insertion angles varying from 20-80 degrees relative to the surface of the skin and the body <b>102</b>. The adjustment mechanism <b>116</b> may be positioned at any point of the needle-positioning member <b>110</b>. For example, the adjustment mechanism <b>116</b> may be at the interface with the body <b>102</b>, at the interface with the needle holder <b>112</b>, and/or at a medial position along a length of the needle-positioning member <b>110</b>.</p><p>[0029] In some embodiments, the cannulation needle <b>114</b> may be advanced using an electrical worm screw motor or equivalent to control for small forward and backward movements. In other embodiments, the cannulation needle <b>114</b> may be advanced manually through a conical or cylindrical locking system wherein a slight twist of the needle holder <b>112</b> will loosen or tighten the grip of the cannulation needle <b>114</b>. Further, the distance of the tip of the cannulation needle <b>114</b> from a middle of the contact point of the ultrasound probe with the skin can vary in some embodiments from between 0.5 cm and 4 cm to facilitate stable and proper insertion of the cannulation needle <b>114</b> into the target vascular access structure. Both the angle of the cannulation needle <b>114</b> and distance from the tip of the cannulation needle <b>114</b> to the contact point of the ultrasound probe may be fixed and/or may be varied by the operator in various embodiments.</p><p>[0030] Once the vessels are imaged, for example with the cannulation device <b>100</b> being held by one of the operator's hands, the cannulation needle <b>114</b> may be advanced through the skin by the operator's other free hand and/or by a motorized delivery system. Once in the femoral artery (in this example) or other vascular access structure, a thin guidewire may be inserted through the hollow bore needle into the artery under visualization of the images displayed on display screen <b>106</b>. At this point the entire cannulation device <b>100</b> may be lifted upward, leaving the guidewire inserted in the artery. In some embodiments, the needle-positioning member <b>110</b> and/or needle holder <b>112</b> may be disconnected from the body <b>102</b>. The guidewire is then used to maintain vascular access and a vascular access sheath and dilator can be inserted into the blood vessel over the wire.</p><p>[0031] The novel approach to vascular access and device to facilitate vascular access and novel approach coupling image-guided transcutaneous arterial access for rapid placement of a REBOA catheter combined with Neuroprotective CPR provides a novel means to rapidly improve blood pressures, especially cerebral and coronary blood flow, during CPR.</p><p>[0032] FIG. <b>2</b> illustrates another embodiment of an ultrasound-guided cannulation device <b>200</b>. The cannulation device <b>200</b> may be similar to cannulation device <b>100</b> and may include any feature described in relation to cannulation device <b>100</b>. The cannulation device <b>200</b> may be a handheld device that may enable an operator to use ultrasound to visually locate and subsequently cannulate (or otherwise access) a desired vascular access structure. The cannulation device <b>200</b> may include an elongate body <b>202</b> that may be sized and shaped to be graspable using a single hand. The body <b>202</b> may include all necessary components to generate and view an ultrasound image. For example, the body <b>202</b> may house a battery or other power source, one or more processors, circuits, ultrasonic transducers, display screens, etc. In some embodiments, a total length of the body <b>202</b> may be between 4 inches and 15 inches, between 6 inches and 12 inches, or between 8 inches and 10 inches, although other lengths are possible. A total weight of the cannulation device <b>200</b> may be less than 5 pounds, less than 4 pounds, less than 3 pounds less than 2 pounds, or less.</p><p>[0033] A patient interface end <b>204</b> may be or include a probe tip that includes one or more ultrasonic transducers that may transmit high frequency sound waves (e.g., &gt;20 kHz) to the body and record reflected or echo waves. An image processor disposed within the body <b>202</b> may use the recorded waves to develop 2-D real-time images. The image processor may also be disposed in other parts of the device. These images may be displayed, in real-time, using a display device <b>208</b> that is positioned on a display end <b>206</b> of the body <b>202</b>. For example, in a particular embodiment, the display device <b>208</b> may be positioned on an opposite end surface as the patient interface end <b>204</b>. The display device <b>208</b> may be integrated directed into the cannulation device <b>200</b> in some embodiments. In other embodiments, the display device <b>208</b> may be a detachable display (e.g., a portable monitor, tablet computer, mobile phone, etc.) that may be secured to the display end <b>206</b> using one or more clamps <b>218</b> and/or other connectors. The display screen may have any dimensions that permits the cannulation device <b>200</b> to be easily operable using a single hand. The display device <b>208</b> may be electrically coupled with the cannulation device <b>200</b> (e.g., to receive image data for display) via one or more wires or other electrical connectors. In some embodiments, the display device <b>208</b> may be wirelessly coupled to the cannulation device <b>200</b> (e.g., to receive image data for display), such as using Bluetooth, Wi-Fi, 5G, LTE, and/or other wireless communication protocol. The display <b>208</b> may be powered by a same or different power source as the rest of the cannulation device <b>200</b>.</p><p>[0034] In some embodiments, the display device <b>208</b> may be adjustably coupled with the body <b>202</b>. For example, the clamp <b>218</b> and/or other mount for the display device <b>208</b> may enable the display device <b>208</b> to be tilted and/or rotated relative to the body <b>202</b>, which may enable the viewing angle of the display device <b>208</b> to be adjusted by a user.</p><p>[0035] The cannulation device <b>200</b> may further include a means to stabilize a hollow bore needle and/or syringe with a needle to cannulate a vascular access structure. Such a design of the cannulation device <b>200</b> may enable the ultrasound imager and needle to be held in one hand. For example, as illustrated in FIG. <b>2</b>, a needle-positioning member <b>210</b> is coupled with the body <b>202</b>, such as at a lower portion of the body <b>202</b> proximate the patient interface end <b>204</b>. The needle-positioning member <b>210</b> may include a needle holder <b>212</b> that may securely receive a cannulation needle during the insertion process. The needle holder <b>212</b> may be a cylindrical channel defined through a length of the needle-positioning member <b>210</b> in some embodiments. For example, the channel may be angled toward a point under the probe tip in some embodiments, such that when the probe tip is aligned with a target vascular access structure, the tip of a cannulation needle inserted within the channel is similarly aligned with the target vascular access structure. The needle-positioning member <b>210</b> may be adjustable in some embodiments. For example, the needle-positioning member <b>210</b> may be pivoted relative to the body <b>202</b> to adjust an angle of the channel/needle holder <b>212</b> to adjust a needle insertion angle to an angle of between 20 degrees and 80 degrees in one or two axes relative to a horizontal plane. Such adjustability may enable the cannulation device <b>200</b> to be used to deliver an intra-aortic balloon to the femoral artery at different angles. Such adjustability may also enable the cannulation device <b>200</b> to be usable with other vascular structures and/or for different cannulation procedures. In some embodiments, a retractable and/or detachable needle-positioning member <b>210</b> may be used, which may enable the needle to be stabilized using the cannulation device <b>200</b> and allowing the rescuer to position the needle, advance the needle into the femoral artery under direct visualization. The needle-positioning member <b>210</b> may then enable a user to remove their hand from the needle to free the hand up for threading a flexible guidewire through the bore of the needle.</p><p>[0036] The angle and/or other position of the needle-positioning member <b>210</b> may be adjusted manually and/or automatically. For example, in some embodiments the angle of the needle holder may be automatically adjusted based upon the determination by the imaging system of the depth of the artery under study. In some embodiments, the needle may be automatically advanced under imaging by the imaging system and motorized needle insertion motor. In some embodiments, the distance from the needle holder to the imaging probe can be adjusted to facilitate both viewing of the needle as it is inserted, regardless of whether the operator views the artery in a cross-section, transverse-section and/or other imaging section.</p><p>[0037] In operation, a user may hold the body <b>202</b> with one hand while the other hand is used to cannulate the femoral artery using a syringe attached to a needle. The user may maneuver the probe over a patient's leg and view the display device <b>208</b> to locate the femoral artery (or other vascular access structure). The needle may be inserted through the needle holder <b>212</b> (which may occur prior to locating the vascular access structure) and inserted into the vascular access structure. The syringe may be removed and a guidewire may be advanced through a hollow bore of the needle and into the artery under ultrasound visualization. A sheath may be advanced over the guidewire and then the needle may be retracted into a sheath to protect the user from an inadvertent needle stick. In some embodiments, the needle-positioning member <b>210</b> may be detached from the body <b>202</b>. A REBOA catheter may be placed into the femoral artery and advanced blindly to the level of the diaphragm. The cannulation device <b>200</b> may be easily rotated (and the display device <b>208</b> adjusted) to visualize the guidewire as it is advanced into the vessel. Such processes may enable intra-aortic balloons to be more easily and accurately positioned for REBOA procedures, even when used by out-of-hospital personnel.</p><p>[0038] FIG. <b>3</b> depicts a process <b>300</b> for performing Neuroprotective CPR in conjunction with REBOA. Process <b>300</b> may be performed using a cannulation device such as those described herein. Process <b>300</b> may include performing active compression decompression cardiopulmonary resuscitation (ACD-CPR) on an individual while the individual is in a supine position in general alignment with a horizontal plane at operation <b>302</b>. At operation <b>304</b>, the individual's head, shoulders, and heart may be elevated relative to the individual's lower body while the individual's lower body remains generally aligned with the horizontal plane. The head may be elevated to a height of between about 10 cm and 30 cm above the horizontal plane and the heart may be elevated to a height of between about 5 cm to 15 cm or 8 cm and 12 cm above the horizontal plane. The individual's chest may be compressed and/or actively decompressed while the individual's head, shoulders, and heart are elevated at operation <b>306</b>. In some embodiments, the elevation of the individual and/or the chest compressions/decompressions may be performed using a head up CPR elevation device, such as those described in U.S. application Ser. No. 17/668,266, U.S. application Ser. No. 17/335,922, U.S. application Ser. No. 17/222,476, U.S. application Ser. No. 16/955,382, U.S. application Ser. No. 16/201,339, U.S. Pat. Nos. 11,259,988, 11,246,794, 11,096,861, 11,020,314, 10,667,987, 10,406,069, 10,406,068, 10,350,137, 9,801,782, 9,750,661, 9,707,152, 10,092,481, and 10,245,209, the complete disclosures of which have previously been incorporated by reference for all intents and purposes.</p><p>[0039] At operation <b>308</b>, the intrathoracic pressure of the individual may be regulated using an impedance threshold device interfaced with the individual's airway and/or other intrathoracic pressure regulation device both while the individual is in the supine position and while the individual's head, shoulders, and heart are elevated relative to the lower body. For example, the intrathoracic pressure regulation may be performed before commencement of chest compressions, during chest compressions, and/or after chest compressions. At operation <b>310</b>, REBOA may be performed. For example, an intra-aortic balloon may be positioned within the patient's aorta to occlude blood flow from the aorta during the performance of CPR. The intra-aortic balloon may be positioned before commencement of chest compressions, during chest compressions, and/or after chest compressions.</p><p>[0040] Placement of the intra-aortic balloon may be performed using a cannulation device, such as cannulation device <b>100</b>/<b>200</b>. For example, an operator may position the patient interface end <b>104</b>/<b>204</b> of the body <b>102</b>/<b>202</b> against the patient. The operator may use the images presented on display screen/device <b>108</b>/<b>208</b> to position the cannulation needle <b>114</b> in the femoral artery and may subsequently advance the cannulation needle <b>114</b> through the skin. A guidewire may then be inserted through the hollow bore needle into the artery under visualization of the images displayed on display screen/device <b>108</b>/<b>208</b>. This guidewire may be used to advance the intra-aortic balloon to the aorta for subsequent inflation and occlusion of the aortic blood flow.</p><p>[0041] In some embodiments after there is a return of circulation to the individual, the REBOA balloon may remain inflated for a period of time (e.g., up to 5 minutes, up to 10 minutes, up to 15 minutes, up to 20 minutes, up to 25 minutes, up to 30 minutes, up to 45 minutes, or up to one hour in duration), depending upon the individual's physiologic status, for example, the individual's blood pressure. In some embodiments, the balloon deflation will occur based upon one or more physiologic measurements, such as a blood pressure of the individual. The physiologic measurements may be determined using one or more physiological sensors, such as (but not limited to) blood pressure sensors that are interfaced with the individual. The balloon may be deflated slowly or rapidly based upon the physiologic measurements. A closed loop or open loop feedback system may be used in some embodiments to control the timing and/or rate of balloon deflation after the return of spontaneous circulation.</p><p>[0042] In some embodiments, the deflation of the REBOA balloon may be coordinated with a change in elevation of the individual's head and thorax in order to maximize blood flow to the brain. In this case, blood flow to the brain may be monitored invasively and/or noninvasively. For example, the blood flow may be monitored based on a measurement of end tidal CO<sub>2 </sub>or/and cerebral oximetry. In this manner, the inflated balloon may be deflated safely, and in a partial manner if needed, in order to better perfuse the heart, the brain, and the organ systems distal to the balloon occlusion. Once the balloon is totally deflated, the balloon may be removed.</p><XXEMP><p>EXAMPLES</p><p>[0043] In one example, Neuroprotective CPR was performed for 30 minutes in a pig. After 30 minutes a REBOA catheter was inserted into the femoral artery and the aorta was occluded at the level of the diaphragm. Cerebral perfusion pressures increased by over 20 mmHg. When the head and thorax of the pig were lowered so they were horizontal instead of elevated, this effect was markedly decreased. As such, the combination of REBOA and Neuroprotective CPR provide a novel and unexpectedly superior way to perfusion the brain with blood during CPR. This physiology is shown in a series of hemodynamic tracing from a representative pig study.</p><p>[0044] Arterial, venous, and intracranial pressures were measured with microelectronic tipped Millar catheters during Neuroprotective CPR, as previously described (Moore J C, Salverda B, Rojas-Salvador C, Lick M, Debaty G, Lurie K. Controlled sequential elevation of the head and thorax combined with active compression decompression cardiopulmonary resuscitation and an impedance threshold device improves neurological survival in a porcine model of cardiac arrest. Resuscitation. 2020a; 150-23-28 DOI: https://doi.org/10.1016/j.resuscitation.2020.09.030). These tracings are illustrated in FIG. <b>4</b>.</p><p>[0045] At the start of the tracings in FIG. <b>4</b>, the pig had been receiving Neuroprotective CPR for 30 minutes. Before inflating the REBOA the arterial blood pressure was about 70/38 mmHg and the cerebral perfusion pressure was about 90/45 mmHg. After the REBOA inflation the arterial pressure increased to ˜125/50 mmHg and the cerebral perfusion pressure increased to 100/45 mmHg. Upon lowering the head and thorax, arterial and cerebral perfusion pressures decreased abruptly and with deflation of the REBOA the arterial and cerebral perfusion pressures decreased to 50/20 mmHg and 40/0 mmHg, respectively. These tracings demonstrate the synergistic and surprise finding that REBOA+Neuroprotective CPR can uniquely provide nearly normal arterial and cerebral perfusion pressures after prolonged Neuroprotective CPR, in the absence of any pharmacological intervention.</p><p>[0046] The Y axis on the right provides the unit of measurement of the physiological parameters. The Y axis on the left describes the parameter measured. The tracing <b>402</b> shows the aortic pressure, the tracing <b>404</b> shows the right atrial pressure, the tracing <b>406</b> shows the intracranial pressure, the tracing <b>408</b> shows the pressure in the distal aorta, and the tracing <b>410</b> shows the cerebral perfusion pressure. The x axis is time, in minutes.</p><p>[0047] These findings are shown again in FIG. <b>5</b>, from another similar pig study, where the impact of inflating the REBOA after 30 minutes of ongoing Neuroprotective CPR is shown in a higher resolution. The tracing <b>502</b> is the airway tracing. A positive pressure breath is delivered approximately 10 times per minute. The rise in aortic and cerebral pressures with REBOA during Neuroprotective CPR is profound. In FIG. <b>5</b>, the rise in coronary (CorPP) (tracing <b>504</b>) and cerebral (CerPP) (tracing <b>506</b>) pressures with inflation of the REBOA during Neuroprotective CPR is clearly seen.</p><p>[0048] In another study, female and male swine (˜40 kg) were sedated, intubated, and anesthetized. Bilateral femoral arterial access was obtained for continuous arterial pressure measurement and REBOA placement in the proximal thoracic aorta. Central venous and intracranial access was obtained for continuous measurement during CPR. Hemodynamics were measured with solid state pressure catheters. The arterial pressure catheter was placed in the thoracic aorta and confirmed via fluoroscopy. After 36 minutes of AHUP-CPR, animals underwent two-minute epochs of AHUP-CPR, AHUP+REBOA CPR, ACD+ITD CPR flat+REBOA, and ACD+ITD CPR flat. The reverse sequence was then performed. Pressures are reported as mean (mmHg) plus or minus standard deviation. A paired t-test was performed for the outcomes of Cerebral Perfusion Pressure (CerPP) and Coronary Perfusion Pressure (CorPP).</p><p>[0049] As illustrated in FIG. <b>6</b>, the CerPP was 42.8 plus or minus 12.2 with AHUP+REBOA CerPP versus a CerPP with ACD+ITD CPR flat+REBOA of 25.9 plus or minus 11.5 (p&lt;0.0001). The CorPP was 20.2 plus or minus 19.9 with AHUP+REBOA CorPP versus ACD+ITD CPR flat+REBOA CorPP of 15.7 plus or minus 18.8 (p&lt;0.01). The AHUP+REBOA CerPP was 63 percent of the baseline CerPP value after 38 minutes of CPR. The return of Spontaneous Circulation (ROSC) was obtained in 4/7 (57 percent) after 50+ minutes of CPR.</p><p>[0050] In another study, REBOA was added to the treatment of pigs (n=7) after 8 min of untreated VF and &gt;40 min of AHUP-CPR, there was an immediate, profound, and unanticipated rise in coronary and most of all cerebral perfusion pressure, without a significant rise in ICP as shown in FIG. <b>7</b>. All key hemodynamic parameters, especially cerebral perfusion pressure, increased immediately upon initiation of REBOA. By contrast, the benefits of REBOA were significantly less when the pigs were placed in the flat position. Statistically significant differences were observed only when REBOA was added to AHUP CPR (ACD+ITD in elevated position) and not when REBOA was added to ACD+ITD in the flat position.</p><p>[0051] In another study, a porcine model of untreated VF (15 min) was used to simulate refractory out-of-hospital cardiac arrest (r-OHCA). <b>22</b> pigs were randomized to either 25 min of AHUP-CPR (early AHUP) or 10 min of C-CPR followed by 15 min of AHUP-CPR (late AHUP) and then epinephrine, amiodarone, and defibrillation. Pigs in the early AHUP-CPR group had a 50 percent 24-hour survival rate and all survivors had normal pupillary reflexes. By contrast, with late AHUP-CPR only 1/10 pigs survived, and this animal remained comatose. Neurologic deficit scores were significantly better with early AHUP-CPR.</p><p>[0052] For the primary endpoint, 24-hour neurologic function, a veterinarian blinded to the CPR intervention assessed pigs using a Neurological Deficit Score (NDS, 0=normal and 260=worst deficit score or death). The cumulative 24-hour survival was 45.5 percent for early AHUP-CPR versus 9.1 percent for late AHUP-CPR (p&lt;0.02) as shown in FIG. <b>8</b>. The mean plus or minus SD NDS was 203 plus or minus 80 with early AHUP-CPR versus 259 plus or minus 3 with late AHUP-CPR group (p=0.04). Mean plus or minus SD ETCO2 (mmHg) values, known to correlate with circulation during CPR, were significantly higher with early versus late AHUP-CPR after 10 min of CPR (45 plus or minus 4 vs 27 plus or minus 6, p&lt;0.001) and 24 min of CPR (42 plus or minus 9 vs 31 plus or minus 12, p&lt;0.001). These findings are strongly supportive that the more rapid the restoration of circulation to the heart and brain after a prolonged arrest, the better the outcome.</p><p>[0053] These studies demonstrate that the combination of Neuroprotective CPR and REBOA may markedly improve brain perfusion during CPR by providing near normal cerebral perfusion pressures, higher brain blood flow, a higher likelihood of survival after cardiac arrest, higher compression and decompression phase arterial pressures proximal to the occlusion, and unexpectedly higher cerebral perfusion pressures. In particular, the examples have demonstrated increases of coronary and cerebral perfusion pressure of nearly 50 percent, which increases the effectiveness of head up CPR. In these studies, the femoral artery access was obtained with ultrasound guidance. In addition, these studies show the need for further interventions when pigs and patients cannot be resuscitated with just AHUP-CPR. There remains a great need for newer CPR techniques, methods and devices to further improve outcomes beyond which are shown in FIGS. <b>7</b> and <b>8</b>.</p><p>[0054] The methods, systems, and devices discussed above are examples. Various configurations may omit, substitute, or add various method steps or procedures, or system components as appropriate. For instance, in alternative configurations, the methods may be performed in an order different from that described, and/or various stages may be added, omitted, and/or combined. Also, features described with respect to certain configurations may be combined in various other configurations. Different aspects and elements of the configurations may be combined in a similar manner. Also, technology evolves and, thus, many of the elements are examples and do not limit the scope of the disclosure or claims.</p><p>[0055] Specific details are given in the description to provide a thorough understanding of example configurations (including implementations). However, configurations may be practiced without these specific details. For example, well-known circuits, processes, algorithms, structures, and techniques have been shown without unnecessary detail in order to avoid obscuring the configurations. This description provides example configurations only, and does not limit the scope, applicability, or configurations of the claims. Rather, the preceding description of the configurations will provide those skilled in the art with an enabling description for implementing described techniques. Various changes may be made in the function and arrangement of elements without departing from the spirit or scope of the disclosure.</p><p>[0056] Also, configurations may be described as a process which is depicted as a flow diagram or block diagram. Although each may describe the operations as a sequential process, many of the operations may be performed in parallel or concurrently. In addition, the order of the operations may be rearranged. A process may have additional steps not included in the figure. Furthermore, examples of the methods may be implemented by hardware, software, firmware, middleware, microcode, hardware description languages, or any combination thereof. When implemented in software, firmware, middleware, or microcode, the program code or code segments to perform the necessary tasks may be stored in a non-transitory computer-readable medium such as a storage medium. Processors may perform the described tasks.</p><p>[0057] Furthermore, the examples described herein may be implemented as logical operations in a computing device in a networked computing system environment. The logical operations may be implemented as: (i) a sequence of computer implemented instructions, steps, or program modules running on a computing device; and (ii) interconnected logic or hardware modules running within a computing device.</p><p>[0058] Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.</p><p>[0059] As used herein and in the appended claims, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" include plural references unless the context clearly dictates otherwise. Thus, for example, reference to \\\"a heater\\\" includes a plurality of such heaters, and reference to \\\"the aperture\\\" includes reference to one or more apertures and equivalents thereof known to those skilled in the art, and so forth.</p><p>[0060] Also, the words \\\"comprise(s)\\\", \\\"comprising\\\", \\\"contain(s)\\\", \\\"containing\\\", \\\"include(s)\\\", and \\\"including\\\", when used in this specification and in the following claims, are intended to specify the presence of stated features, integers, components, or operations, but they do not preclude the presence or addition of one or more other features, integers, components, operations, acts, or groups.</p></XXEMP>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A method of performing cardiovascular resuscitation, comprising: <BR />elevating an individual&apos;s head, shoulders, and heart relative to the individual&apos;s lower body while the individual&apos;s lower body remains generally aligned with a horizontal plane;<BR />compressing the individual&apos;s chest while the individual&apos;s head, shoulders, and heart are elevated;<BR />regulating an intrathoracic pressure of the individual while the individual&apos;s head, shoulders, and heart are elevated relative to the lower body; and<BR />performing REBOA on the individual by positioning an intra-aortic balloon in the individual&apos;s aorta using an ultrasound-guided cannulation device and inflating the intra-aortic balloon.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />performing active compression decompression cardiopulmonary resuscitation on the individual while the individual is in a supine position in general alignment with the horizontal plane prior to elevating the individual&apos;s head, shoulders, and heart.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>3. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />actively decompressing the individual&apos;s chest between each chest compression.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>4. The method of performing cardiovascular resuscitation of claim 1, wherein: <BR />regulating the intrathoracic pressure comprises interfacing an impedance threshold device with the individual&apos;s airway.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>5. The method of performing cardiovascular resuscitation of claim 1, wherein: <BR />elevating the individual&apos;s head, shoulders, and heart comprises elevating the individuals&apos; head to a height of between 10 cm and 30 cm above the horizontal plane and elevating the individual&apos;s heart to a height of between 5 cm to 15 cm above the horizontal plane.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>6. The method of performing cardiovascular resuscitation of claim 1, wherein: <BR />REBOA is performed on the individual while the individual&apos;s head, shoulders, and heart are elevated.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>7. The method of performing cardiovascular resuscitation of claim 1, wherein: <BR />positioning the intra-aortic balloon in the individual&apos;s aorta using the ultrasound-guided cannulation device comprises: <BR />locating a femoral artery of the individual using a display device of the ultrasound-guided cannulation device;<BR />inserting a needle into the femoral artery;<BR />advancing a guidewire into the femoral artery via a hollow bore of the needle; and<BR />advancing the intra-aortic balloon to the aorta using the guidewire.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />keeping the intra-aortic balloon in an inflated state for a predetermined period of time after a return of circulation to the individual.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p>9. The method of performing cardiovascular resuscitation of claim 8, wherein: <BR />the predetermined period of time is up to one hour.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>10. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />deflating the intra-aortic balloon after a return of circulation to the individual upon determining that a physiological measurement of the individual has reached a particular value.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"10\\\"><p>11. The method of performing cardiovascular resuscitation of claim 10, wherein: <BR />the physiological measurement comprises a blood pressure of the individual.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>12. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />deflating the intra-aortic balloon after a return of circulation to the individual, wherein one or both of a timing and a rate of deflation is controlled using a closed loop feedback system with one or more physiologic measurements.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>13. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />deflating the intra-aortic balloon after a return of circulation to the individual, wherein one or both of a timing and a rate of deflation is controlled using an open loop feedback system with one or more physiologic measurements.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>14. The method of performing cardiovascular resuscitation of claim 1, further comprising: <BR />deflating the intra-aortic balloon after a return of circulation to the individual, wherein deflation of the intra-aortic balloon is coordinated with a change in elevation of the individual&apos;s head, shoulders, and heart.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>15. The method of performing cardiovascular resuscitation of claim 14, wherein: <BR />coordination of the deflation of the intra-aortic balloon is performed by monitoring blood flow to the individual&apos;s brain.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"15\\\"><p>16. The method of performing cardiovascular resuscitation of claim 15, wherein: <BR />monitoring blood flow to the individual&apos;s brain is performed using one or both of a measurement of end tidal CO<sub>2 </sub>and a measurement of cerebral oximetry.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B90/00\",\n",
      "\n",
      "\"A61H31/00\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2090/378\",\n",
      "\n",
      "\"A61B90/37\",\n",
      "\n",
      "\"A61H31/007\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"RESUSCITATION INNOVATIONS LLC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20221103,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US12213845\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20250204,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US12213845\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20431103,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=102542833\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20230501913\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"ULTRASOUND GUIDED FEMORAL ARTERY ACCESS SYSTEM (UFAAS) AND METHOD TO ENHANCE BRAIN PERFUSION DURING NEUROPROTECTIVE CPR\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A handheld ultrasound guiding system for the transcutaneous delivery of a needle into blood vessel that can be used to then deliver a guidewire, the battery powered light-weight device consisting of a phased array ultrasound generator, a transducer probe, a screen, computer processor that is mechanically coupled to a needle holder mounted at an angle to the ultrasound probe, the needle is mounted such that it can be advanced into a target blood vessel that is being visualized real-time by the ultrasound probe.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATION</p><p>[0001] This application claims benefit of U.S. Provisional Patent Application No. 63/382,231, entitled, \\\"ULTRASOUND GUIDED FEMORAL ARTERY ACCESS SYSTEM (UFAAS) AND METHOD TO ENHANCE BRAIN PERFUSION DURING NEUROPROTECTIVE CPR\\\", filed Nov. 3, 2022, the contents of which are hereby incorporated by reference in their entirety.</p></relapp><shortsum><p>BACKGROUND OF THE INVENTION</p><p>[0002] Ultrasound is widely used to identify vascular structures, including arteries and veins for various medical tests and/or procedures, and in particular, procedures such as Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA) in which a catheter is inserted into an artery. Conventionally an ultrasound probe containing one or more sensors may be positioned against a patient to produce an ultrasound image that may be viewed on a screen. There are a wide range of sizes and shapes for the ultrasound probes, which are generally hand-held, and the ultrasound machine, which is used to generate and generally display the ultrasound image. One such machine, the Butterfly (US201461981491P) is a pocket-size hand-held that can be used to easily image the femoral artery. When ultrasound-guided vascular access is used, a probe is often held by the operator in one hand and the needle is held in the other hand. Once the needle is inserted into the vessel, typically the operator may either let go of the needle while finding and then inserting a guide wire through the needle or the operator may let go of the ultrasound probe to stabilize the needle. Both approaches can work but can result in small movement of the needle which results in loss of access and the inability to thread wire and ultimately cannulate the vessel. Such approaches may be difficult or impractical when pulsations of the veins and/or arteries occur and/or when time is of the essence this approach can result in worse clinical outcomes. For example, ultrasound visualization to implant a REBOA catheter, an operator can see real-time images of both the femoral artery and immediately adjacent vein, but vibrations can result in movement of the small insertion needle that is used to initially cannulate the artery. This can reduce the success rate of cannulation and thus reduce the benefit of REBOA. Therefore, improvements in ultrasound techniques for transcutaneous vascular access are desired.</p><p>SUMMARY OF THE CLAIMS</p><p>[0003] A handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure may include a body. The system may include an ultrasound probe coupled with a first end of the body. The system may include a display device coupled with a second end of the body. The display device may be configured to present an image generated using the ultrasound probe.</p><p>[0004] In some embodiments, the system may include a needle-positioning member coupled with the body. The needle-positioning member may define a needle channel. The needle channel may be alignable with an imaging axis of the ultrasound probe. The needle-positioning member may be detachably coupled with the body. An angle of the needle-positioning member may be adjustable relative to the body. The needle-positioning member may be retractable into the body. The display device and the ultrasound probe may be battery powered. The display device may be detachably coupled with the body. A total length of the body may be between 4 inches and 15 inches.</p><p>[0005] Some embodiments of the present technology may encompass methods of performing cardiovascular resuscitation. The methods may include elevating an individual's head, shoulders, and heart relative to the individual's lower body while the individual's lower body remains generally aligned with a horizontal plane. The methods may include compressing the individual's chest while the individual's head, shoulders, and heart are elevated. The methods may include regulating an intrathoracic pressure of the individual while the individual's head, shoulders, and heart are elevated relative to the lower body. The methods may include performing REBOA on the individual.</p><p>[0006] In some embodiments, performing REBOA may include positioning an intra-aortic balloon in the individual's aorta using an ultrasound-guided cannulation device and inflating the intra-aortic balloon. The methods may include performing active compression decompression cardiopulmonary resuscitation on the individual while the individual is in a supine position in general alignment with the horizontal plane prior to elevating the individual's head, shoulders, and heart. The methods may include actively decompressing the individual's chest between each chest compression. Regulating the intrathoracic pressure may include interfacing an impedance threshold device with the individual's airway. Elevating the individual's head, shoulders, and heart may include elevating the individuals' head to a height of between 10 cm and 30 cm above the horizontal plane and elevating the individual's heart to a height of between 5 cm to 15 cm above the horizontal plane. REBOA may be performed on the individual while the individual's head, shoulders, and heart are elevated.</p><p>[0007] Some embodiments of the present technology may encompass methods for cannulating a vascular access structure. The methods may include maneuvering an ultrasound probe coupled with a body of a cannulation device over an individual's body. The methods may include locating a vascular access structure of the individual based on an image presented on a display device coupled with the body of the cannulation device. The image may be generated based on signals from the ultrasound probe. The methods may include inserting a medical implement into the vascular access structure.</p><p>[0008] In some embodiments, the medical implement may include a hollow needle. Inserting the medical implement into the vascular access structure may include aligning the hollow needle with the vascular access structure using a needle-positioning member of the body that holds the hollow needle into alignment with an imaging axis of the ultrasound probe. The methods may include advancing a guidewire through a hollow bore of the hollow needle and into the vascular access structure under ultrasound visualization. The methods may include retracting the hollow needle into a sheath prior to advancing the guidewire. The methods may include adjusting an orientation of the display device relative to the body.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0009] A further understanding of the nature and advantages of the disclosed technology may be realized by reference to the remaining portions of the specification and the drawings.</p><p>[0010] FIG. <b>1</b> illustrates a handheld cannulation device according to embodiments of the present technology.</p><p>[0011] FIG. <b>2</b> illustrates a handheld cannulation device according to embodiments of the present technology.</p><p>[0012] FIG. <b>3</b> illustrates operations of a method for performing Neuroprotective CPR in conjunction with REBOA according to embodiments of the present technology.</p><p>[0013] FIG. <b>4</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0014] FIG. <b>5</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0015] FIG. <b>6</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0016] FIG. <b>7</b> illustrates tracings of physiological parameters during a pig study.</p><p>[0017] FIG. <b>8</b> illustrates a cumulative 24-hour survival rate during a pig study.</p></shortdesdrw><p>[0018] Several of the figures are included as schematics. It is to be understood that the figures are for illustrative purposes, and are not to be considered of scale or proportion unless specifically stated to be of scale or proportion. Additionally, as schematics, the figures are provided to aid comprehension and may not include all aspects or information compared to realistic representations, and may include exaggerated material for illustrative purposes.</p><p>[0019] In the appended figures, similar components and/or features may have the same reference label. Further, various components of the same type may be distinguished by following the reference label by a letter that distinguishes among the similar components. If only the first reference label is used in the specification, the description is applicable to any one of the similar components having the same first reference label irrespective of the letter.</p><p>DETAILED DESCRIPTION OF THE INVENTION</p><p>[0020] The subject matter of embodiments of the present invention is described here with specificity to meet statutory requirements, but this description is not necessarily intended to limit the scope of the claims. The claimed subject matter may be embodied in other ways, may include different elements or steps, and may be used in conjunction with other existing or future technologies. This description should not be interpreted as implying any particular order or arrangement among or between various steps or elements except when the order of individual steps or arrangement of elements is explicitly described.</p><p>[0021] Outcomes with conventional cardiopulmonary resuscitation (CPR) are generally dismal, especially in out-of-hospital cardiac arrest patients. For example, in 2021 and 2022 only 7.4 percent of out-of-hospital cardiac arrest patients in the USA survived with favorable brain function. This is often due to lack of timely treatment and/or treatment using techniques that do not generate sufficient levels of cerebral perfusion pressure to adequately deliver oxygenated blood to the brain. Embodiments of the present invention are directed to devices and methods to improve outcomes after cardiac arrest when CPR is needed. In particular, embodiments may enable new methods that markedly improve brain perfusion during CPR, such as by performing Neuroprotective CPR, (defined herein as the combination of active compression and decompression of the chest, an impedance threshold device, and controlled gradual elevation of the head and thorax), in combination with ultrasound-guided delivery and deployment of a Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA) catheter. This sequential method of resuscitation provides a novel means to achieve markedly higher brain blood flow during CPR. In particular, while Neuroprotective CPR results in near normal cerebral perfusion pressures, higher brain blood flow, and a higher likelihood of survival after cardiac arrest, additional hemodynamic support can be particularly of benefit, especially during prolonged resuscitation efforts that last more than 20 minutes. The REBOA occludes the aorta to limit arterial blood flow distal to the occlusion balloon, and restricts the effective circulating vascular volume to a region above the occlusion, thereby increasing compression and decompression phase arterial pressures proximal to the occlusion. This results in unexpectedly higher cerebral perfusion pressures and coronary perfusion when used in conjunction with Neuroprotective CPR.</p><p>[0022] Performing Neuroprotective CPR in conjunction with REBOA may require specialized equipment and devices, including an ultrasound-guided femoral artery cannulation device. During REBOA, the femoral artery is often used as a catheterization access point. For example, femoral artery catheterization is used during coronary angiography, for arterial blood pressure monitoring, and for blood sampling. Access to the femoral artery is generally performed in a ‘blinded’ manner, by feeling for a pulse and then inserting a needle into the femoral artery, at times the pulse may be absent, difficult to feel, or difficult to identify, especially on patients exhibiting low blood pressure, during cardiac arrest, and/or during the performance of CPR. Additionally, in the setting of a cardiac arrest, when it may be of benefit to insert a REBOA catheter, motion associated with delivery of CPR can result pulsations of both the femoral artery and adjacent vein that may make it difficult to successfully cannulate the femoral artery. The ability to access the femoral artery may be further compromised when treating patients in out-of-hospital environments, as conventional imaging devices to visualize the artery may not be available, even in ambulances and/or smaller medical facilities. Moreover, oftentimes, non-hospital staff (e.g., paramedics, clinicians, etc.) are not properly trained in how to access the femoral artery for delivery of the occlusion device. Such issues make it even more difficult to implement REBOA in out-of-hospital treatment settings.</p><p>[0023] To further improve outcomes, a means to selectively and rapidly insert an aortic balloon occlusion catheter may be utilized in conjunction with Neuroprotective CPR. For example, embodiments of the present technology may include femoral artery catheterization devices that use ultrasound imaging to visualize and locate the femoral artery. The devices may include a needle holder and insertion system to assist in advancing a needle into the femoral artery and stabilizing the needle thereafter with on-going real time direct ultrasound visualization. By integrating an ultrasound probe and needle holder into a single device, the femoral artery catheterization devices and cannulation methods described herein may provide a safer, quicker, and more effective way to cannulate the femoral artery and other arteries and veins during Neuroprotective CPR, or as part of any other medical procedure. Such devices may help properly cannulate the femoral artery, even in the presence of vibrations or other movements from the performance of CPR that may result in movement of the small insertion needle that is used to initially cannulate the artery, thereby improving the success rate of cannulation. While disclosed primarily in the context of REBOA cannulation during Neuroprotective CPR, it will be appreciated that the cannulation techniques disclosed herein are not so limited. The cannulation devices and techniques described herein may be used to cannulate the femoral artery, and other vascular access structures (e.g., brachial artery, radial artery, carotid artery, jugular vein, and femoral vein) for any number of medical procedures. As such, it will be appreciated that the catheterization devices that use ultrasound imaging, including the cannula devices and techniques described herein, are not intended to be limited solely for use with CPR, but may be used with various other procedures as well.</p><p>[0024] In some embodiments, the Neuroprotective CPR+REBOA may be performed using existing CPR devices and methods that combine active compression and decompression of the chest, an impedance threshold device, and a means and structure for controlled gradual elevation of the head and thorax. Such CPR devices and methods may include those described herein, as well as those described in U.S. application Ser. No. 17/668,266, U.S. application Ser. No. 17/335,922, U.S. application Ser. No. 17/222,476, U.S. application Ser. No. 16/955,382, U.S. application Ser. No. 16/201,339, U.S. Pat. Nos. 11,259,988, 11,246,794, 11,096,861, 11,020,314, 10,667,987, 10,406,069, 10,406,068, 10,350,137, 9,801,782, 9,750,661, 9,707,152, 10,092,481, and 10,245,209, the complete disclosures of which have previously been incorporated by reference for all intents and purposes.</p><p>[0025] Turning now to FIG. <b>1</b>, one embodiment of an ultrasound-guided cannulation device <b>100</b> is illustrated. The cannulation device <b>100</b> may be in the form of a handheld device that may enable an operator to use ultrasound to visually locate and subsequently cannulate (or otherwise access) a desired vascular access structure. The cannulation device <b>100</b> may include an elongate body <b>102</b> that may be sized and shaped to be graspable using a single hand. In some embodiments, the body <b>102</b> may have a substantially constant cross-section along a length of the body <b>102</b>, while in other embodiments the body <b>102</b> may have a variable cross-section along the length. For example, as illustrated, the body <b>102</b> tapers from a patient interface end <b>104</b> to a display end <b>106</b>. Other profiles (e.g., stepped, angled, curved, etc.) may be used in various embodiments.</p><p>[0026] The body <b>102</b> may include all necessary components to generate and view an ultrasound image. For example, the body <b>102</b> may house a battery or other power source, one or more processors, circuits, ultrasonic transducers, display screens, etc. In other words, the entire ultrasound device including the screen, probe, power supply, and hardware and software are a single unit. This may enable an operator to view an ultrasound image and the cannulation device <b>100</b> simultaneously, which may help facilitate improved cannulation of a vascular access structure as will be described below. The patient interface end <b>104</b> may be or include a probe tip that includes one or more ultrasonic transducers that may transmit high frequency sound waves (e.g., &gt;20 kHz) to the body and record reflected or echo waves. An image processor disposed within the body <b>102</b> may use the recorded waves to develop 2-D real-time images. The image processor may also be disposed in other parts of the device. These images may be displayed, in real-time, using a display screen <b>108</b> that is integrated into the display end <b>106</b> of the body <b>102</b>. For example, in a particular embodiment, the display screen <b>108</b> may be positioned on an opposite end surface as the probe and/or ultrasonic transducer. In some embodiments, the display screen <b>108</b> may include a touchscreen and/or may be electrically coupled with one or more buttons and/or other user interface inputs that enable an operator to control operation of the cannulation device <b>100</b>. A smallest lateral dimension (e.g., length or width) of the display screen <b>108</b> may range from between about 0.5 inches and 3 inches in some embodiments, although larger display screens <b>100</b> are possible in various embodiments. In some embodiments images may be capable of being recorded, stored, and/or transmitted to a remote receiver.</p><p>[0027] The cannulation device <b>100</b> may further include a means to stabilize a hollow bore needle and/or syringe with a needle to cannulate a vascular access structure. Such a design of the cannulation device <b>100</b> may enable the ultrasound imager and needle to be held in one hand. For example, as illustrated in FIG. <b>1</b>, a needle-positioning member <b>110</b> is coupled with the body <b>102</b>, such as at a medial portion of the body <b>102</b> between the patient interface end <b>104</b> and the display end <b>106</b>. The needle-positioning member <b>110</b> may be permanently or removably coupled with the body <b>102</b> in various embodiments. For example, a fastener and/or quick connect/disconnect mechanism (such as a clip) may be used to couple the needle-positioning member <b>110</b> with the body <b>102</b>, which may enable the needle-positioning member <b>110</b> to be removed and replaced (e.g., may be disposable), while the body <b>102</b> (and ultrasound components) may be reused. In some embodiments, the needle-positioning member <b>110</b> may extend laterally outward from the body <b>102</b> at an angle, such as less than or about 90 degrees relative to a longitudinal axis of the body <b>102</b>, with a distal end of the needle-positioning member <b>110</b> pointing substantially away from the display end <b>106</b>. The distal end of the needle-positioning member <b>110</b> may include a needle holder <b>112</b> that may securely receive a cannulation needle <b>114</b> during the insertion process. For example, a tip of the cannulation needle <b>114</b> may be angled toward a point under the probe tip in some embodiments, such that when the probe tip is aligned with a target vascular access structure, the tip of the cannulation needle <b>114</b> is similarly aligned with the target vascular access structure. In some embodiments, the cannulation needle <b>114</b> may be stored in a retracted position and advanced from the protective sheath only when the cannulation needle <b>114</b> is being deployed.</p><p>[0028] In some embodiments, the needle-positioning member <b>110</b> may include an adjustment mechanism <b>116</b>, such as (but not limited to) a ball joint, that may enable an angle of the needle holder <b>112</b> (and associated cannulation needle <b>114</b>) to be adjusted relative to the body <b>102</b> and upper portion of the needle-positioning member <b>110</b>. This may enable the angle of the cannulation needle <b>114</b> to be adjusted such that the cannulation needle <b>114</b> can be inserted over a range of insertion angles varying from 20-80 degrees relative to the surface of the skin and the body <b>102</b>. The adjustment mechanism <b>116</b> may be positioned at any point of the needle-positioning member <b>110</b>. For example, the adjustment mechanism <b>116</b> may be at the interface with the body <b>102</b>, at the interface with the needle holder <b>112</b>, and/or at a medial position along a length of the needle-positioning member <b>110</b>.</p><p>[0029] In some embodiments, the cannulation needle <b>114</b> may be advanced using an electrical worm screw motor or equivalent to control for small forward and backward movements. In other embodiments, the cannulation needle <b>114</b> may be advanced manually through a conical or cylindrical locking system wherein a slight twist of the needle holder <b>112</b> will loosen or tighten the grip of the cannulation needle <b>114</b>. Further, the distance of the tip of the cannulation needle <b>114</b> from a middle of the contact point of the ultrasound probe with the skin can vary in some embodiments from between 0.5 cm and 4 cm to facilitate stable and proper insertion of the cannulation needle <b>114</b> into the target vascular access structure. Both the angle of the cannulation needle <b>114</b> and distance from the tip of the cannulation needle <b>114</b> to the contact point of the ultrasound probe may be fixed and/or may be varied by the operator in various embodiments.</p><p>[0030] Once the vessels are imaged, for example with the cannulation device <b>100</b> being held by one of the operator's hands, the cannulation needle <b>114</b> may be advanced through the skin by the operator's other free hand and/or by a motorized delivery system. Once in the femoral artery (in this example) or other vascular access structure, a thin guidewire may be inserted through the hollow bore needle into the artery under visualization of the images displayed on display screen <b>106</b>. At this point the entire cannulation device <b>100</b> may be lifted upward, leaving the guidewire inserted in the artery. In some embodiments, the needle-positioning member <b>110</b> and/or needle holder <b>112</b> may be disconnected from the body <b>102</b>. The guidewire is then used to maintain vascular access and a vascular access sheath and dilator can be inserted into the blood vessel over the wire.</p><p>[0031] The novel approach to vascular access and device to facilitate vascular access and novel approach coupling image-guided transcutaneous arterial access for rapid placement of a REBOA catheter combined with Neuroprotective CPR provides a novel means to rapidly improve blood pressures, especially cerebral and coronary blood flow, during CPR.</p><p>[0032] FIG. <b>2</b> illustrates another embodiment of an ultrasound-guided cannulation device <b>200</b>. The cannulation device <b>200</b> may be similar to cannulation device <b>100</b> and may include any feature described in relation to cannulation device <b>100</b>. The cannulation device <b>200</b> may be a handheld device that may enable an operator to use ultrasound to visually locate and subsequently cannulate (or otherwise access) a desired vascular access structure. The cannulation device <b>200</b> may include an elongate body <b>202</b> that may be sized and shaped to be graspable using a single hand. The body <b>202</b> may include all necessary components to generate and view an ultrasound image. For example, the body <b>202</b> may house a battery or other power source, one or more processors, circuits, ultrasonic transducers, display screens, etc. In some embodiments, a total length of the body <b>202</b> may be between 4 inches and 15 inches, between 6 inches and 12 inches, or between 8 inches and 10 inches, although other lengths are possible. A total weight of the cannulation device <b>200</b> may be less than 5 pounds, less than 4 pounds, less than 3 pounds less than 2 pounds, or less.</p><p>[0033] A patient interface end <b>204</b> may be or include a probe tip that includes one or more ultrasonic transducers that may transmit high frequency sound waves (e.g., &gt;20 kHz) to the body and record reflected or echo waves. An image processor disposed within the body <b>202</b> may use the recorded waves to develop 2-D real-time images. The image processor may also be disposed in other parts of the device. These images may be displayed, in real-time, using a display device <b>208</b> that is positioned on a display end <b>206</b> of the body <b>202</b>. For example, in a particular embodiment, the display device <b>208</b> may be positioned on an opposite end surface as the patient interface end <b>204</b>. The display device <b>208</b> may be integrated directed into the cannulation device <b>200</b> in some embodiments. In other embodiments, the display device <b>208</b> may be a detachable display (e.g., a portable monitor, tablet computer, mobile phone, etc.) that may be secured to the display end <b>206</b> using one or more clamps <b>218</b> and/or other connectors. The display screen may have any dimensions that permits the cannulation device <b>200</b> to be easily operable using a single hand. The display device <b>208</b> may be electrically coupled with the cannulation device <b>200</b> (e.g., to receive image data for display) via one or more wires or other electrical connectors. In some embodiments, the display device <b>208</b> may be wirelessly coupled to the cannulation device <b>200</b> (e.g., to receive image data for display), such as using Bluetooth, Wi-Fi, 5G, LTE, and/or other wireless communication protocol. The display <b>208</b> may be powered by a same or different power source as the rest of the cannulation device <b>200</b>.</p><p>[0034] In some embodiments, the display device <b>208</b> may be adjustably coupled with the body <b>202</b>. For example, the clamp <b>218</b> and/or other mount for the display device <b>208</b> may enable the display device <b>208</b> to be tilted and/or rotated relative to the body <b>202</b>, which may enable the viewing angle of the display device <b>208</b> to be adjusted by a user.</p><p>[0035] The cannulation device <b>200</b> may further include a means to stabilize a hollow bore needle and/or syringe with a needle to cannulate a vascular access structure. Such a design of the cannulation device <b>200</b> may enable the ultrasound imager and needle to be held in one hand. For example, as illustrated in FIG. <b>2</b>, a needle-positioning member <b>210</b> is coupled with the body <b>202</b>, such as at a lower portion of the body <b>202</b> proximate the patient interface end <b>204</b>. The needle-positioning member <b>210</b> may include a needle holder <b>212</b> that may securely receive a cannulation needle during the insertion process. The needle holder <b>212</b> may be a cylindrical channel defined through a length of the needle-positioning member <b>210</b> in some embodiments. For example, the channel may be angled toward a point under the probe tip in some embodiments, such that when the probe tip is aligned with a target vascular access structure, the tip of a cannulation needle inserted within the channel is similarly aligned with the target vascular access structure. The needle-positioning member <b>210</b> may be adjustable in some embodiments. For example, the needle-positioning member <b>210</b> may be pivoted relative to the body <b>202</b> to adjust an angle of the channel/needle holder <b>212</b> to adjust a needle insertion angle to an angle of between 20 degrees and 80 degrees in one or two axes relative to a horizontal plane. Such adjustability may enable the cannulation device <b>200</b> to be used to deliver an intra-aortic balloon to the femoral artery at different angles. Such adjustability may also enable the cannulation device <b>200</b> to be usable with other vascular structures and/or for different cannulation procedures. In some embodiments, a retractable and/or detachable needle-positioning member <b>210</b> may be used, which may enable the needle to be stabilized using the cannulation device <b>200</b> and allowing the rescuer to position the needle, advance the needle into the femoral artery under direct visualization. The needle-positioning member <b>210</b> may then enable a user to remove their hand from the needle to free the hand up for threading a flexible guidewire through the bore of the needle.</p><p>[0036] The angle and/or other position of the needle-positioning member <b>210</b> may be adjusted manually and/or automatically. For example, in some embodiments the angle of the needle holder may be automatically adjusted based upon the determination by the imaging system of the depth of the artery under study. In some embodiments, the needle may be automatically advanced under imaging by the imaging system and motorized needle insertion motor. In some embodiments, the distance from the needle holder to the imaging probe can be adjusted to facilitate both viewing of the needle as it is inserted, regardless of whether the operator views the artery in a cross-section, transverse-section and/or other imaging section.</p><p>[0037] In operation, a user may hold the body <b>202</b> with one hand while the other hand is used to cannulate the femoral artery using a syringe attached to a needle. The user may maneuver the probe over a patient's leg and view the display device <b>208</b> to locate the femoral artery (or other vascular access structure). The needle may be inserted through the needle holder <b>212</b> (which may occur prior to locating the vascular access structure) and inserted into the vascular access structure. The syringe may be removed and a guidewire may be advanced through a hollow bore of the needle and into the artery under ultrasound visualization. A sheath may be advanced over the guidewire and then the needle may be retracted into a sheath to protect the user from an inadvertent needle stick. In some embodiments, the needle-positioning member <b>210</b> may be detached from the body <b>202</b>. A REBOA catheter may be placed into the femoral artery and advanced blindly to the level of the diaphragm. The cannulation device <b>200</b> may be easily rotated (and the display device <b>208</b> adjusted) to visualize the guidewire as it is advanced into the vessel. Such processes may enable intra-aortic balloons to be more easily and accurately positioned for REBOA procedures, even when used by out-of-hospital personnel.</p><p>[0038] FIG. <b>3</b> depicts a process <b>300</b> for performing Neuroprotective CPR in conjunction with REBOA. Process <b>300</b> may be performed using a cannulation device such as those described herein. Process <b>300</b> may include performing active compression decompression cardiopulmonary resuscitation (ACD-CPR) on an individual while the individual is in a supine position in general alignment with a horizontal plane at operation <b>302</b>. At operation <b>304</b>, the individual's head, shoulders, and heart may be elevated relative to the individual's lower body while the individual's lower body remains generally aligned with the horizontal plane. The head may be elevated to a height of between about 10 cm and 30 cm above the horizontal plane and the heart may be elevated to a height of between about 5 cm to 15 cm or 8 cm and 12 cm above the horizontal plane. The individual's chest may be compressed and/or actively decompressed while the individual's head, shoulders, and heart are elevated at operation <b>306</b>. In some embodiments, the elevation of the individual and/or the chest compressions/decompressions may be performed using a head up CPR elevation device, such as those described in U.S. application Ser. No. 17/668,266, U.S. application Ser. No. 17/335,922, U.S. application Ser. No. 17/222,476, U.S. application Ser. No. 16/955,382, U.S. application Ser. No. 16/201,339, U.S. Pat. Nos. 11,259,988, 11,246,794, 11,096,861, 11,020,314, 10,667,987, 10,406,069, 10,406,068, 10,350,137, 9,801,782, 9,750,661, 9,707,152, 10,092,481, and 10,245,209, the complete disclosures of which have previously been incorporated by reference for all intents and purposes.</p><p>[0039] At operation <b>308</b>, the intrathoracic pressure of the individual may be regulated using an impedance threshold device interfaced with the individual's airway and/or other intrathoracic pressure regulation device both while the individual is in the supine position and while the individual's head, shoulders, and heart are elevated relative to the lower body. For example, the intrathoracic pressure regulation may be performed before commencement of chest compressions, during chest compressions, and/or after chest compressions. At operation <b>310</b>, REBOA may be performed. For example, an intra-aortic balloon may be positioned within the patient's aorta to occlude blood flow from the aorta during the performance of CPR. The intra-aortic balloon may be positioned before commencement of chest compressions, during chest compressions, and/or after chest compressions.</p><p>[0040] Placement of the intra-aortic balloon may be performed using a cannulation device, such as cannulation device <b>100</b>/<b>200</b>. For example, an operator may position the patient interface end <b>104</b>/<b>204</b> of the body <b>102</b>/<b>202</b> against the patient. The operator may use the images presented on display screen/device <b>108</b>/<b>208</b> to position the cannulation needle <b>114</b> in the femoral artery and may subsequently advance the cannulation needle <b>114</b> through the skin. A guidewire may then be inserted through the hollow bore needle into the artery under visualization of the images displayed on display screen/device <b>108</b>/<b>208</b>. This guidewire may be used to advance the intra-aortic balloon to the aorta for subsequent inflation and occlusion of the aortic blood flow.</p><p>[0041] In some embodiments after there is a return of circulation to the individual, the REBOA balloon may remain inflated for a period of time (e.g., up to 5 minutes, up to 10 minutes, up to 15 minutes, up to 20 minutes, up to 25 minutes, up to 30 minutes, up to 45 minutes, or up to one hour in duration), depending upon the individual's physiologic status, for example, the individual's blood pressure. In some embodiments, the balloon deflation will occur based upon one or more physiologic measurements, such as a blood pressure of the individual. The physiologic measurements may be determined using one or more physiological sensors, such as (but not limited to) blood pressure sensors that are interfaced with the individual. The balloon may be deflated slowly or rapidly based upon the physiologic measurements. A closed loop or open loop feedback system may be used in some embodiments to control the timing and/or rate of balloon deflation after the return of spontaneous circulation.</p><p>[0042] In some embodiments, the deflation of the REBOA balloon may be coordinated with a change in elevation of the individual's head and thorax in order to maximize blood flow to the brain. In this case, blood flow to the brain may be monitored invasively and/or noninvasively. For example, the blood flow may be monitored based on a measurement of end tidal CO<sub>2 </sub>or/and cerebral oximetry. In this manner, the inflated balloon may be deflated safely, and in a partial manner if needed, in order to better perfuse the heart, the brain, and the organ systems distal to the balloon occlusion. Once the balloon is totally deflated, the balloon may be removed.</p><p>Examples</p><p>[0043] In one example, Neuroprotective CPR was performed for 30 minutes in a pig. After 30 minutes a REBOA catheter was inserted into the femoral artery and the aorta was occluded at the level of the diaphragm. Cerebral perfusion pressures increased by over 20 mmHg. When the head and thorax of the pig were lowered so they were horizontal instead of elevated, this effect was markedly decreased. As such, the combination of REBOA and Neuroprotective CPR provide a novel and unexpectedly superior way to perfusion the brain with blood during CPR. This physiology is shown in a series of hemodynamic tracing from a representative pig study.</p><p>[0044] Arterial, venous, and intracranial pressures were measured with microelectronic tipped Millar catheters during Neuroprotective CPR, as previously described (Moore J C, Salverda B, Rojas-Salvador C, Lick M, Debaty G, Lurie K. Controlled sequential elevation of the head and thorax combined with active compression decompression cardiopulmonary resuscitation and an impedance threshold device improves neurological survival in a porcine model of cardiac arrest. Resuscitation. 2020a; 150-23-28 DOI: https://doi.org/10.1016/j.resuscitation.2020.09.030). These tracings are illustrated in FIG. <b>4</b>.</p><p>[0045] At the start of the tracings in FIG. <b>4</b>, the pig had been receiving Neuroprotective CPR for 30 minutes. Before inflating the REBOA the arterial blood pressure was about 70/38 mmHg and the cerebral perfusion pressure was about 90/45 mmHg. After the REBOA inflation the arterial pressure increased to ˜125/50 mmHg and the cerebral perfusion pressure increased to 100/45 mmHg. Upon lowering the head and thorax, arterial and cerebral perfusion pressures decreased abruptly and with deflation of the REBOA the arterial and cerebral perfusion pressures decreased to 50/20 mmHg and 40/0 mmHg, respectively. These tracings demonstrate the synergistic and surprise finding that REBOA+Neuroprotective CPR can uniquely provide nearly normal arterial and cerebral perfusion pressures after prolonged Neuroprotective CPR, in the absence of any pharmacological intervention.</p><p>[0046] The Y axis on the right provides the unit of measurement of the physiological parameters. The Y axis on the left describes the parameter measured. The tracing <b>402</b> shows the aortic pressure, the tracing <b>404</b> shows the right atrial pressure, the tracing <b>406</b> shows the intracranial pressure, the tracing <b>408</b> shows the pressure in the distal aorta, and the tracing <b>410</b> shows the cerebral perfusion pressure. The x axis is time, in minutes.</p><p>[0047] These findings are shown again in FIG. <b>5</b>, from another similar pig study, where the impact of inflating the REBOA after 30 minutes of ongoing Neuroprotective CPR is shown in a higher resolution. The tracing <b>502</b> is the airway tracing. A positive pressure breath is delivered approximately 10 times per minute. The rise in aortic and cerebral pressures with REBOA during Neuroprotective CPR is profound. In FIG. <b>5</b>, the rise in coronary (CorPP) (tracing <b>504</b>) and cerebral (CerPP) (tracing <b>506</b>) pressures with inflation of the REBOA during Neuroprotective CPR is clearly seen.</p><p>[0048] In another study, female and male swine (˜40 kg) were sedated, intubated, and anesthetized. Bilateral femoral arterial access was obtained for continuous arterial pressure measurement and REBOA placement in the proximal thoracic aorta. Central venous and intracranial access was obtained for continuous measurement during CPR. Hemodynamics were measured with solid state pressure catheters. The arterial pressure catheter was placed in the thoracic aorta and confirmed via fluoroscopy. After 36 minutes of AHUP-CPR, animals underwent two-minute epochs of AHUP-CPR, AHUP+REBOA CPR, ACD+ITD CPR flat+REBOA, and ACD+ITD CPR flat. The reverse sequence was then performed. Pressures are reported as mean (mmHg) plus or minus standard deviation. A paired t-test was performed for the outcomes of Cerebral Perfusion Pressure (CerPP) and Coronary Perfusion Pressure (CorPP).</p><p>[0049] As illustrated in FIG. <b>6</b>, the CerPP was 42.8 plus or minus 12.2 with AHUP+REBOA CerPP versus a CerPP with ACD+ITD CPR flat+REBOA of 25.9 plus or minus 11.5 (p&lt;0.0001). The CorPP was 20.2 plus or minus 19.9 with AHUP+REBOA CorPP versus ACD+ITD CPR flat+REBOA CorPP of 15.7 plus or minus 18.8 (p&lt;0.01). The AHUP+REBOA CerPP was 63 percent of the baseline CerPP value after 38 minutes of CPR. The return of Spontaneous Circulation (ROSC) was obtained in 4/7 (57 percent) after 50+ minutes of CPR.</p><p>[0050] In another study, REBOA was added to the treatment of pigs (n=7) after 8 min of untreated VF and &gt;40 min of AHUP-CPR, there was an immediate, profound, and unanticipated rise in coronary and most of all cerebral perfusion pressure, without a significant rise in ICP as shown in FIG. <b>7</b>. All key hemodynamic parameters, especially cerebral perfusion pressure, increased immediately upon initiation of REBOA. By contrast, the benefits of REBOA were significantly less when the pigs were placed in the flat position. Statistically significant differences were observed only when REBOA was added to AHUP CPR (ACD+ITD in elevated position) and not when REBOA was added to ACD+ITD in the flat position.</p><p>[0051] In another study, a porcine model of untreated VF (15 min) was used to simulate refractory out-of-hospital cardiac arrest (r-OHCA). <b>22</b> pigs were randomized to either 25 min of AHUP-CPR (early AHUP) or 10 min of C-CPR followed by 15 min of AHUP-CPR (late AHUP) and then epinephrine, amiodarone, and defibrillation. Pigs in the early AHUP-CPR group had a 50 percent 24-hour survival rate and all survivors had normal pupillary reflexes. By contrast, with late AHUP-CPR only 1/10 pigs survived, and this animal remained comatose. Neurologic deficit scores were significantly better with early AHUP-CPR.</p><p>[0052] For the primary endpoint, 24-hour neurologic function, a veterinarian blinded to the CPR intervention assessed pigs using a Neurological Deficit Score (NDS, 0=normal and 260=worst deficit score or death). The cumulative 24-hour survival was 45.5 percent for early AHUP-CPR versus 9.1 percent for late AHUP-CPR (p&lt;0.02) as shown in FIG. <b>8</b>. The mean plus or minus SD NDS was 203 plus or minus 80 with early AHUP-CPR versus 259 plus or minus 3 with late AHUP-CPR group (p=0.04). Mean plus or minus SD ETCO2 (mmHg) values, known to correlate with circulation during CPR, were significantly higher with early versus late AHUP-CPR after 10 min of CPR (45 plus or minus 4 vs 27 plus or minus 6, p&lt;0.001) and 24 min of CPR (42 plus or minus 9 vs 31 plus or minus 12, p&lt;0.001). These findings are strongly supportive that the more rapid the restoration of circulation to the heart and brain after a prolonged arrest, the better the outcome.</p><p>[0053] These studies demonstrate that the combination of Neuroprotective CPR and REBOA may markedly improve brain perfusion during CPR by providing near normal cerebral perfusion pressures, higher brain blood flow, a higher likelihood of survival after cardiac arrest, higher compression and decompression phase arterial pressures proximal to the occlusion, and unexpectedly higher cerebral perfusion pressures. In particular, the examples have demonstrated increases of coronary and cerebral perfusion pressure of nearly 50 percent, which increases the effectiveness of head up CPR. In these studies, the femoral artery access was obtained with ultrasound guidance. In addition, these studies show the need for further interventions when pigs and patients cannot be resuscitated with just AHUP-CPR. There remains a great need for newer CPR techniques, methods and devices to further improve outcomes beyond which are shown in FIGS. <b>7</b> and <b>8</b>.</p><p>[0054] The methods, systems, and devices discussed above are examples. Various configurations may omit, substitute, or add various method steps or procedures, or system components as appropriate. For instance, in alternative configurations, the methods may be performed in an order different from that described, and/or various stages may be added, omitted, and/or combined. Also, features described with respect to certain configurations may be combined in various other configurations. Different aspects and elements of the configurations may be combined in a similar manner. Also, technology evolves and, thus, many of the elements are examples and do not limit the scope of the disclosure or claims.</p><p>[0055] Specific details are given in the description to provide a thorough understanding of example configurations (including implementations). However, configurations may be practiced without these specific details. For example, well-known circuits, processes, algorithms, structures, and techniques have been shown without unnecessary detail in order to avoid obscuring the configurations. This description provides example configurations only, and does not limit the scope, applicability, or configurations of the claims. Rather, the preceding description of the configurations will provide those skilled in the art with an enabling description for implementing described techniques. Various changes may be made in the function and arrangement of elements without departing from the spirit or scope of the disclosure.</p><p>[0056] Also, configurations may be described as a process which is depicted as a flow diagram or block diagram. Although each may describe the operations as a sequential process, many of the operations may be performed in parallel or concurrently. In addition, the order of the operations may be rearranged. A process may have additional steps not included in the figure. Furthermore, examples of the methods may be implemented by hardware, software, firmware, middleware, microcode, hardware description languages, or any combination thereof. When implemented in software, firmware, middleware, or microcode, the program code or code segments to perform the necessary tasks may be stored in a non-transitory computer-readable medium such as a storage medium. Processors may perform the described tasks.</p><p>[0057] Furthermore, the examples described herein may be implemented as logical operations in a computing device in a networked computing system environment. The logical operations may be implemented as: (i) a sequence of computer implemented instructions, steps, or program modules running on a computing device; and (ii) interconnected logic or hardware modules running within a computing device.</p><p>[0058] Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.</p><p>[0059] As used herein and in the appended claims, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" include plural references unless the context clearly dictates otherwise. Thus, for example, reference to \\\"a heater\\\" includes a plurality of such heaters, and reference to \\\"the aperture\\\" includes reference to one or more apertures and equivalents thereof known to those skilled in the art, and so forth.</p><p>[0060] Also, the words \\\"comprise(s)\\\", \\\"comprising\\\", \\\"contain(s)\\\", \\\"containing\\\", \\\"include(s)\\\", and \\\"including\\\", when used in this specification and in the following claims, are intended to specify the presence of stated features, integers, components, or operations, but they do not preclude the presence or addition of one or more other features, integers, components, operations, acts, or groups.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. A handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure, comprising: <BR />a body;<BR />an ultrasound probe coupled with a first end of the body; and<BR />a display device coupled with a second end of the body, wherein the display device is configured to present an image generated using the ultrasound probe.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 1, further comprising: <BR />a needle-positioning member coupled with the body, the needle-positioning member defining a needle channel, wherein the needle channel is alignable with an imaging axis of the ultrasound probe.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>3</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 2, wherein: <BR />the needle-positioning member is detachably coupled with the body.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>4</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 2, wherein: <BR />an angle of the needle-positioning member is adjustable relative to the body.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"2\\\"><p><b>5</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 2, wherein: <BR />the needle-positioning member is retractable into the body.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>6</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 1, wherein: <BR />the display device and the ultrasound probe are battery powered.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>7</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 1, wherein: <BR />the display device is detachably coupled with the body.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>8</b>. The handheld ultrasound guiding system for the transcutaneous delivery of a medical implement into a vascular access structure of claim 1, wherein: <BR />a total length of the body is between 4 inches and 15 inches.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"yes\\\"><p><b>9</b>. A method of performing cardiovascular resuscitation, comprising: <BR />elevating an individual&apos;s head, shoulders, and heart relative to the individual&apos;s lower body while the individual&apos;s lower body remains generally aligned with a horizontal plane;<BR />compressing the individual&apos;s chest while the individual&apos;s head, shoulders, and heart are elevated;<BR />regulating an intrathoracic pressure of the individual while the individual&apos;s head, shoulders, and heart are elevated relative to the lower body; and<BR />performing REBOA on the individual.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>10</b>. The method of performing cardiovascular resuscitation of claim 9, wherein: <BR />performing REBOA comprises positioning an intra-aortic balloon in the individual&apos;s aorta using an ultrasound-guided cannulation device and inflating the intra-aortic balloon.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>11</b>. The method of performing cardiovascular resuscitation of claim 9, further comprising: <BR />performing active compression decompression cardiopulmonary resuscitation on the individual while the individual is in a supine position in general alignment with the horizontal plane prior to elevating the individual&apos;s head, shoulders, and heart.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>12</b>. The method of performing cardiovascular resuscitation of claim 9, further comprising: <BR />actively decompressing the individual&apos;s chest between each chest compression.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>13</b>. The method of performing cardiovascular resuscitation of claim 9, wherein: <BR />regulating the intrathoracic pressure comprises interfacing an impedance threshold device with the individual&apos;s airway.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>14</b>. The method of performing cardiovascular resuscitation of claim 9, wherein: <BR />elevating the individual&apos;s head, shoulders, and heart comprises elevating the individuals&apos; head to a height of between 10 cm and 30 cm above the horizontal plane and elevating the individual&apos;s heart to a height of between 5 cm to 15 cm above the horizontal plane.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>15</b>. The method of performing cardiovascular resuscitation of claim 9, wherein: <BR />REBOA is performed on the individual while the individual&apos;s head, shoulders, and heart are elevated.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"yes\\\"><p><b>16</b>. A method for cannulating a vascular access structure, comprising: <BR />maneuvering an ultrasound probe coupled with a body of a cannulation device over an individual&apos;s body;<BR />locating a vascular access structure of the individual based on an image presented on a display device coupled with the body of the cannulation device, wherein the image is generated based on signals from the ultrasound probe; and<BR />inserting a medical implement into the vascular access structure.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"16\\\"><p><b>17</b>. The method for cannulating a vascular access structure of claim 16, wherein: <BR />the medical implement comprises a hollow needle; and<BR />inserting the medical implement into the vascular access structure comprises aligning the hollow needle with the vascular access structure using a needle-positioning member of the body that holds the hollow needle into alignment with an imaging axis of the ultrasound probe.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"17\\\"><p><b>18</b>. The method for cannulating a vascular access structure of claim 17, further comprising: <BR />advancing a guidewire through a hollow bore of the hollow needle and into the vascular access structure under ultrasound visualization.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"18\\\"><p><b>19</b>. The method for cannulating a vascular access structure of claim 18, further comprising: <BR />retracting the hollow needle into a sheath prior to advancing the guidewire.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"16\\\"><p><b>20</b>. The method for cannulating a vascular access structure of claim 16, further comprising: <BR />adjusting an orientation of the display device relative to the body.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B90/00\",\n",
      "\n",
      "\"A61H31/00\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B17/3403\",\n",
      "\n",
      "\"A61B2090/372\",\n",
      "\n",
      "\"A61B2090/378\",\n",
      "\n",
      "\"A61B90/37\",\n",
      "\n",
      "\"A61H31/007\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"RESUSCITATION INNOVATIONS LLC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20221103,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20231108,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20231108, OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LURIE, KEITH G;REEL/FRAME:065499/0091, Owner: VITALINC LLC, MINNESOTA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20231108\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LURIE, KEITH G;REEL/FRAME:065499/0091\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"VITALINC LLC, MINNESOTA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240402,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240402,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240509,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240806,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240806,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240808,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20240808, OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:VITALINC LLC;REEL/FRAME:068229/0589, Owner: RESUSCITATION INNOVATIONS LLC, MINNESOTA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20240808\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:VITALINC LLC;REEL/FRAME:068229/0589\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESUSCITATION INNOVATIONS LLC, MINNESOTA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240923,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20240923,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241224,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPV1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Verified\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20241224,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PAT1\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"Patented Case\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230501913\",\n",
      "\n",
      "\"ad\": 20231103,\n",
      "\n",
      "\"pn\": \"US2024148465\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20250115,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STCF\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT GRANT\",\n",
      "\n",
      "\"details\": \"Text: PATENTED CASE\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PATENTED CASE\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=102542833\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20190415992\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"INTERLOCK MECHANISMS TO DISENGAGE AND ENGAGE A TELEOPERATION MODE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method for engaging and disengaging a surgical instrument of a surgical robotic system comprising: receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<shortsum><p>BACKGROUND</p><p>Field</p><p>[0001] Embodiments related surgical robotic systems, are disclosed. More particularly, embodiments related to interlock mechanisms that disengage and engage a teleoperation mode, are disclosed.</p><p>Background</p><p>[0002] Endoscopic surgery involves looking into a patient's body and performing surgery inside the body using endoscopes and other surgical tools. For example, laparoscopic surgery can use a laparoscope to access and view an abdominal cavity. Endoscopic surgery can be performed using manual tools and/or a surgical robotic system having robotically-assisted tools.</p><p>[0003] A surgical robotic system may be remotely operated by a surgeon to command a robotically-assisted tool located at an operating table. Such operation of a robotically-assisted tool remotely by a surgeon may be commonly referred to as teleoperation. For example, the surgeon may use a computer console located in the operating room, or it may be located in a different city, to command a robot to manipulate the surgical tool mounted on the operating table. The robotically-controlled surgical tool can be an endoscope mounted on a robotic arm. Accordingly, the surgical robotic system may be used by the remote surgeon to perform an endoscopic surgery.</p><p>[0004] The surgeon may provide input commands to the surgical robotic system, and one or more processors of the surgical robotic system can control system components in response to the input commands. For example, the surgeon may hold in her hand a user input device such as a joystick or a computer mouse that she manipulates to generate control signals to cause motion of the surgical robotic system components, e.g., an actuator, a robotic arm, and/or a surgical tool of the robotic system.</p><p>SUMMARY</p><p>[0005] During teleoperation with an open display in which the user can view their surroundings (as compared to a periscope type display) there is the possibility that the surgeon is looking away from the screen but still holding the user input devices that control the robotic tools. This introduces a risk since the surgeon could move the user input devices and unintentionally move the tools while not focusing on the screen. Therefore, in some aspects, the processes disclosed herein provide methods for determining whether a number of interlocks required for teleoperation are met, and therefore the user is looking at the open display (or an immersive display such as a periscope) and focused on teleoperation, such that teleoperation mode may be engaged or continue. In this aspect, the system detects a number of interlock parameters, conditions, inputs or the like, and then determines based on the detection of the interlock parameters, whether the system should disengage or engage teleoperation. In general, the interlock mechanisms or parameters are designed to determine the following (1) is the user looking at the screen, (2) is the user holding the user interface device in a usable manner, and (3) is the environment set up for teleoperation mode. If the answer to all of these conditions is yes, than teleoperation mode is engaged, if the answer to any of them is no, teleoperation mode may be disengaged.</p><p>[0006] Representatively, in one aspect, a method for engaging and disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument. The user interface devices may include at least one of a handheld user input device and a foot pedal. The display may be an open display. The one or more interlock detection components may include at least one of an eye and/or head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display may include detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner may include detecting at least one user interface device is inside the surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some cases, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner may include detecting the surgical robotic system is not in a transportation configuration.</p><p>[0007] In another aspect, a method for disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument. In some aspects, determining the user is looking away from the display may include at least one of the following: detecting a user gaze is outside of the display; detecting a user head is rotated away from the display; detecting a chair associated of the surgical robotic system is facing away from the display; detecting a user is not seated on the chair; and detecting the display is in a non-use position. Iin some aspects, determining the surgical robotic system is configured in a non-usable manner may include at least one of the following: detecting at least one user interface device is outside a surgical workspace; detecting a received user interface device location is inaccurate; and detecting at least one user interface device is dropped by the user. In some aspects, determining the surgical workspace of the surgical robotic system is configured in a non-usable manner may include detecting the surgical robotic system is in a transportation configuration.</p><p>[0008] In still further aspects, a surgical robotic system is disclosed. The system may include a surgical instrument; a user console comprising a display, an interlock detecting component, and a user interface device; and one or more processors communicatively coupled to the interlock detecting component, the processors configured to: receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device; determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument. In some aspects, the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner. The interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display may include: least one of an eye and/or a head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor. The interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner may include: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor. The interlock detecting component for detecting the interlock indicating the surgical workspace may be configured in a usable manner comprises a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display comprise: detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: detecting at least one user interface device is inside a surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some aspects, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner includes detecting the surgical robotic system is not in a transportation configuration.</p><p>[0009] The above summary does not include an exhaustive list of all aspects of the present invention. It is contemplated that the invention includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0010] The embodiments of the invention are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \\\"an\\\" or \\\"one\\\" embodiment of the invention in this disclosure are not necessarily to the same embodiment, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one embodiment of the invention, and not all elements in the figure may be required for a given embodiment.</p><p>[0011] FIG. 1 is a pictorial view of an example surgical robotic system in an operating arena, in accordance with an embodiment.</p><p>[0012] FIG. 2 is a pictorial view of a user console, in accordance with an embodiment.</p><p>[0013] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment.</p><p>[0014] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment.</p><p>[0015] FIG. 5 is a block diagram of a computer portion of a surgical robotic system, in accordance with an embodiment.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0016] In various embodiments, description is made with reference to the figures. However, certain embodiments may be practiced without one or more of these specific details, or in combination with other known methods and configurations. In the following description, numerous specific details are set forth, such as specific configurations, dimensions, and processes, in order to provide a thorough understanding of the embodiments. In other instances, well-known processes and manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the description. Reference throughout this specification to \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, means that a particular feature, structure, configuration, or characteristic described is included in at least one embodiment. Thus, the appearance of the phrase \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p>[0017] In addition, the terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the invention. Spatially relative terms, such as \\\"beneath\\\", \\\"below\\\", \\\"lower\\\", \\\"above\\\", \\\"upper\\\", and the like may be used herein for ease of description to describe one element's or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as \\\"below\\\" or \\\"beneath\\\" other elements or features would then be oriented \\\"above\\\" the other elements or features. Thus, the exemplary term \\\"below\\\" can encompass both an orientation of above and below. The device may be otherwise oriented (e.g., rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.</p><p>[0018] As used herein, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" are intended to include the plural forms as well, unless the context indicates otherwise. It will be further understood that the terms \\\"comprises\\\" and/or \\\"comprising\\\" specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.</p><p>[0019] The terms \\\"or\\\" and \\\"and/or\\\" as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, \\\"A, B or C\\\" or \\\"A, B and/or C\\\" mean \\\"any of the following: A; B; C; A and B; A and C; B and C; A, B and C.\\\" An exception to this definition will occur only when a combination of elements, functions, steps or acts are in some way inherently mutually exclusive.</p><p>[0020] Moreover, the use of relative terms throughout the description may denote a relative position or direction. For example, \\\"distal\\\" may indicate a first direction away from a reference point, e.g., away from a user. Similarly, \\\"proximal\\\" may indicate a location in a second direction opposite to the first direction, e.g., toward the user. Such terms are provided to establish relative frames of reference, however, and are not intended to limit the use or orientation of any particular surgical robotic component to a specific configuration described in the various embodiments below.</p><p>[0021] Referring to FIG. 1, this is a pictorial view of an example surgical robotic system <b>100</b> in an operating arena. The surgical robotic system <b>100</b> includes a user console <b>102</b>, a control tower <b>103</b>, and one or more surgical robots <b>120</b>, including robotic arms <b>104</b> at a surgical robotic platform <b>105</b>, e.g., an operating table, a bed, etc. The system <b>100</b> can incorporate any number of devices, tools, or accessories used to perform surgery on a patient <b>106</b>. For example, the system <b>100</b> may include one or more surgical tools <b>107</b> used to perform surgery. A surgical tool <b>107</b> may be an end effector that is attached to a distal end of a surgical arm <b>104</b>, for executing a surgical procedure.</p><p>[0022] Each surgical tool <b>107</b> may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool <b>107</b> may be a tool used to enter, view, or manipulate an internal anatomy of the patient <b>106</b>. In an embodiment, the surgical tool <b>107</b> is a grasper that can grasp tissue of the patient. The surgical tool <b>107</b> may be controlled manually, by a bedside operator <b>108</b>; or it may be controlled robotically, via actuated movement of the surgical robotic arm <b>104</b> to which it is attached. The robotic arms <b>104</b> are shown as a table-mounted system, but in other configurations the arms <b>104</b> may be mounted in a cart, ceiling or sidewall, or in another suitable structural support.</p><p>[0023] Generally, a remote operator <b>109</b>, such as a surgeon or other operator, may use the user console <b>102</b> to remotely manipulate the arms <b>104</b> and/or the attached surgical tools <b>107</b>, e.g., teleoperation. The user console <b>102</b> may be located in the same operating room as the rest of the system <b>100</b>, as shown in FIG. 1. In other environments however, the user console <b>102</b> may be located in an adjacent or nearby room, or it may be at a remote location, e.g., in a different building, city, or country. The user console <b>102</b> may comprise a seat <b>110</b>, one or more user interface devices, for example, foot-operated controls <b>113</b> or handheld user input devices (UID) <b>114</b>, and at least one user display <b>115</b> that is configured to display, for example, a view of the surgical site inside the patient <b>106</b>. In the example user console <b>102</b>, the remote operator <b>109</b> is sitting in the seat <b>110</b> and viewing the user display <b>115</b> while manipulating a foot-operated control <b>113</b> and a handheld UID <b>114</b> in order to remotely control the arms <b>104</b> and the surgical tools <b>107</b> (that are mounted on the distal ends of the arms <b>104</b>).</p><p>[0024] In some variations, the bedside operator <b>108</b> may also operate the system <b>100</b> in an \\\"over the bed\\\" mode, in which the bedside operator <b>108</b> (user) is now at a side of the patient <b>106</b> and is simultaneously manipulating a robotically-driven tool (end effector as attached to the arm <b>104</b>), e.g., with a handheld UID <b>114</b> held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator <b>108</b> may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient <b>106</b>.</p><p>[0025] During an example procedure (surgery), the patient <b>106</b> is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system <b>100</b> are in a stowed configuration or withdrawn configuration (to facilitate access to the surgical site.) Once access is completed, initial positioning or preparation of the robotic system <b>100</b> including its arms <b>104</b> may be performed. Next, the surgery proceeds with the remote operator <b>109</b> at the user console <b>102</b> utilizing the foot-operated controls <b>113</b> and the UIDs <b>114</b> to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, e.g., the bedside operator <b>108</b> who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms <b>104</b>. Non-sterile personnel may also be present to assist the remote operator <b>109</b> at the user console <b>102</b>. When the procedure or surgery is completed, the system <b>100</b> and the user console <b>102</b> may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilization and healthcare record entry or printout via the user console <b>102</b>.</p><p>[0026] In one embodiment, the remote operator <b>109</b> holds and moves the UID <b>114</b> to provide an input command to move a robot arm actuator <b>117</b> in the robotic system <b>100</b>. The UID <b>114</b> may be communicatively coupled to the rest of the robotic system <b>100</b>, e.g., via a console computer system <b>116</b>. Representatively, in some embodiments, UID <b>114</b> may be a portable handheld user input device or controller that is ungrounded with respect to another component of the surgical robotic system. For example, UID <b>114</b> may be ungrounded while either tethered or untethered from the user console. The term \\\"ungrounded\\\" is intended to refer to implementations where, for example, both UIDs are neither mechanically nor kinematically constrained with respect to the user console. For example, a user may hold a UID <b>114</b> in a hand and move freely to any possible position and orientation within space only limited by, for example, a tracking mechanism of the user console. The UID <b>114</b> can generate spatial state signals corresponding to movement of the UID <b>114</b>, e.g. position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator <b>117</b>. The robotic system <b>100</b> may use control signals derived from the spatial state signals, to control proportional motion of the actuator <b>117</b>. In one embodiment, a console processor of the console computer system <b>116</b> receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator <b>117</b> is energized to move a segment or link of the arm <b>104</b>, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID <b>114</b>. Similarly, interaction between the remote operator <b>109</b> and the UID <b>114</b> can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool <b>107</b> to close and grip the tissue of patient <b>106</b>.</p><p>[0027] The surgical robotic system <b>100</b> may include several UIDs <b>114</b>, where respective control signals are generated for each UID that control the actuators and the surgical tool (end effector) of a respective arm <b>104</b>. For example, the remote operator <b>109</b> may move a first UID <b>114</b> to control the motion of an actuator <b>117</b> that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc., in that arm <b>104</b>. Similarly, movement of a second UID <b>114</b> by the remote operator <b>109</b> controls the motion of another actuator <b>117</b>, which in turn moves other linkages, gears, etc., of the robotic system <b>100</b>. The robotic system <b>100</b> may include a right arm <b>104</b> that is secured to the bed or table to the right side of the patient, and a left arm <b>104</b> that is at the left side of the patient. An actuator <b>117</b> may include one or more motors that are controlled so that they drive the rotation of a joint of the arm <b>104</b>, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool <b>107</b> that is attached to that arm. Motion of several actuators <b>117</b> in the same arm <b>104</b> can be controlled by the spatial state signals generated from a particular UID <b>114</b>. The UIDs <b>114</b> can also control motion of respective surgical tool graspers. For example, each UID <b>114</b> can generate a respective grip signal to control motion of an actuator, e.g., a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool <b>107</b> to grip tissue within patient <b>106</b>.</p><p>[0028] In some aspects, the communication between the platform <b>105</b> and the user console <b>102</b> may be through a control tower <b>103</b>, which may translate user commands that are received from the user console <b>102</b> (and more particularly from the console computer system <b>116</b>) into robotic control commands that are transmitted to the arms <b>104</b> on the robotic platform <b>105</b>. The control tower <b>103</b> may also transmit status and feedback from the platform <b>105</b> back to the user console <b>102</b>. The communication connections between the robotic platform <b>105</b>, the user console <b>102</b>, and the control tower <b>103</b> may be via wired and/or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor and/or walls or ceiling of the operating room. The robotic system <b>100</b> may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. It will be appreciated that the operating room scene in FIG. 1 is illustrative and may not accurately represent certain medical practices.</p><p>[0029] In addition, in some aspects, surgical robotic system <b>100</b> may further include one or more interlock detecting mechanisms, devices, components or systems (e.g., trackers, sensors, etc), that can be used to determine whether a teleoperation mode should be engaged or, once engaged, should be disengaged. The interlock detecting mechanisms may detect, for example, whether a user is looking toward or away from the display <b>115</b>, the UID is configured in a usable manner and/or the surgical environment is set up in a usable manner, and in turn, whether a teleoperation mode should be engaged or disengaged. For example, during a teleoperation mode of the surgical robotic system <b>100</b>, in which the user is controlling the surgical tool <b>107</b> using the UID <b>114</b>, the user should be viewing the tool movement on display <b>115</b>. In some cases, however, the user may look away from the display <b>115</b> (intentionally or unintentionally) while still holding the UID <b>114</b> and controlling the surgical tool <b>107</b>. This introduces a risk since the user could move the UID <b>114</b> and, in turn, unintentionally move the tool <b>107</b> while not focusing on the display <b>115</b>. Surgical robotic system <b>100</b> may therefore further include interlock detecting mechanisms that can be used to determine whether the user is focused on the display <b>115</b>, or otherwise interlocked or engaged with the system, in such a way that teleoperation mode is appropriate. The various interlock detecting mechanisms may be coupled to, in communication with, or otherwise associated with components of, the user console <b>102</b>, and will be described in more detail in reference to FIG. 2.</p><p>[0030] It should be understood that \\\"engaging\\\" the teleoperation mode is intended to refer to an operation in which, for example, a UID or foot pedal that is prevented from controlling the surgical instrument, is transitioned to a mode (e.g., a teleoperation mode) in which it can now control the surgical instrument. On the other hand, disengaging the teleoperation mode is intended to refer to an operation which occurs when the system is in a teleoperation mode, and then transitioned to a mode (non-teleoperation mode) in which the UID or foot pedal can no longer control the surgical instrument. For example, teleoperation mode may be disengaged when the system determines that one or more of a number of interlocks required for teleoperation mode is no longer present or met. The interlocks required for teleoperation mode may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner.</p><p>[0031] Referring now to FIG. 2, FIG. 2 illustrates a pictorial view of an exemplary user console including a number of devices, mechanisms, systems and/or components used to detect whether the interlocks required for teleoperation mode are met. Representatively, user console <b>102</b> is shown including a chair <b>110</b> and display <b>115</b>, as previously discussed. In addition, user console <b>102</b> is shown having an optional display <b>202</b> mounted to chair <b>110</b> by a display arm <b>204</b>. For example, display <b>115</b> may be an open display, and display <b>202</b> may be an immersive display. The term \\\"open\\\" display is intended to refer to a display which is designed to allow the user to see outside of the display, for example with their peripheral vision, even when directly facing the display. In addition, in an open display, the user can be a distance from the display screen or not directly in front of the display, and still view a surgical procedure on the display. Therefore, in an open display such as display <b>115</b>, the fact that the user may not be close to the display or have their face directly in front of the display, would not necessarily mean the user is not looking at the display or focused on the surgical procedure. In the case of an open display, in which the user can turn their head and still see the display using their peripheral vision, it is therefore important that turning of the head slightly not be interpreted to mean the user is not focused on the display. Rather, in the case of an open display, there will be some tolerance to such actions and allow for teleoperation mode to continue. This is in contrast to an immersive display such as display <b>202</b>. For example, display <b>202</b> could be a completely immersive display (as in the case of a periscope) that prevents the user from seeing outside of the display screen when they are facing the display and requires the user to be relatively close to the display screen. For example, in the case of an immersive display, the user must have their face relatively close to, and facing, the display screen to use it to view the surgical procedure in progress. If the user pulls their head away from the display screen, or doesn't face the display screen, they can no longer see the display screen, therefore this would typically be interpreted to mean the user is not focused on the screen or not paying sufficient attention to continue in teleoperation mode.</p><p>[0032] To detect whether all the interlocks required for engagement of the teleoperation mode (or continuing in the teleoperation mode) are met, user console <b>102</b> may further include a number of interlock detecting devices, mechanisms, systems and/or components. The interlock detecting mechanisms may include, but are not limited to, an eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, an immersive display deployment sensor <b>214</b>, a UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, UID contact sensor <b>220</b>, and/or a transport sensor <b>222</b>. Each of the interlock detecting mechanisms are configured to detect at least one of a number of conditions (referred to herein as \\\"interlocks\\\") which are required to engage the teleoperation mode. As previously discussed, the interlocks may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner. Table 1 provides an exemplary listing of the specific interlocks which are required to engage a teleoperation mode, and to continue teleoperation mode, once engaged.</p><p>[0033] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Interlock</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze on the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated</TD></TR><TR><TD>toward the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is</TD></TR><TR><TD>rotated toward the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display</TD></TR><TR><TD>is in a use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is inside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace not distorted</TD></TR><TR><TD>and tracker signal is accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is being held by the user</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is not in a transportation</TD></TR><TR><TD>mode</TD></TR><TR></TR></Table></p><p>[0034] If any of interlocks 1-11 are not detected, a teleoperation mode cannot be engaged, or if already engaged, may be automatically disengaged. Said another way, if any one or more of the following conditions listed in Table 2 are determined (e.g., when any of interlocks 1-11 are not detected) teleoperation mode is either not engaged or disengaged.</p><p>[0035] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 2</TD></TR><TR></TR><TR><TD>Condition</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze is outside of the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated</TD></TR><TR><TD>away from the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is</TD></TR><TR><TD>rotated away from the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing away from the</TD></TR><TR><TD>display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is not seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is not resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display</TD></TR><TR><TD>is in a non-use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is outside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace distorted</TD></TR><TR><TD>and tracker signal is not accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is not being held by the user,</TD></TR><TR><TD>or is in a stored configuration</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is in a transportation</TD></TR><TR><TD>mode</TD></TR><TR></TR></Table></p><p>[0036] The interlock detecting mechanisms for detecting, for example, interlocks 1-7 listed in Table 1, which generally relate to whether the user is looking at or focused on the display, may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, chair swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, headrest pressure sensor <b>212</b>, and immersive display deployment sensor <b>214</b>. The interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether the user is holding the controllers in a usable manner, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b> and UID contact sensor <b>220</b>. The interlock detecting mechanisms for detecting interlock 11, which relates to whether the surgical environment is set up in a usable manner, may include transport sensor <b>222</b>, in addition to one or more of the UID sensors <b>216</b>, <b>218</b>, <b>220</b>. Once a condition or interlock is detected by the interlock detecting mechanism, the information is output to a processing component of the surgical robotic system, where it is received as an interlock input and monitored to determine whether teleoperation should continue, should be engaged or should be disengaged. For example, if interlock inputs corresponding to all the required interlocks for teleoperation mode are received, teleoperation mode is engaged. If any interlocks are missing, teleoperation mode is not engaged (if currently in a non-teleoperation mode), or disengaged (if currently in teleoperation mode).</p><p>[0037] Referring now in more detail to each of the interlock detecting mechanisms, eye and/or head tracker <b>206</b> may be a tracking device that is coupled to the display <b>115</b> to track a user's gaze and/or head with respect to the display <b>115</b>, and indicate whether or not the user is looking toward or away from the display. Eye and/or head tracker <b>206</b> may be attached to any portion of display <b>115</b> suitable for tracking a gaze of a user. For example, eye and/or head tracker <b>206</b> may be attached to a housing <b>224</b> of display <b>115</b>, for example, a top wall, a bottom wall, or a side wall of housing <b>224</b>, or integrated within a screen (not shown) mounted within the housing <b>224</b> of display <b>115</b>. Eye and/or head tracker <b>206</b> may include one or more proj ector(s) and camera(s) which face the user and can be used to track a gaze of a user. Representatively, the projector(s) may create a pattern of near-infrared light on the eyes of the user, and the camera(s) may take images of the user's eyes and the pattern. Eye and/or head tracker <b>206</b> may further be programmed to use this information (e.g., execute machine learning, image processing and/or mathematical algorithms) to determine where the user is looking based on a position of each of the user's eyes and/or gaze point or location relative to one another, and display <b>115</b>. For example, when the user is sitting on chair <b>110</b> in front of display <b>115</b>, eye and/or head tracker <b>206</b> may be configured to create a pattern of near-infrared light on the eyes of the user and take images of the user's eyes and the pattern. This information may, in turn, be input to the surgical robotic system processing component (e.g., an interlock input signal) to determine whether the gaze of the user is inside or outside of display <b>115</b>. If the gaze is determined to be inside the display, the user is determined to be looking toward the display. On the other hand, if the gaze is determined to be outside the display, the user is determined to be looking away from the display.</p><p>[0038] In addition, eye and/or head tracker <b>206</b> can also track a position, orientation and/or location of the user's head as an indication of whether the user is looking at the display. For example, tracker <b>206</b> may detect that the user's head is in front of the display and/or the user's face or nose is facing display <b>115</b>, which suggests the user is looking at the display, or that the user's head is not in front of the display and/or their face or nose is turned away from display <b>115</b>, which suggests the user is looking away from the display. In addition, the tracker <b>206</b> may be used to determine automatically if the open display <b>115</b> or the immersive display <b>202</b> should be used for teleoperation.</p><p>[0039] Still further, tracking markers <b>226</b> on the user's glasses and/or tracking markers <b>228</b> on the user's face mask may be used to determine whether the user is looking toward or away from the display <b>115</b>. The tracking markers <b>226</b> on the user's glasses (e.g., 3D glasses) and/or markers <b>228</b> on the face mask (if user is sterile) may be active or passive markers. The markers <b>226</b> and/or <b>228</b> may track if the user's head is rotated toward the screen, indicating the user is looking toward the display. The markers <b>226</b> and/or <b>228</b> may further track whether the head is rotated away from the screen, indicating the user is looking away from the display. In some cases where the markers <b>226</b> are attached to the user's glasses, they may be similar to the previously discussed eye and/or head tracker and include one or more projectors, cameras and/or processing algorithms that can analyze information received from the camera(s) and projector(s) to determine whether the user is looking at the display.</p><p>[0040] Additional interlock mechanisms for detecting whether or not the user is looking toward or away from the display may include a swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, and a headrest pressure sensor <b>212</b>. The swivel sensor <b>208</b> may be any type of sensing device or component suitable for detecting a rotation (or swivel) and/or orientation of the chair <b>110</b>, with respect to, for example, a stationary base <b>230</b>. For example, the swivel sensor <b>208</b> may include a position encoder, switch or the like. The swivel sensor <b>208</b> may then output a corresponding signal to a processing component of the surgical robotic system corresponding to the rotation, position and/or orientation of the chair <b>110</b>. The surgical robotic system can then use this information to determine whether the chair <b>110</b> is facing display <b>115</b>, and therefore a user seated in the chair is facing the display <b>115</b>. If, on the other hand, swivel sensor <b>208</b> detects the chair is facing away from the display, it is determined the user is looking away from the display. The chair pressure sensor <b>210</b> and the headrest pressure sensor <b>212</b> may be pressure sensors which are attached to the chair headrest and chair seat, respectively. The chair pressure sensor <b>210</b> can be used to detect when the user is seated in the chair <b>110</b>, and the headrest sensor <b>212</b> can be used to detect when the user's head is resting on the headrest. Sensors <b>210</b>, <b>212</b> may output a signal indicating that the user is seated in chair <b>110</b> and/or the head is resting on the headrest to the system processing component, which can be used as indicators that the user is looking at the display <b>115</b>. If the sensors <b>210</b>, <b>212</b> do not detect the user seated in the chair <b>110</b>, this is interpreted as an indication the user is not looking toward the display, or looking away from the display. Sensors <b>210</b> and <b>212</b> may be any type of sensor suitable for detecting the user applying pressure to the seat in the particular location in which the sensor is positioned, for example, they may be pressure plates that are mounted within portions of the headrest and seat which would be contacted by a user seated in the chair <b>110</b>. It should be understood, however, if chair <b>110</b> of console <b>102</b> does not swivel, swivel sensor <b>208</b> may be omitted, and interlock 4 of Table 1 (e.g., chair sensor indicates chair is facing the display) may not be required to engage teleoperation mode.</p><p>[0041] In addition, in embodiments where user console <b>102</b> includes immersive display <b>202</b>, an immersive display deployment sensor <b>214</b> to detect a location of the immersive display <b>202</b> may be provided. The immersive display deployment sensor <b>214</b> may be coupled to the immersive display <b>202</b> and include switches, encoders or the like that are operable to determine if display <b>202</b> is in a \\\"use\\\" position (e.g., position suitable for viewing by a user seated in the chair), \\\"non-use\\\" position (e.g., positioned not suitable for viewing by a user seated in the chair) and/or deployed position (e.g., positioned in front of the chair). This information can be used in conjunction with a sensor (e.g., pressure or light gate sensor) to determine if the user's head is in the immersive display <b>202</b>. A teleoperation mode will only be engaged if the immersive display <b>202</b> is in the \\\"use\\\" configuration and the sensor detects that the user's head is in the immersive display <b>202</b>. If display is determined to be in \\\"non-use\\\" position and/or the user's head is not detected in display <b>202</b>, this is interpreted as an indication the user is looking away from the display. It should be understood, however, if immersive display <b>202</b> is omitted, deployment sensor <b>214</b> may be omitted, and interlock 7 of Table 1 (e.g., immersive display sensor indicates immersive display is in a use position) is not required to engage teleoperation mode.</p><p>[0042] In addition, the interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether a user is holding the controllers (e.g., UID) in a usable or non-usable manner or configuration, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be, for example, an optical tracker, a position tracker and/or an encoder, that is coupled to the user console and/or the UID. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be used to track, for example, the position of the UID, whether the user is holding the UID in a usable or non-usable manner, and/or whether the UID is inside or outside of the trackable workspace. If the UID is outside of the workspace, the UID is not being held in a usable manner, and the system should be disengaged. In addition, if the tracking space is distorted and the signal received from the tracker does not give an accurate reading, the UID is considered not to be in a usable manner or in a non-usable configuration, and teleoperation should be disengaged. Still further, the sensor <b>216</b>, <b>218</b> and/or <b>220</b> can be used to track whether the UID is stored within the user console (e.g., within a holster or other holding mechanism) and/or the UID has been dropped. If the UID is stored within the console and/or has been dropped, such that the user is no longer holding the UID, the UID is considered to be in a non-usable configuration, and teleoperation should be disengaged. In some embodiments, one or more of sensors <b>216</b>, <b>218</b>, <b>220</b> may be an optical tracker and/or an absolute encoders. In some embodiments, a position sensor (optical tracker) is mounted on the user console (e.g., near display <b>115</b>) and/or the base of the UID. In some embodiments, the UID is tracked by a sensor coupled to the console or the user. The position sensor can provide the position of the UID relative to a user and/or a ground reference point (not shown). In some embodiments, more than one (e.g., a plurality, several) tracking technologies are used in conjunction.</p><p>[0043] In addition, the interlock detecting mechanisms for detecting, for example, interlock 11 listed in Table 1, which generally relates to whether the surgical environment is set up in a usable manner or non-usable manner, may include transport sensor <b>222</b>. Transport sensor <b>222</b> may be a sensor which detects when the surgical robotic system is transitioned to a transportation configuration or mode (e.g., console <b>102</b> can be moved), by sensing, for example, that the brakes are released. If a transition to a transportation configuration is detected, the transportation interlock is no longer met, and the surgical environment is considered to be a non-usable configuration and teleoperation mode should be disengaged. If, on the other hand, the transport sensor <b>222</b> determines the system is not in a transportation configuration (e.g., the brakes are engaged), the surgical environment is in a usable configuration, and teleoperation can be engaged.</p><p>[0044] Additional interlock detecting mechanisms may include the use of the eye tracking aspects previously discussed for determining interpupillary distance (IPD) for the immersive display <b>202</b>. For example, given the position of the center of the eyeballs from the eye tracker, the IPD can be calculated for the immersive display. This may remove a requirement for the user to manually adjust the immersive display IPD.</p><p>[0045] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment. Representatively, process <b>300</b> includes the operations of receiving a plurality of interlock inputs (block <b>302</b>). The plurality of interlock inputs may be one or more of the interlocks 1-11 received from the interlock detecting mechanisms previously discussed in reference to FIG. 2 and Table 1. Based on the plurality of interlock inputs received, the system then determines whether a user is looking toward or away from the display (block <b>304</b>). As previously discussed, the interlocks indicating that the user is looking toward the display may be interlocks 1-7 listed in Table 1. If any one of interlocks 1-7 are not met or satisfied, in other words at least one of conditions 1-7 listed in Table 2 is detected, it is determined that the user is not looking toward the display (e.g., looking away) and teleoperation mode is not engaged (block <b>306</b>). On the other hand, if all of interlocks 1-7 listed in Table <b>1</b> are met or satisfied, it is determined at operation <b>304</b> that the user is looking toward the display and process <b>300</b> continues to operation <b>308</b>.</p><p>[0046] At operation <b>308</b>, the system determines whether a user interface device is positioned in a usable or non-usable manner. A user interface device is considered positioned in a usable manner when it is in a position suitable for a user to control a surgical instrument in a teleoperation mode. The user interface device may be considered to be in a non-usable manner when it is not in a position suitable for the user to control the surgical instrument. Representatively, the user interface device is considered positioned in a usable manner if all of the interlocks 8-10 listed in Table 1 are met or satisfied, and the process <b>300</b> continues to operation <b>310</b>. For example, if UID <b>114</b> is inside a trackable workspace, the tracking workspace is not distorted and the UID <b>114</b> is being held by the user, the UID <b>114</b> is positioned in a usable manner. On the other hand, if it is determined at operation <b>308</b> that one of conditions 8-10 listed in Table 2 are detected, namely the UID is outside of the trackable workspace and the information received from the tracker is not accurate, the tracking space is distorted and the signal received from the tracker does not give an accurate reading, or the UID is not being held by a user (e.g., the UID is stored or dropped), the system determines the user interface device is not positioned in a usable manner, or is positioned in a non-usable manner, and does not engage teleoperation mode.</p><p>[0047] In operation <b>310</b>, the system determines whether the surgical workspace is configured in a usable manner or a non-usable manner. The surgical workspace (e.g., the user console, robotic arms, surgical table, etc) may be considered to be positioned or configured in a usable manner when components of the surgical workspace are properly configured for a user to control a surgical instrument in a teleoperation mode. For example, the surgical workspace is considered to be configured in a usable manner if the interlock 11 listed in Table 1 is met or satisfied. For example, if the user console, chair, display or other associated component is in a fixed configuration (e.g., not in a transportation configuration in which the brakes are released), the surgical workspace is considered to be properly configured. If the surgical workspace is configured in a usable manner, the process continues to operation <b>312</b> and teleoperation mode is engaged. If, on the other hand, it is determined at operation <b>310</b> that the system is transitioned to a transportation configuration (e.g., the brakes are released), in other words condition 11 of Table 2 is detected, the surgical workspace is configured in a non-usable manner for teleoperation mode, and the process proceeds to operation <b>306</b> and teleoperation mode is not engaged.</p><p>[0048] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment. Representatively, similarly to process <b>300</b>, process <b>400</b> includes the operation of receiving a plurality of interlock inputs (block <b>402</b>), for example, interlocks 1-11 previously discussed in reference to Table 1. When all of the interlock inputs required for teleoperation mode are received (e.g., the user is looking toward or otherwise focused on the display), a teleoperation mode is engaged at operation <b>404</b>. Process <b>400</b> then continues to monitor whether all of the interlock inputs are still present at operation <b>406</b>. As long as the interlocks continue to be detected, teleoperation mode will continue (block <b>410</b>). If, on the other hand, it is determined at operation <b>406</b> that one or more of the interlock inputs is no longer detected, in other words one or more of conditions 1-11 listed in Table 2 are present, the system transitions out of the teleoperation mode at operation <b>408</b>, for example to a non-teleoperation mode in which the user interface device is prevented from controlling a surgical instrument.</p><p>[0049] FIG. 5 is a block diagram of a computer portion of a surgical robotic system, which is operable to implement the previously discussed operations, in accordance with an embodiment. The exemplary surgical robotic system <b>500</b> may include a user console <b>102</b>, a surgical robot <b>120</b>, and a control tower <b>103</b>. The surgical robotic system <b>500</b> may include other or additional hardware components; thus, the diagram is provided by way of example and not a limitation to the system architecture.</p><p>[0050] As described above, the user console <b>102</b> may include console computers <b>511</b>, one or more UIDs <b>512</b>, console actuators <b>513</b>, displays <b>514</b>, foot pedals <b>516</b>, console computers <b>511</b> and a network interface <b>518</b>. In addition, user console <b>102</b> may include a number of interlock detecting mechanisms, devices, or components, for example, a UID tracker(s) <b>515</b>, a display tracker(s) <b>517</b> and a console tracker(s) <b>519</b>, for detecting whether the previously discussed interlocks required for teleoperation are satisfied. For example, UID tracker(s) <b>515</b> may include the previously discussed UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. Display tracker(s) <b>517</b> may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, and/or an immersive display deployment sensor <b>214</b>. Console tracker(s) may include transport sensor <b>222</b>. The interlock information detected and/or tracked by any one or more of the trackers may be monitored and communicated to the console computer <b>511</b>, and dispatched to the control tower <b>103</b> via the network interface <b>518</b>, so that the system can determine whether to engage and/or disengage a teleoperation mode.</p><p>[0051] It should further be understood that a user or surgeon sitting at the user console <b>102</b> can adjust ergonomic settings of the user console <b>102</b> manually, or the settings can be automatically adjusted according to user profile or preference. The manual and automatic adjustments may be achieved through driving the console actuators <b>513</b> based on user input or stored configurations by the console computers <b>511</b>. The user may perform robot-assisted surgeries by controlling the surgical robot <b>120</b> using one or more master UIDs <b>512</b> and foot pedals <b>516</b>. Positions and orientations of the UIDs <b>512</b> are continuously tracked by the UID tracker <b>515</b>, and status changes are recorded by the console computers <b>511</b> as user input and dispatched to the control tower <b>103</b> via the network interface <b>518</b>. Real-time surgical video of patient anatomy, instrumentation, and relevant software apps can be presented to the user on the high resolution 3D displays <b>514</b> including open or immersive displays.</p><p>[0052] The user console <b>102</b> may be communicatively coupled to the control tower <b>103</b>. The user console also provides additional features for improved ergonomics. For example, the user console may be an open architecture system including an open display, although an immersive display, in some cases, may be provided. Furthermore, a highly-adjustable seat for surgeons and master UIDs tracked through electromagnetic or optical trackers are included at the user console <b>102</b> for improved ergonomics.</p><p>[0053] The control tower <b>103</b> can be a mobile point-of-care cart housing touchscreen displays, computers that control the surgeon's robotically-assisted manipulation of instruments, safety systems, graphical user interface (GUI), light source, and video and graphics computers. As shown in FIG. 5, the control tower <b>103</b> may include central computers <b>531</b> including at least a visualization computer, a control computer, and an auxiliary computer, various displays <b>533</b> including a team display and a nurse display, and a network interface <b>518</b> coupling the control tower <b>103</b> to both the user console <b>102</b> and the surgical robot <b>120</b>. The control tower <b>103</b> may offer additional features for user convenience, such as the nurse display touchscreen, soft power and E-hold buttons, user-facing USB for video and still images, and electronic caster control interface. The auxiliary computer may also run a real-time Linux, providing logging/monitoring and interacting with cloud-based web services.</p><p>[0054] The surgical robot <b>120</b> may include an articulated operating table <b>524</b> with a plurality of integrated arms <b>522</b> that can be positioned over the target patient anatomy. A suite of compatible tools <b>523</b> can be attached to or detached from the distal ends of the arms <b>522</b>, enabling the surgeon to perform various surgical procedures. The surgical robot <b>120</b> may also comprise control interface <b>525</b> for manual control of the arms <b>522</b>, table <b>524</b>, and tools <b>523</b>. The control interface can include items such as, but not limited to, remote controls, buttons, panels, and touchscreens. Other accessories such as trocars (sleeves, seal cartridge, and obturators) and drapes may also be needed to perform procedures with the system. In some variations, the plurality of the arms <b>522</b> includes four arms mounted on both sides of the operating table <b>524</b>, with two arms on each side. For certain surgical procedures, an arm mounted on one side of the table can be positioned on the other side of the table by stretching out and crossing over under the table and arms mounted on the other side, resulting in a total of three arms positioned on the same side of the table <b>524</b>. The surgical tool can also comprise table computers <b>521</b> and a network interface <b>518</b>, which can place the surgical robot <b>120</b> in communication with the control tower <b>103</b>.</p><p>[0055] In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A method for engaging and disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system, wherein the surgical robotic system comprises a console having a display, at least one or more user interface devices for controlling the surgical instrument, and a chair, and wherein at least one of the one or more interlock detection components comprises a transport sensor coupled to the console that detects whether the console is in a transportation configuration as one of the plurality of interlock inputs;<BR />determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner based on whether the console is in a transportation configuration;<BR />in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and<BR />in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of claim 1, wherein the one or more user interface devices comprise at least one of a handheld user input device and a foot pedal.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>3. The method of claim 1 wherein the display is an open display.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>4. The method of claim 1 wherein the one or more interlock detection components further comprise at least one of an eye and/or head tracker, a tracking marker on a user&apos;s glasses, a tracking marker on a user&apos;s face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, ora user interface device contact sensor.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>5. The method of claim 1 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward the display;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair associated of the surgical robotic system is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>6. The method of claim 1 wherein the plurality of interlock inputs indicating at least one or more user interface devices of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside the surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>7. The method of claim 1 wherein at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner comprises: <BR />detecting the console is not in a transportation configuration.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"yes\\\"><p>8. A method for disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner, and wherein the surgical robotic system comprises a console having a display, a user interface device for controlling the surgical instrument, and a chair, and wherein at least one of the one or more interlock detection components comprises a transport sensor coupled to the console for detecting engagement of a brake associated with the console, and the at least one previously received interlock input that is no longer present is engagement of the brake when the surgical workspace is configured in a non-usable manner; and<BR />in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that the user interface device of the surgical robotic system is prevented from controlling a surgical instrument.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p>9. The method of claim 8 wherein determining the user is looking away from the display comprises at least one of the following: <BR />detecting a user gaze is outside of the display;<BR />detecting a user head is rotated away from the display;<BR />detecting a chair associated of the surgical robotic system is facing away from the display;<BR />detecting a user is not seated on the chair; and<BR />detecting the display is in a non-use position.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>10. The method of claim 9 wherein determining the surgical robotic system is configured in a non-usable manner comprises at least one of the following: <BR />detecting at least one user interface device is outside a surgical workspace;<BR />detecting a received user interface device location is inaccurate; and<BR />detecting at least one user interface device is dropped by the user.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"yes\\\"><p>11. A surgical robotic system, comprising: <BR />a surgical instrument; <BR />a user console comprising a display, an interlock detecting component, a user interface device and a chair; and<BR />one or more processors communicatively coupled to the interlock detecting component, the one or more processors configured to: <BR />receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device, and at least one of the plurality of interlock inputs comprises engagement of a brake associated with the user console;<BR />determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and<BR />transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p>12. The system of claim 11 wherein the display is an open display.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"11\\\"><p>13. The system of claim 11 wherein the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>14. The system of claim 13 wherein the interlock detecting component for detecting the plurality of interlocks inputs indicating a user is looking toward a display comprise:<BR />at least one of an eye and/or a head tracker, a tracking marker on a user&apos;s glasses, a tracking marker on a user&apos;s face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>15. The system of claim 13 wherein the interlock detecting components for detecting the plurality of interlock inputs indicating at least one or more user interface devices are configured in a usable manner comprise: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>16. The system of claim 13 wherein the interlock detecting component for detecting the interlock indicating the surgical workspace is configured in a usable manner comprises a transport sensor.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>17. The system of claim 13 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward the display;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair associated of the surgical robotic system is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>18. The system of claim 13 wherein the plurality of interlock inputs indicating at least one or more user interface devices of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside a surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"13\\\"><p>19. The system of claim 13 wherein at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner comprises: <BR />detecting the surgical robotic system is not in a transportation configuration.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/00\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/00\",\n",
      "\n",
      "\"B25J9/16\",\n",
      "\n",
      "\"G06F3/01\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2017/00212\",\n",
      "\n",
      "\"A61B2017/00216\",\n",
      "\n",
      "\"A61B2034/305\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"A61B34/37\",\n",
      "\n",
      "\"A61B34/74\",\n",
      "\n",
      "\"A61B90/03\",\n",
      "\n",
      "\"B25J9/0009\",\n",
      "\n",
      "\"B25J9/1689\",\n",
      "\n",
      "\"G06F3/012\",\n",
      "\n",
      "\"G06F3/013\",\n",
      "\n",
      "\"G06F3/017\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"VERB SURGICAL INC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190517,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US11337767\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20220524,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US11337767\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20400726,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry (PTA 436 days)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82037919\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20220725351\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"INTERLOCK MECHANISMS TO DISENGAGE AND ENGAGE A TELEOPERATION MODE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method for engaging and disengaging a surgical instrument of a surgical robotic system comprising: receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATIONS</p><p>[0001] This application is a continuation of co-pending U.S. application Ser. No. 16/415,992 filed May 17, 2019, which is incorporated by reference herein.</p></relapp><shortsum><p>FIELD</p><p>[0002] Embodiments related surgical robotic systems, are disclosed. More particularly, embodiments related to interlock mechanisms that disengage and engage a teleoperation mode, are disclosed.</p><p>BACKGROUND</p><p>[0003] Endoscopic surgery involves looking into a patient's body and performing surgery inside the body using endoscopes and other surgical tools. For example, laparoscopic surgery can use a laparoscope to access and view an abdominal cavity. Endoscopic surgery can be performed using manual tools and/or a surgical robotic system having robotically-assisted tools.</p><p>[0004] A surgical robotic system may be remotely operated by a surgeon to command a robotically-assisted tool located at an operating table. Such operation of a robotically-assisted tool remotely by a surgeon may be commonly referred to as teleoperation. For example, the surgeon may use a computer console located in the operating room, or it may be located in a different city, to command a robot to manipulate the surgical tool mounted on the operating table. The robotically-controlled surgical tool can be an endoscope mounted on a robotic arm. Accordingly, the surgical robotic system may be used by the remote surgeon to perform an endoscopic surgery.</p><p>[0005] The surgeon may provide input commands to the surgical robotic system, and one or more processors of the surgical robotic system can control system components in response to the input commands. For example, the surgeon may hold in her hand a user input device such as a joystick or a computer mouse that she manipulates to generate control signals to cause motion of the surgical robotic system components, e.g., an actuator, a robotic arm, and/or a surgical tool of the robotic system.</p><p>SUMMARY</p><p>[0006] During teleoperation with an open display in which the user can view their surroundings (as compared to a periscope type display) there is the possibility that the surgeon is looking away from the screen but still holding the user input devices that control the robotic tools. This introduces a risk since the surgeon could move the user input devices and unintentionally move the tools while not focusing on the screen. Therefore, in some aspects, the processes disclosed herein provide methods for determining whether a number of interlocks required for teleoperation are met, and therefore the user is looking at the open display (or an immersive display such as a periscope) and focused on teleoperation, such that teleoperation mode may be engaged or continue. In this aspect, the system detects a number of interlock parameters, conditions, inputs or the like, and then determines based on the detection of the interlock parameters, whether the system should disengage or engage teleoperation. In general, the interlock mechanisms or parameters are designed to determine the following (1) is the user looking at the screen, (2) is the user holding the user interface device in a usable manner, and (3) is the environment set up for teleoperation mode. If the answer to all of these conditions is yes, than teleoperation mode is engaged, if the answer to any of them is no, teleoperation mode may be disengaged.</p><p>[0007] Representatively, in one aspect, a method for engaging and disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument. The user interface devices may include at least one of a handheld user input device and a foot pedal. The display may be an open display. The one or more interlock detection components may include at least one of an eye and/or head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display may include detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner may include detecting at least one user interface device is inside the surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some cases, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner may include detecting the surgical robotic system is not in a transportation configuration.</p><p>[0008] In another aspect, a method for disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument. In some aspects, determining the user is looking away from the display may include at least one of the following: detecting a user gaze is outside of the display; detecting a user head is rotated away from the display; detecting a chair associated of the surgical robotic system is facing away from the display; detecting a user is not seated on the chair; and detecting the display is in a non-use position. In some aspects, determining the surgical robotic system is configured in a non-usable manner may include at least one of the following: detecting at least one user interface device is outside a surgical workspace; detecting a received user interface device location is inaccurate; and detecting at least one user interface device is dropped by the user. In some aspects, determining the surgical workspace of the surgical robotic system is configured in a non-usable manner may include detecting the surgical robotic system is in a transportation configuration.</p><p>[0009] In still further aspects, a surgical robotic system is disclosed. The system may include a surgical instrument; a user console comprising a display, an interlock detecting component, and a user interface device; and one or more processors communicatively coupled to the interlock detecting component, the processors configured to: receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device; determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument. In some aspects, the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner. The interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display may include: least one of an eye and/or a head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor. The interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner may include: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor. The interlock detecting component for detecting the interlock indicating the surgical workspace may be configured in a usable manner comprises a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display comprise: detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: detecting at least one user interface device is inside a surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some aspects, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner includes detecting the surgical robotic system is not in a transportation configuration.</p><p>[0010] The above summary does not include an exhaustive list of all aspects of the present invention. It is contemplated that the invention includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0011] The embodiments of the invention are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \\\"an\\\" or \\\"one\\\" embodiment of the invention in this disclosure are not necessarily to the same embodiment, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one embodiment of the invention, and not all elements in the figure may be required for a given embodiment.</p><p>[0012] FIG. <b>1</b> is a pictorial view of an example surgical robotic system in an operating arena, in accordance with an embodiment.</p><p>[0013] FIG. <b>2</b> is a pictorial view of a user console, in accordance with an embodiment.</p><p>[0014] FIG. <b>3</b> is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment.</p><p>[0015] FIG. <b>4</b> is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment.</p><p>[0016] FIG. <b>5</b> is a block diagram of a computer portion of a surgical robotic system, in accordance with an embodiment.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0017] In various embodiments, description is made with reference to the figures. However, certain embodiments may be practiced without one or more of these specific details, or in combination with other known methods and configurations. In the following description, numerous specific details are set forth, such as specific configurations, dimensions, and processes, in order to provide a thorough understanding of the embodiments. In other instances, well-known processes and manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the description. Reference throughout this specification to \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, means that a particular feature, structure, configuration, or characteristic described is included in at least one embodiment. Thus, the appearance of the phrase \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p>[0018] In addition, the terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the invention. Spatially relative terms, such as \\\"beneath\\\", \\\"below\\\", \\\"lower\\\", \\\"above\\\", \\\"upper\\\", and the like may be used herein for ease of description to describe one element's or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as \\\"below\\\" or \\\"beneath\\\" other elements or features would then be oriented \\\"above\\\" the other elements or features. Thus, the exemplary term \\\"below\\\" can encompass both an orientation of above and below. The device may be otherwise oriented (e.g., rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.</p><p>[0019] As used herein, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" are intended to include the plural forms as well, unless the context indicates otherwise. It will be further understood that the terms \\\"comprises\\\" and/or \\\"comprising\\\" specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.</p><p>[0020] The terms \\\"or\\\" and \\\"and/or\\\" as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, \\\"A, B or C\\\" or \\\"A, B and/or C\\\" mean \\\"any of the following: A; B; C; A and B; A and C; B and C; A, B and C.\\\" An exception to this definition will occur only when a combination of elements, functions, steps or acts are in some way inherently mutually exclusive.</p><p>[0021] Moreover, the use of relative terms throughout the description may denote a relative position or direction. For example, \\\"distal\\\" may indicate a first direction away from a reference point, e.g., away from a user. Similarly, \\\"proximal\\\" may indicate a location in a second direction opposite to the first direction, e.g., toward the user. Such terms are provided to establish relative frames of reference, however, and are not intended to limit the use or orientation of any particular surgical robotic component to a specific configuration described in the various embodiments below.</p><p>[0022] Referring to FIG. <b>1</b>, this is a pictorial view of an example surgical robotic system <b>100</b> in an operating arena. The surgical robotic system <b>100</b> includes a user console <b>102</b>, a control tower <b>103</b>, and one or more surgical robots <b>120</b>, including robotic arms <b>104</b> at a surgical robotic platform <b>105</b>, e.g., an operating table, a bed, etc. The system <b>100</b> can incorporate any number of devices, tools, or accessories used to perform surgery on a patient <b>106</b>. For example, the system <b>100</b> may include one or more surgical tools <b>107</b> used to perform surgery. A surgical tool <b>107</b> may be an end effector that is attached to a distal end of a surgical arm <b>104</b>, for executing a surgical procedure.</p><p>[0023] Each surgical tool <b>107</b> may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool <b>107</b> may be a tool used to enter, view, or manipulate an internal anatomy of the patient <b>106</b>. In an embodiment, the surgical tool <b>107</b> is a grasper that can grasp tissue of the patient. The surgical tool <b>107</b> may be controlled manually, by a bedside operator <b>108</b>; or it may be controlled robotically, via actuated movement of the surgical robotic arm <b>104</b> to which it is attached. The robotic arms <b>104</b> are shown as a table-mounted system, but in other configurations the arms <b>104</b> may be mounted in a cart, ceiling or sidewall, or in another suitable structural support.</p><p>[0024] Generally, a remote operator <b>109</b>, such as a surgeon or other operator, may use the user console <b>102</b> to remotely manipulate the arms <b>104</b> and/or the attached surgical tools <b>107</b>, e.g., teleoperation. The user console <b>102</b> may be located in the same operating room as the rest of the system <b>100</b>, as shown in FIG. <b>1</b>. In other environments however, the user console <b>102</b> may be located in an adjacent or nearby room, or it may be at a remote location, e.g., in a different building, city, or country. The user console <b>102</b> may comprise a seat <b>110</b>, one or more user interface devices, for example, foot-operated controls <b>113</b> or handheld user input devices (UID) <b>114</b>, and at least one user display <b>115</b> that is configured to display, for example, a view of the surgical site inside the patient <b>106</b>. In the example user console <b>102</b>, the remote operator <b>109</b> is sitting in the seat <b>110</b> and viewing the user display <b>115</b> while manipulating a foot-operated control <b>113</b> and a handheld UID <b>114</b> in order to remotely control the arms <b>104</b> and the surgical tools <b>107</b> (that are mounted on the distal ends of the arms <b>104</b>).</p><p>[0025] In some variations, the bedside operator <b>108</b> may also operate the system <b>100</b> in an \\\"over the bed\\\" mode, in which the bedside operator <b>108</b> (user) is now at a side of the patient <b>106</b> and is simultaneously manipulating a robotically-driven tool (end effector as attached to the arm <b>104</b>), e.g., with a handheld UID <b>114</b> held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator <b>108</b> may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient <b>106</b>.</p><p>[0026] During an example procedure (surgery), the patient <b>106</b> is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system <b>100</b> are in a stowed configuration or withdrawn configuration (to facilitate access to the surgical site.) Once access is completed, initial positioning or preparation of the robotic system <b>100</b> including its arms <b>104</b> may be performed. Next, the surgery proceeds with the remote operator <b>109</b> at the user console <b>102</b> utilizing the foot-operated controls <b>113</b> and the UIDs <b>114</b> to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, e.g., the bedside operator <b>108</b> who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms <b>104</b>. Non-sterile personnel may also be present to assist the remote operator <b>109</b> at the user console <b>102</b>. When the procedure or surgery is completed, the system <b>100</b> and the user console <b>102</b> may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilisation and healthcare record entry or printout via the user console <b>102</b>.</p><p>[0027] In one embodiment, the remote operator <b>109</b> holds and moves the UID <b>114</b> to provide an input command to move a robot arm actuator <b>117</b> in the robotic system <b>100</b>. The UID <b>114</b> may be communicatively coupled to the rest of the robotic system <b>100</b>, e.g., via a console computer system <b>116</b>. Representatively, in some embodiments, UID <b>114</b> may be a portable handheld user input device or controller that is ungrounded with respect to another component of the surgical robotic system. For example, UID <b>114</b> may be ungrounded while either tethered or untethered from the user console. The term \\\"ungrounded\\\" is intended to refer to implementations where, for example, both UIDs are neither mechanically nor kinematically constrained with respect to the user console. For example, a user may hold a UID <b>114</b> in a hand and move freely to any possible position and orientation within space only limited by, for example, a tracking mechanism of the user console. The UID <b>114</b> can generate spatial state signals corresponding to movement of the UID <b>114</b>, e.g. position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator <b>117</b>. The robotic system <b>100</b> may use control signals derived from the spatial state signals, to control proportional motion of the actuator <b>117</b>. In one embodiment, a console processor of the console computer system <b>116</b> receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator <b>117</b> is energized to move a segment or link of the arm <b>104</b>, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID <b>114</b>. Similarly, interaction between the remote operator <b>109</b> and the UID <b>114</b> can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool <b>107</b> to close and grip the tissue of patient <b>106</b>.</p><p>[0028] The surgical robotic system <b>100</b> may include several UIDs <b>114</b>, where respective control signals are generated for each UID that control the actuators and the surgical tool (end effector) of a respective arm <b>104</b>. For example, the remote operator <b>109</b> may move a first UID <b>114</b> to control the motion of an actuator <b>117</b> that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc., in that arm <b>104</b>. Similarly, movement of a second UID <b>114</b> by the remote operator <b>109</b> controls the motion of another actuator <b>117</b>, which in turn moves other linkages, gears, etc., of the robotic system <b>100</b>. The robotic system <b>100</b> may include a right arm <b>104</b> that is secured to the bed or table to the right side of the patient, and a left arm <b>104</b> that is at the left side of the patient. An actuator <b>117</b> may include one or more motors that are controlled so that they drive the rotation of a joint of the arm <b>104</b>, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool <b>107</b> that is attached to that arm. Motion of several actuators <b>117</b> in the same arm <b>104</b> can be controlled by the spatial state signals generated from a particular UID <b>114</b>. The UIDs <b>114</b> can also control motion of respective surgical tool graspers. For example, each UID <b>114</b> can generate a respective grip signal to control motion of an actuator, e.g., a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool <b>107</b> to grip tissue within patient <b>106</b>.</p><p>[0029] In some aspects, the communication between the platform <b>105</b> and the user console <b>102</b> may be through a control tower <b>103</b>, which may translate user commands that are received from the user console <b>102</b> (and more particularly from the console computer system <b>116</b>) into robotic control commands that are transmitted to the arms <b>104</b> on the robotic platform <b>105</b>. The control tower <b>103</b> may also transmit status and feedback from the platform <b>105</b> back to the user console <b>102</b>. The communication connections between the robotic platform <b>105</b>, the user console <b>102</b>, and the control tower <b>103</b> may be via wired and/or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor and/or walls or ceiling of the operating room. The robotic system <b>100</b> may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. It will be appreciated that the operating room scene in FIG. <b>1</b> is illustrative and may not accurately represent certain medical practices.</p><p>[0030] In addition, in some aspects, surgical robotic system <b>100</b> may further include one or more interlock detecting mechanisms, devices, components or systems (e.g., trackers, sensors, etc), that can be used to determine whether a teleoperation mode should be engaged or, once engaged, should be disengaged. The interlock detecting mechanisms may detect, for example, whether a user is looking toward or away from the display <b>115</b>, the UID is configured in a usable manner and/or the surgical environment is set up in a usable manner, and in turn, whether a teleoperation mode should be engaged or disengaged. For example, during a teleoperation mode of the surgical robotic system <b>100</b>, in which the user is controlling the surgical tool <b>107</b> using the UID <b>114</b>, the user should be viewing the tool movement on display <b>115</b>. In some cases, however, the user may look away from the display <b>115</b> (intentionally or unintentionally) while still holding the UID <b>114</b> and controlling the surgical tool <b>107</b>. This introduces a risk since the user could move the UID <b>114</b> and, in turn, unintentionally move the tool <b>107</b> while not focusing on the display <b>115</b>. Surgical robotic system <b>100</b> may therefore further include interlock detecting mechanisms that can be used to determine whether the user is focused on the display <b>115</b>, or otherwise interlocked or engaged with the system, in such a way that teleoperation mode is appropriate. The various interlock detecting mechanisms may be coupled to, in communication with, or otherwise associated with components of, the user console <b>102</b>, and will be described in more detail in reference to FIG. <b>2</b>.</p><p>[0031] It should be understood that \\\"engaging\\\" the teleoperation mode is intended to refer to an operation in which, for example, a UID or foot pedal that is prevented from controlling the surgical instrument, is transitioned to a mode (e.g., a teleoperation mode) in which it can now control the surgical instrument. On the other hand, disengaging the teleoperation mode is intended to refer to an operation which occurs when the system is in a teleoperation mode, and then transitioned to a mode (non-teleoperation mode) in which the UID or foot pedal can no longer control the surgical instrument. For example, teleoperation mode may be disengaged when the system determines that one or more of a number of interlocks required for teleoperation mode is no longer present or met. The interlocks required for teleoperation mode may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner.</p><p>[0032] Referring now to FIG. <b>2</b>, FIG. <b>2</b> illustrates a pictorial view of an exemplary user console including a number of devices, mechanisms, systems and/or components used to detect whether the interlocks required for teleoperation mode are met. Representatively, user console <b>102</b> is shown including a chair <b>110</b> and display <b>115</b>, as previously discussed. In addition, user console <b>102</b> is shown having an optional display <b>202</b> mounted to chair <b>110</b> by a display arm <b>204</b>. For example, display <b>115</b> may be an open display, and display <b>202</b> may be an immersive display. The term \\\"open\\\" display is intended to refer to a display which is designed to allow the user to see outside of the display, for example with their peripheral vision, even when directly facing the display. In addition, in an open display, the user can be a distance from the display screen or not directly in front of the display, and still view a surgical procedure on the display. Therefore, in an open display such as display <b>115</b>, the fact that the user may not be close to the display or have their face directly in front of the display, would not necessarily mean the user is not looking at the display or focused on the surgical procedure. In the case of an open display, in which the user can turn their head and still see the display using their peripheral vision, it is therefore important that turning of the head slightly not be interpreted to mean the user is not focused on the display. Rather, in the case of an open display, there will be some tolerance to such actions and allow for teleoperation mode to continue. This is in contrast to an immersive display such as display <b>202</b>. For example, display <b>202</b> could be a completely immersive display (as in the case of a periscope) that prevents the user from seeing outside of the display screen when they are facing the display and requires the user to be relatively close to the display screen. For example, in the case of an immersive display, the user must have their face relatively close to, and facing, the display screen to use it to view the surgical procedure in progress. If the user pulls their head away from the display screen, or doesn't face the display screen, they can no longer see the display screen, therefore this would typically be interpreted to mean the user is not focused on the screen or not paying sufficient attention to continue in teleoperation mode.</p><p>[0033] To detect whether all the interlocks required for engagement of the teleoperation mode (or continuing in the teleoperation mode) are met, user console <b>102</b> may further include a number of interlock detecting devices, mechanisms, systems and/or components. The interlock detecting mechanisms may include, but are not limited to, an eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, an immersive display deployment sensor <b>214</b>, a UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, UID contact sensor <b>220</b>, and/or a transport sensor <b>222</b>. Each of the interlock detecting mechanisms are configured to detect at least one of a number of conditions (referred to herein as \\\"interlocks\\\") which are required to engage the teleoperation mode. As previously discussed, the interlocks may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner. Table 1 provides an exemplary listing of the specific interlocks which are required to engage a teleoperation mode, and to continue teleoperation mode, once engaged.</p><p>[0034] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Interlock</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze on the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated toward</TD></TR><TR><TD>the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated</TD></TR><TR><TD>toward the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is resting on the headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a</TD></TR><TR><TD>use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is inside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace not distorted and</TD></TR><TR><TD>tracker signal is accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is being held by the user</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is not in a transportation mode</TD></TR><TR></TR></Table></p><p>[0035] If any of interlocks 1-11 are not detected, a teleoperation mode cannot be engaged, or if already engaged, may be automatically disengaged. Said another way, if any one or more of the following conditions listed in Table 2 are determined (e.g., when any of interlocks 1-11 are not detected) teleoperation mode is either not engaged or disengaged.</p><p>[0036] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 2</TD></TR><TR></TR><TR><TD>Condition</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze is outside of the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated away</TD></TR><TR><TD>from the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated</TD></TR><TR><TD>away from the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing away from the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is not seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is not resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a</TD></TR><TR><TD>non-use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is outside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace distorted and</TD></TR><TR><TD>tracker signal is not accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is not being held by the user, or is in</TD></TR><TR><TD>a stored configuration</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is in a transportation mode</TD></TR><TR></TR></Table></p><p>[0037] The interlock detecting mechanisms for detecting, for example, interlocks 1-7 listed in Table 1, which generally relate to whether the user is looking at or focused on the display, may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, chair swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, headrest pressure sensor <b>212</b>, and immersive display deployment sensor <b>214</b>. The interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether the user is holding the controllers in a usable manner, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b> and UID contact sensor <b>220</b>. The interlock detecting mechanisms for detecting interlock 11, which relates to whether the surgical environment is set up in a usable manner, may include transport sensor <b>222</b>, in addition to one or more of the UID sensors <b>216</b>, <b>218</b>, <b>220</b>. Once a condition or interlock is detected by the interlock detecting mechanism, the information is output to a processing component of the surgical robotic system, where it is received as an interlock input and monitored to determine whether teleoperation should continue, should be engaged or should be disengaged. For example, if interlock inputs corresponding to all the required interlocks for teleoperation mode are received, teleoperation mode is engaged. If any interlocks are missing, teleoperation mode is not engaged (if currently in a non-teleoperation mode), or disengaged (if currently in teleoperation mode).</p><p>[0038] Referring now in more detail to each of the interlock detecting mechanisms, eye and/or head tracker <b>206</b> may be a tracking device that is coupled to the display <b>115</b> to track a user's gaze and/or head with respect to the display <b>115</b>, and indicate whether or not the user is looking toward or away from the display. Eye and/or head tracker <b>206</b> may be attached to any portion of display <b>115</b> suitable for tracking a gaze of a user. For example, eye and/or head tracker <b>206</b> may be attached to a housing <b>224</b> of display <b>115</b>, for example, a top wall, a bottom wall, or a side wall of housing <b>224</b>, or integrated within a screen (not shown) mounted within the housing <b>224</b> of display <b>115</b>. Eye and/or head tracker <b>206</b> may include one or more projector(s) and camera(s) which face the user and can be used to track a gaze of a user. Representatively, the projector(s) may create a pattern of near-infrared light on the eyes of the user, and the camera(s) may take images of the user's eyes and the pattern. Eye and/or head tracker <b>206</b> may further be programmed to use this information (e.g., execute machine learning, image processing and/or mathematical algorithms) to determine where the user is looking based on a position of each of the user's eyes and/or gaze point or location relative to one another, and display <b>115</b>. For example, when the user is sitting on chair <b>110</b> in front of display <b>115</b>, eye and/or head tracker <b>206</b> may be configured to create a pattern of near-infrared light on the eyes of the user and take images of the user's eyes and the pattern. This information may, in turn, be input to the surgical robotic system processing component (e.g., an interlock input signal) to determine whether the gaze of the user is inside or outside of display <b>115</b>. If the gaze is determined to be inside the display, the user is determined to be looking toward the display. On the other hand, if the gaze is determined to be outside the display, the user is determined to be looking away from the display.</p><p>[0039] In addition, eye and/or head tracker <b>206</b> can also track a position, orientation and/or location of the user's head as an indication of whether the user is looking at the display. For example, tracker <b>206</b> may detect that the user's head is in front of the display and/or the user's face or nose is facing display <b>115</b>, which suggests the user is looking at the display, or that the user's head is not in front of the display and/or their face or nose is turned away from display <b>115</b>, which suggests the user is looking away from the display. In addition, the tracker <b>206</b> may be used to determine automatically if the open display <b>115</b> or the immersive display <b>202</b> should be used for teleoperation.</p><p>[0040] Still further, tracking markers <b>226</b> on the user's glasses and/or tracking markers <b>228</b> on the user's face mask may be used to determine whether the user is looking toward or away from the display <b>115</b>. The tracking markers <b>226</b> on the user's glasses (e.g., 3D glasses) and/or markers <b>228</b> on the face mask (if user is sterile) may be active or passive markers. The markers <b>226</b> and/or <b>228</b> may track if the user's head is rotated toward the screen, indicating the user is looking toward the display. The markers <b>226</b> and/or <b>228</b> may further track whether the head is rotated away from the screen, indicating the user is looking away from the display. In some cases where the markers <b>226</b> are attached to the user's glasses, they may be similar to the previously discussed eye and/or head tracker and include one or more projectors, cameras and/or processing algorithms that can analyze information received from the camera(s) and projector(s) to determine whether the user is looking at the display.</p><p>[0041] Additional interlock mechanisms for detecting whether or not the user is looking toward or away from the display may include a swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, and a headrest pressure sensor <b>212</b>. The swivel sensor <b>208</b> may be any type of sensing device or component suitable for detecting a rotation (or swivel) and/or orientation of the chair <b>110</b>, with respect to, for example, a stationary base <b>230</b>. For example, the swivel sensor <b>208</b> may include a position encoder, switch or the like. The swivel sensor <b>208</b> may then output a corresponding signal to a processing component of the surgical robotic system corresponding to the rotation, position and/or orientation of the chair <b>110</b>. The surgical robotic system can then use this information to determine whether the chair <b>110</b> is facing display <b>115</b>, and therefore a user seated in the chair is facing the display <b>115</b>. If, on the other hand, swivel sensor <b>208</b> detects the chair is facing away from the display, it is determined the user is looking away from the display. The chair pressure sensor <b>210</b> and the headrest pressure sensor <b>212</b> may be pressure sensors which are attached to the chair headrest and chair seat, respectively. The chair pressure sensor <b>210</b> can be used to detect when the user is seated in the chair <b>110</b>, and the headrest sensor <b>212</b> can be used to detect when the user's head is resting on the headrest. Sensors <b>210</b>, <b>212</b> may output a signal indicating that the user is seated in chair <b>110</b> and/or the head is resting on the headrest to the system processing component, which can be used as indicators that the user is looking at the display <b>115</b>. If the sensors <b>210</b>, <b>212</b> do not detect the user seated in the chair <b>110</b>, this is interpreted as an indication the user is not looking toward the display, or looking away from the display. Sensors <b>210</b> and <b>212</b> may be any type of sensor suitable for detecting the user applying pressure to the seat in the particular location in which the sensor is positioned, for example, they may be pressure plates that are mounted within portions of the headrest and seat which would be contacted by a user seated in the chair <b>110</b>. It should be understood, however, if chair <b>110</b> of console <b>102</b> does not swivel, swivel sensor <b>208</b> may be omitted, and interlock 4 of Table 1 (e.g., chair sensor indicates chair is facing the display) may not be required to engage teleoperation mode.</p><p>[0042] In addition, in embodiments where user console <b>102</b> includes immersive display <b>202</b>, an immersive display deployment sensor <b>214</b> to detect a location of the immersive display <b>202</b> may be provided. The immersive display deployment sensor <b>214</b> may be coupled to the immersive display <b>202</b> and include switches, encoders or the like that are operable to determine if display <b>202</b> is in a \\\"use\\\" position (e.g., position suitable for viewing by a user seated in the chair), \\\"non-use\\\" position (e.g., positioned not suitable for viewing by a user seated in the chair) and/or deployed position (e.g., positioned in front of the chair). This information can be used in conjunction with a sensor (e.g., pressure or light gate sensor) to determine if the user's head is in the immersive display <b>202</b>. A teleoperation mode will only be engaged if the immersive display <b>202</b> is in the \\\"use\\\" configuration and the sensor detects that the user's head is in the immersive display <b>202</b>. If display is determined to be in \\\"non-use\\\" position and/or the user's head is not detected in display <b>202</b>, this is interpreted as an indication the user is looking away from the display. It should be understood, however, if immersive display <b>202</b> is omitted, deployment sensor <b>214</b> may be omitted, and interlock 7 of Table 1 (e.g., immersive display sensor indicates immersive display is in a use position) is not required to engage teleoperation mode.</p><p>[0043] In addition, the interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether a user is holding the controllers (e.g., UID) in a usable or non-usable manner or configuration, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be, for example, an optical tracker, a position tracker and/or an encoder, that is coupled to the user console and/or the UID. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be used to track, for example, the position of the UID, whether the user is holding the UID in a usable or non-usable manner, and/or whether the UID is inside or outside of the trackable workspace. If the UID is outside of the workspace, the UID is not being held in a usable manner, and the system should be disengaged. In addition, if the tracking space is distorted and the signal received from the tracker does not give an accurate reading, the UID is considered not to be in a usable manner or in a non-usable configuration, and teleoperation should be disengaged. Still further, the sensor <b>216</b>, <b>218</b> and/or <b>220</b> can be used to track whether the UID is stored within the user console (e.g., within a holster or other holding mechanism) and/or the UID has been dropped. If the UID is stored within the console and/or has been dropped, such that the user is no longer holding the UID, the UID is considered to be in a non-usable configuration, and teleoperation should be disengaged. In some embodiments, one or more of sensors <b>216</b>, <b>218</b>, <b>220</b> may be an optical tracker and/or an absolute encoders. In some embodiments, a position sensor (optical tracker) is mounted on the user console (e.g., near display <b>115</b>) and/or the base of the UID. In some embodiments, the UID is tracked by a sensor coupled to the console or the user. The position sensor can provide the position of the UID relative to a user and/or a ground reference point (not shown). In some embodiments, more than one (e.g., a plurality, several) tracking technologies are used in conjunction.</p><p>[0044] In addition, the interlock detecting mechanisms for detecting, for example, interlock 11 listed in Table 1, which generally relates to whether the surgical environment is set up in a usable manner or non-usable manner, may include transport sensor <b>222</b>. Transport sensor <b>222</b> may be a sensor which detects when the surgical robotic system is transitioned to a transportation configuration or mode (e.g., console <b>102</b> can be moved), by sensing, for example, that the brakes are released. If a transition to a transportation configuration is detected, the transportation interlock is no longer met, and the surgical environment is considered to be a non-usable configuration and teleoperation mode should be disengaged. If, on the other hand, the transport sensor <b>222</b> determines the system is not in a transportation configuration (e.g., the brakes are engaged), the surgical environment is in a usable configuration, and teleoperation can be engaged.</p><p>[0045] Additional interlock detecting mechanisms may include the use of the eye tracking aspects previously discussed for determining interpupillary distance (IPD) for the immersive display <b>202</b>. For example, given the position of the center of the eyeballs from the eye tracker, the IPD can be calculated for the immersive display. This may remove a requirement for the user to manually adjust the immersive display IPD.</p><p>[0046] FIG. <b>3</b> is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment. Representatively, process <b>300</b> includes the operations of receiving a plurality of interlock inputs (block <b>302</b>). The plurality of interlock inputs may be one or more of the interlocks 1-11 received from the interlock detecting mechanisms previously discussed in reference to FIG. <b>2</b> and Table 1. Based on the plurality of interlock inputs received, the system then determines whether a user is looking toward or away from the display (block <b>304</b>). As previously discussed, the interlocks indicating that the user is looking toward the display may be interlocks 1-7 listed in Table 1. If any one of interlocks 1-7 are not met or satisfied, in other words at least one of conditions 1-7 listed in Table 2 is detected, it is determined that the user is not looking toward the display (e.g., looking away) and teleoperation mode is not engaged (block <b>306</b>). On the other hand, if all of interlocks 1-7 listed in Table 1 are met or satisfied, it is determined at operation <b>304</b> that the user is looking toward the display and process <b>300</b> continues to operation <b>308</b>.</p><p>[0047] At operation <b>308</b>, the system determines whether a user interface device is positioned in a usable or non-usable manner. A user interface device is considered positioned in a usable manner when it is in a position suitable for a user to control a surgical instrument in a teleoperation mode. The user interface device may be considered to be in a non-usable manner when it is not in a position suitable for the user to control the surgical instrument. Representatively, the user interface device is considered positioned in a usable manner if all of the interlocks 8-10 listed in Table 1 are met or satisfied, and the process <b>300</b> continues to operation <b>310</b>. For example, if UID <b>114</b> is inside a trackable workspace, the tracking workspace is not distorted and the UID <b>114</b> is being held by the user, the UID <b>114</b> is positioned in a usable manner. On the other hand, if it is determined at operation <b>308</b> that one of conditions 8-10 listed in Table 2 are detected, namely the UID is outside of the trackable workspace and the information received from the tracker is not accurate, the tracking space is distorted and the signal received from the tracker does not give an accurate reading, or the UID is not being held by a user (e.g., the UID is stored or dropped), the system determines the user interface device is not positioned in a usable manner, or is positioned in a non-usable manner, and does not engage teleoperation mode.</p><p>[0048] In operation <b>310</b>, the system determines whether the surgical workspace is configured in a usable manner or a non-usable manner. The surgical workspace (e.g., the user console, robotic arms, surgical table, etc) may be considered to be positioned or configured in a usable manner when components of the surgical workspace are properly configured for a user to control a surgical instrument in a teleoperation mode. For example, the surgical workspace is considered to be configured in a usable manner if the interlock 11 listed in Table 1 is met or satisfied. For example, if the user console, chair, display or other associated component is in a fixed configuration (e.g., not in a transportation configuration in which the brakes are released), the surgical workspace is considered to be properly configured. If the surgical workspace is configured in a usable manner, the process continues to operation <b>312</b> and teleoperation mode is engaged. If, on the other hand, it is determined at operation <b>310</b> that the system is transitioned to a transportation configuration (e.g., the brakes are released), in other words condition 11 of Table 2 is detected, the surgical workspace is configured in a non-usable manner for teleoperation mode, and the process proceeds to operation <b>306</b> and teleoperation mode is not engaged.</p><p>[0049] FIG. <b>4</b> is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment. Representatively, similarly to process <b>300</b>, process <b>400</b> includes the operation of receiving a plurality of interlock inputs (block <b>402</b>), for example, interlocks 1-11 previously discussed in reference to Table 1. When all of the interlock inputs required for teleoperation mode are received (e.g., the user is looking toward or otherwise focused on the display), a teleoperation mode is engaged at operation <b>404</b>. Process <b>400</b> then continues to monitor whether all of the interlock inputs are still present at operation <b>406</b>. As long as the interlocks continue to be detected, teleoperation mode will continue (block <b>410</b>). If, on the other hand, it is determined at operation <b>406</b> that one or more of the interlock inputs is no longer detected, in other words one or more of conditions 1-11 listed in Table 2 are present, the system transitions out of the teleoperation mode at operation <b>408</b>, for example to a non-teleoperation mode in which the user interface device is prevented from controlling a surgical instrument.</p><p>[0050] FIG. <b>5</b> is a block diagram of a computer portion of a surgical robotic system, which is operable to implement the previously discussed operations, in accordance with an embodiment. The exemplary surgical robotic system <b>500</b> may include a user console <b>102</b>, a surgical robot <b>120</b>, and a control tower <b>103</b>. The surgical robotic system <b>500</b> may include other or additional hardware components; thus, the diagram is provided by way of example and not a limitation to the system architecture.</p><p>[0051] As described above, the user console <b>102</b> may include console computers <b>511</b>, one or more UIDs <b>512</b>, console actuators <b>513</b>, displays <b>514</b>, foot pedals <b>516</b>, console computers <b>511</b> and a network interface <b>518</b>. In addition, user console <b>102</b> may include a number of interlock detecting mechanisms, devices, or components, for example, a UID tracker(s) <b>515</b>, a display tracker(s) <b>517</b> and a console tracker(s) <b>519</b>, for detecting whether the previously discussed interlocks required for teleoperation are satisfied. For example, UID tracker(s) <b>515</b> may include the previously discussed UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. Display tracker(s) <b>517</b> may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, and/or an immersive display deployment sensor <b>214</b>. Console tracker(s) may include transport sensor <b>222</b>. The interlock information detected and/or tracked by any one or more of the trackers may be monitored and communicated to the console computer <b>511</b>, and dispatched to the control tower <b>103</b> via the network interface <b>518</b>, so that the system can determine whether to engage and/or disengage a teleoperation mode.</p><p>[0052] It should further be understood that a user or surgeon sitting at the user console <b>102</b> can adjust ergonomic settings of the user console <b>102</b> manually, or the settings can be automatically adjusted according to user profile or preference. The manual and automatic adjustments may be achieved through driving the console actuators <b>513</b> based on user input or stored configurations by the console computers <b>511</b>. The user may perform robot-assisted surgeries by controlling the surgical robot <b>120</b> using one or more master UIDs <b>512</b> and foot pedals <b>516</b>. Positions and orientations of the UIDs <b>512</b> are continuously tracked by the UID tracker <b>515</b>, and status changes are recorded by the console computers <b>511</b> as user input and dispatched to the control tower <b>103</b> via the network interface <b>518</b>. Real-time surgical video of patient anatomy, instrumentation, and relevant software apps can be presented to the user on the high resolution 3D displays <b>514</b> including open or immersive displays.</p><p>[0053] The user console <b>102</b> may be communicatively coupled to the control tower <b>103</b>. The user console also provides additional features for improved ergonomics. For example, the user console may be an open architecture system including an open display, although an immersive display, in some cases, may be provided. Furthermore, a highly-adjustable seat for surgeons and master UIDs tracked through electromagnetic or optical trackers are included at the user console <b>102</b> for improved ergonomics.</p><p>[0054] The control tower <b>103</b> can be a mobile point-of-care cart housing touchscreen displays, computers that control the surgeon's robotically-assisted manipulation of instruments, safety systems, graphical user interface (GUI), light source, and video and graphics computers. As shown in FIG. <b>5</b>, the control tower <b>103</b> may include central computers <b>531</b> including at least a visualization computer, a control computer, and an auxiliary computer, various displays <b>533</b> including a team display and a nurse display, and a network interface <b>518</b> coupling the control tower <b>103</b> to both the user console <b>102</b> and the surgical robot <b>120</b>. The control tower <b>103</b> may offer additional features for user convenience, such as the nurse display touchscreen, soft power and E-hold buttons, user-facing USB for video and still images, and electronic caster control interface. The auxiliary computer may also run a real-time Linux, providing logging/monitoring and interacting with cloud-based web services.</p><p>[0055] The surgical robot <b>120</b> may include an articulated operating table <b>524</b> with a plurality of integrated arms <b>522</b> that can be positioned over the target patient anatomy. A suite of compatible tools <b>523</b> can be attached to or detached from the distal ends of the arms <b>522</b>, enabling the surgeon to perform various surgical procedures. The surgical robot <b>120</b> may also comprise control interface <b>525</b> for manual control of the arms <b>522</b>, table <b>524</b>, and tools <b>523</b>. The control interface can include items such as, but not limited to, remote controls, buttons, panels, and touchscreens. Other accessories such as trocars (sleeves, seal cartridge, and obturators) and drapes may also be needed to perform procedures with the system. In some variations, the plurality of the arms <b>522</b> includes four arms mounted on both sides of the operating table <b>524</b>, with two arms on each side. For certain surgical procedures, an arm mounted on one side of the table can be positioned on the other side of the table by stretching out and crossing over under the table and arms mounted on the other side, resulting in a total of three arms positioned on the same side of the table <b>524</b>. The surgical tool can also comprise table computers <b>521</b> and a network interface <b>518</b>, which can place the surgical robot <b>120</b> in communication with the control tower <b>103</b>.</p><p>[0056] In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A method for engaging and disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system, and wherein at least one of the one or more interlock detection components comprises a transport sensor for detecting whether the surgical robotic system is in a transportation mode as one of the plurality of interlock inputs;<BR />determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) the transport sensor detects the surgical robotic system is not in the transportation mode; and<BR />in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of claim 1 wherein the transport sensor is coupled to a user console of the surgical robotic system and the transport sensor detects the surgical robotic system is not in the transportation mode when a brake associated with the user console is engaged.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>3. The method of claim 1 wherein in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>4. The method of claim 1, wherein the one or more user interface devices comprise at least one of a handheld user input device and a foot pedal.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>5. The method of claim 1 wherein the display is an open display.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>6. The method of claim 1 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward a display of a console;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair of the console is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>7. The method of claim 1 wherein the plurality of interlock inputs indicating at least one or more user interface devices of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside a surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The method of claim 1 wherein the one or more interlock detection components further comprise an eye tracker, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the eye tracker detecting a user gaze is toward the display.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>9. The method of claim 1 wherein the one or more interlock detection components further comprise a tracking marker on a user&apos;s glasses or a tracking marker on a user&apos;s face mask, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the tracking marker indicating a user&apos;s head is rotated toward the display.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>10. The method of claim 1 wherein the one or more interlock detection components further comprise a chair sensor, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the chair sensor indicating a chair is facing the display, a user is seated in the chair or a user&apos;s head is contacting a headrest of the chair.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>11. The method of claim 1 wherein the one or more interlock detection components further comprise a user interface device sensor, and at least one of the interlock requirements comprises the user interface device sensor indicating the user interface device is inside a surgical workspace, the user interface device is being held by a user or the user interface device is not in a stored position.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"yes\\\"><p>12. A surgical robotic system, comprising: <BR />a surgical instrument;<BR />a user console comprising a display, an interlock detecting component, and a user interface device; and<BR />one or more processors communicatively coupled to the interlock detecting component, the processors configured to:<BR />receive a plurality of interlock inputs from the interlock detecting component and transition the surgical robotic system to a teleoperation mode in which the surgical instrument is controlled by the user interface device; and<BR />in response to determining at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present, transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument, and wherein the at least one of the plurality of interlock inputs no longer present comprises a transport sensor indicating engagement of a brake associated with the user console.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>13. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises an eye tracker indicating a user gaze on the display.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>14. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a tracking marker on a user&apos;s glasses or face mask indicating a head of the user is rotated toward the display.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>15. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a chair sensor indicating a chair is facing the display, a user is seated in the chair or a user&apos;s head is resting on a headrest of the chair.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>16. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a display sensor indicating the display is in a use position.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>17. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a tracker indicating the user interface device is inside a surgical workspace or the user interface device is being held by a user.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>18. The system of claim 12 wherein the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p>19. The system of claim 12 wherein the one or more user interface devices comprise at least one of a handheld user input device and a foot pedal, and the display is an open display.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/00\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/00\",\n",
      "\n",
      "\"B25J9/16\",\n",
      "\n",
      "\"G06F3/01\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2017/00212\",\n",
      "\n",
      "\"A61B2017/00216\",\n",
      "\n",
      "\"A61B2034/305\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"A61B34/37\",\n",
      "\n",
      "\"A61B34/74\",\n",
      "\n",
      "\"A61B90/03\",\n",
      "\n",
      "\"B25J9/0009\",\n",
      "\n",
      "\"B25J9/1689\",\n",
      "\n",
      "\"G06F3/012\",\n",
      "\n",
      "\"G06F3/013\",\n",
      "\n",
      "\"G06F3/017\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"VERB SURGICAL INC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190517,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US11806104\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20231107,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US11806104\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20390517,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry (a terminal disclaimer has been filed)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82037919\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20230470252\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"INTERLOCK MECHANISMS TO DISENGAGE AND ENGAGE A TELEOPERATION MODE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method for engaging and disengaging a surgical instrument of a surgical robotic system comprising: receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATIONS</p><p>[0001] This application is a continuation of U.S. application Ser. No. 17/725,351, filed Apr. 20, 2022, which is a continuation of U.S. application Ser. No. 16/415,992, filed May 17, 2019, now U.S. Pat. No. 11,337,767, issued May 24, 2022, which are incorporated by reference herein.</p></relapp><shortsum><p>FIELD</p><p>[0002] Embodiments related surgical robotic systems, are disclosed. More particularly, embodiments related to interlock mechanisms that disengage and engage a teleoperation mode, are disclosed.</p><p>BACKGROUND</p><p>[0003] Endoscopic surgery involves looking into a patient's body and performing surgery inside the body using endoscopes and other surgical tools. For example, laparoscopic surgery can use a laparoscope to access and view an abdominal cavity. Endoscopic surgery can be performed using manual tools and/or a surgical robotic system having robotically-assisted tools.</p><p>[0004] A surgical robotic system may be remotely operated by a surgeon to command a robotically-assisted tool located at an operating table. Such operation of a robotically-assisted tool remotely by a surgeon may be commonly referred to as teleoperation. For example, the surgeon may use a computer console located in the operating room, or it may be located in a different city, to command a robot to manipulate the surgical tool mounted on the operating table. The robotically-controlled surgical tool can be an endoscope mounted on a robotic arm. Accordingly, the surgical robotic system may be used by the remote surgeon to perform an endoscopic surgery.</p><p>[0005] The surgeon may provide input commands to the surgical robotic system, and one or more processors of the surgical robotic system can control system components in response to the input commands. For example, the surgeon may hold in her hand a user input device such as a joystick or a computer mouse that she manipulates to generate control signals to cause motion of the surgical robotic system components, e.g., an actuator, a robotic arm, and/or a surgical tool of the robotic system.</p><p>SUMMARY</p><p>[0006] During teleoperation with an open display in which the user can view their surroundings (as compared to a periscope type display) there is the possibility that the surgeon is looking away from the screen but still holding the user input devices that control the robotic tools. This introduces a risk since the surgeon could move the user input devices and unintentionally move the tools while not focusing on the screen. Therefore, in some aspects, the processes disclosed herein provide methods for determining whether a number of interlocks required for teleoperation are met, and therefore the user is looking at the open display (or an immersive display such as a periscope) and focused on teleoperation, such that teleoperation mode may be engaged or continue. In this aspect, the system detects a number of interlock parameters, conditions, inputs or the like, and then determines based on the detection of the interlock parameters, whether the system should disengage or engage teleoperation. In general, the interlock mechanisms or parameters are designed to determine the following (1) is the user looking at the screen, (2) is the user holding the user interface device in a usable manner, and (3) is the environment set up for teleoperation mode. If the answer to all of these conditions is yes, than teleoperation mode is engaged, if the answer to any of them is no, teleoperation mode may be disengaged.</p><p>[0007] Representatively, in one aspect, a method for engaging and disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument. The user interface devices may include at least one of a handheld user input device and a foot pedal. The display may be an open display. The one or more interlock detection components may include at least one of an eye and/or head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display may include detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner may include detecting at least one user interface device is inside the surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some cases, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner may include detecting the surgical robotic system is not in a transportation configuration.</p><p>[0008] In another aspect, a method for disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument. In some aspects, determining the user is looking away from the display may include at least one of the following: detecting a user gaze is outside of the display; detecting a user head is rotated away from the display; detecting a chair associated of the surgical robotic system is facing away from the display; detecting a user is not seated on the chair; and detecting the display is in a non-use position. In some aspects, determining the surgical robotic system is configured in a non-usable manner may include at least one of the following: detecting at least one user interface device is outside a surgical workspace; detecting a received user interface device location is inaccurate; and detecting at least one user interface device is dropped by the user. In some aspects, determining the surgical workspace of the surgical robotic system is configured in a non-usable manner may include detecting the surgical robotic system is in a transportation configuration.</p><p>[0009] In still further aspects, a surgical robotic system is disclosed. The system may include a surgical instrument; a user console comprising a display, an interlock detecting component, and a user interface device; and one or more processors communicatively coupled to the interlock detecting component, the processors configured to: receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device; determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument. In some aspects, the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner. The interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display may include: least one of an eye and/or a head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor. The interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner may include: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor. The interlock detecting component for detecting the interlock indicating the surgical workspace may be configured in a usable manner comprises a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display comprise: detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: detecting at least one user interface device is inside a surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some aspects, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner includes detecting the surgical robotic system is not in a transportation configuration.</p><p>[0010] The above summary does not include an exhaustive list of all aspects of the present invention. It is contemplated that the invention includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0011] The embodiments of the invention are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \\\"an\\\" or \\\"one\\\" embodiment of the invention in this disclosure are not necessarily to the same embodiment, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one embodiment of the invention, and not all elements in the figure may be required for a given embodiment.</p><p>[0012] FIG. <b>1</b> is a pictorial view of an example surgical robotic system in an operating arena, in accordance with an embodiment.</p><p>[0013] FIG. <b>2</b> is a pictorial view of a user console, in accordance with an embodiment.</p><p>[0014] FIG. <b>3</b> is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment.</p><p>[0015] FIG. <b>4</b> is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment.</p><p>[0016] FIG. <b>5</b> is a block diagram of a computer portion of a surgical robotic system, in accordance with an embodiment.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0017] In various embodiments, description is made with reference to the figures. However, certain embodiments may be practiced without one or more of these specific details, or in combination with other known methods and configurations. In the following description, numerous specific details are set forth, such as specific configurations, dimensions, and processes, in order to provide a thorough understanding of the embodiments. In other instances, well-known processes and manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the description. Reference throughout this specification to \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, means that a particular feature, structure, configuration, or characteristic described is included in at least one embodiment. Thus, the appearance of the phrase \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p>[0018] In addition, the terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the invention. Spatially relative terms, such as \\\"beneath\\\", \\\"below\\\", \\\"lower\\\", \\\"above\\\", \\\"upper\\\", and the like may be used herein for ease of description to describe one element's or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as \\\"below\\\" or \\\"beneath\\\" other elements or features would then be oriented \\\"above\\\" the other elements or features. Thus, the exemplary term \\\"below\\\" can encompass both an orientation of above and below. The device may be otherwise oriented (e.g., rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.</p><p>[0019] As used herein, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" are intended to include the plural forms as well, unless the context indicates otherwise. It will be further understood that the terms \\\"comprises\\\" and/or \\\"comprising\\\" specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.</p><p>[0020] The terms \\\"or\\\" and \\\"and/or\\\" as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, \\\"A, B or C\\\" or \\\"A, B and/or C\\\" mean \\\"any of the following: A; B; C; A and B; A and C; B and C; A, B and C.\\\" An exception to this definition will occur only when a combination of elements, functions, steps or acts are in some way inherently mutually exclusive.</p><p>[0021] Moreover, the use of relative terms throughout the description may denote a relative position or direction. For example, \\\"distal\\\" may indicate a first direction away from a reference point, e.g., away from a user. Similarly, \\\"proximal\\\" may indicate a location in a second direction opposite to the first direction, e.g., toward the user. Such terms are provided to establish relative frames of reference, however, and are not intended to limit the use or orientation of any particular surgical robotic component to a specific configuration described in the various embodiments below.</p><p>[0022] Referring to FIG. <b>1</b>, this is a pictorial view of an example surgical robotic system <b>100</b> in an operating arena. The surgical robotic system <b>100</b> includes a user console <b>102</b>, a control tower <b>103</b>, and one or more surgical robots <b>120</b>, including robotic arms <b>104</b> at a surgical robotic platform <b>105</b>, e.g., an operating table, a bed, etc. The system <b>100</b> can incorporate any number of devices, tools, or accessories used to perform surgery on a patient <b>106</b>. For example, the system <b>100</b> may include one or more surgical tools <b>107</b> used to perform surgery. A surgical tool <b>107</b> may be an end effector that is attached to a distal end of a surgical arm <b>104</b>, for executing a surgical procedure.</p><p>[0023] Each surgical tool <b>107</b> may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool <b>107</b> may be a tool used to enter, view, or manipulate an internal anatomy of the patient <b>106</b>. In an embodiment, the surgical tool <b>107</b> is a grasper that can grasp tissue of the patient. The surgical tool <b>107</b> may be controlled manually, by a bedside operator <b>108</b>; or it may be controlled robotically, via actuated movement of the surgical robotic arm <b>104</b> to which it is attached. The robotic arms <b>104</b> are shown as a table-mounted system, but in other configurations the arms <b>104</b> may be mounted in a cart, ceiling or sidewall, or in another suitable structural support.</p><p>[0024] Generally, a remote operator <b>109</b>, such as a surgeon or other operator, may use the user console <b>102</b> to remotely manipulate the arms <b>104</b> and/or the attached surgical tools <b>107</b>, e.g., teleoperation. The user console <b>102</b> may be located in the same operating room as the rest of the system <b>100</b>, as shown in FIG. <b>1</b>. In other environments however, the user console <b>102</b> may be located in an adjacent or nearby room, or it may be at a remote location, e.g., in a different building, city, or country. The user console <b>102</b> may comprise a seat <b>110</b>, one or more user interface devices, for example, foot-operated controls <b>113</b> or handheld user input devices (UID) <b>114</b>, and at least one user display <b>115</b> that is configured to display, for example, a view of the surgical site inside the patient <b>106</b>. In the example user console <b>102</b>, the remote operator <b>109</b> is sitting in the seat <b>110</b> and viewing the user display <b>115</b> while manipulating a foot-operated control <b>113</b> and a handheld UID <b>114</b> in order to remotely control the arms <b>104</b> and the surgical tools <b>107</b> (that are mounted on the distal ends of the arms <b>104</b>).</p><p>[0025] In some variations, the bedside operator <b>108</b> may also operate the system <b>100</b> in an \\\"over the bed\\\" mode, in which the bedside operator <b>108</b> (user) is now at a side of the patient <b>106</b> and is simultaneously manipulating a robotically-driven tool (end effector as attached to the arm <b>104</b>), e.g., with a handheld UID <b>114</b> held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator <b>108</b> may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient <b>106</b>.</p><p>[0026] During an example procedure (surgery), the patient <b>106</b> is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system <b>100</b> are in a stowed configuration or withdrawn configuration (to facilitate access to the surgical site.) Once access is completed, initial positioning or preparation of the robotic system <b>100</b> including its arms <b>104</b> may be performed. Next, the surgery proceeds with the remote operator <b>109</b> at the user console <b>102</b> utilizing the foot-operated controls <b>113</b> and the UIDs <b>114</b> to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, e.g., the bedside operator <b>108</b> who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms <b>104</b>. Non-sterile personnel may also be present to assist the remote operator <b>109</b> at the user console <b>102</b>. When the procedure or surgery is completed, the system <b>100</b> and the user console <b>102</b> may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilization and healthcare record entry or printout via the user console <b>102</b>.</p><p>[0027] In one embodiment, the remote operator <b>109</b> holds and moves the UID <b>114</b> to provide an input command to move a robot arm actuator <b>117</b> in the robotic system <b>100</b>. The UID <b>114</b> may be communicatively coupled to the rest of the robotic system <b>100</b>, e.g., via a console computer system <b>116</b>. Representatively, in some embodiments, UID <b>114</b> may be a portable handheld user input device or controller that is ungrounded with respect to another component of the surgical robotic system. For example, UID <b>114</b> may be ungrounded while either tethered or untethered from the user console. The term \\\"ungrounded\\\" is intended to refer to implementations where, for example, both UIDs are neither mechanically nor kinematically constrained with respect to the user console. For example, a user may hold a UID <b>114</b> in a hand and move freely to any possible position and orientation within space only limited by, for example, a tracking mechanism of the user console. The UID <b>114</b> can generate spatial state signals corresponding to movement of the UID <b>114</b>, e.g. position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator <b>117</b>. The robotic system <b>100</b> may use control signals derived from the spatial state signals, to control proportional motion of the actuator <b>117</b>. In one embodiment, a console processor of the console computer system <b>116</b> receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator <b>117</b> is energized to move a segment or link of the arm <b>104</b>, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID <b>114</b>. Similarly, interaction between the remote operator <b>109</b> and the UID <b>114</b> can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool <b>107</b> to close and grip the tissue of patient <b>106</b>.</p><p>[0028] The surgical robotic system <b>100</b> may include several UIDs <b>114</b>, where respective control signals are generated for each UID that control the actuators and the surgical tool (end effector) of a respective arm <b>104</b>. For example, the remote operator <b>109</b> may move a first UID <b>114</b> to control the motion of an actuator <b>117</b> that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc., in that arm <b>104</b>. Similarly, movement of a second UID <b>114</b> by the remote operator <b>109</b> controls the motion of another actuator <b>117</b>, which in turn moves other linkages, gears, etc., of the robotic system <b>100</b>. The robotic system <b>100</b> may include a right arm <b>104</b> that is secured to the bed or table to the right side of the patient, and a left arm <b>104</b> that is at the left side of the patient. An actuator <b>117</b> may include one or more motors that are controlled so that they drive the rotation of a joint of the arm <b>104</b>, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool <b>107</b> that is attached to that arm. Motion of several actuators <b>117</b> in the same arm <b>104</b> can be controlled by the spatial state signals generated from a particular UID <b>114</b>. The UIDs <b>114</b> can also control motion of respective surgical tool graspers. For example, each UID <b>114</b> can generate a respective grip signal to control motion of an actuator, e.g., a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool <b>107</b> to grip tissue within patient <b>106</b>.</p><p>[0029] In some aspects, the communication between the platform <b>105</b> and the user console <b>102</b> may be through a control tower <b>103</b>, which may translate user commands that are received from the user console <b>102</b> (and more particularly from the console computer system <b>116</b>) into robotic control commands that are transmitted to the arms <b>104</b> on the robotic platform <b>105</b>. The control tower <b>103</b> may also transmit status and feedback from the platform <b>105</b> back to the user console <b>102</b>. The communication connections between the robotic platform <b>105</b>, the user console <b>102</b>, and the control tower <b>103</b> may be via wired and/or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor and/or walls or ceiling of the operating room. The robotic system <b>100</b> may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. It will be appreciated that the operating room scene in FIG. <b>1</b> is illustrative and may not accurately represent certain medical practices.</p><p>[0030] In addition, in some aspects, surgical robotic system <b>100</b> may further include one or more interlock detecting mechanisms, devices, components or systems (e.g., trackers, sensors, etc.), that can be used to determine whether a teleoperation mode should be engaged or, once engaged, should be disengaged. The interlock detecting mechanisms may detect, for example, whether a user is looking toward or away from the display <b>115</b>, the UID is configured in a usable manner and/or the surgical environment is set up in a usable manner, and in turn, whether a teleoperation mode should be engaged or disengaged. For example, during a teleoperation mode of the surgical robotic system <b>100</b>, in which the user is controlling the surgical tool <b>107</b> using the UID <b>114</b>, the user should be viewing the tool movement on display <b>115</b>. In some cases, however, the user may look away from the display <b>115</b> (intentionally or unintentionally) while still holding the UID <b>114</b> and controlling the surgical tool <b>107</b>. This introduces a risk since the user could move the UID <b>114</b> and, in turn, unintentionally move the tool <b>107</b> while not focusing on the display <b>115</b>. Surgical robotic system <b>100</b> may therefore further include interlock detecting mechanisms that can be used to determine whether the user is focused on the display <b>115</b>, or otherwise interlocked or engaged with the system, in such a way that teleoperation mode is appropriate. The various interlock detecting mechanisms may be coupled to, in communication with, or otherwise associated with components of, the user console <b>102</b>, and will be described in more detail in reference to FIG. <b>2</b>.</p><p>[0031] It should be understood that \\\"engaging\\\" the teleoperation mode is intended to refer to an operation in which, for example, a UID or foot pedal that is prevented from controlling the surgical instrument, is transitioned to a mode (e.g., a teleoperation mode) in which it can now control the surgical instrument. On the other hand, disengaging the teleoperation mode is intended to refer to an operation which occurs when the system is in a teleoperation mode, and then transitioned to a mode (non-teleoperation mode) in which the UID or foot pedal can no longer control the surgical instrument. For example, teleoperation mode may be disengaged when the system determines that one or more of a number of interlocks required for teleoperation mode is no longer present or met. The interlocks required for teleoperation mode may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner.</p><p>[0032] Referring now to FIG. <b>2</b>, FIG. <b>2</b> illustrates a pictorial view of an exemplary user console including a number of devices, mechanisms, systems and/or components used to detect whether the interlocks required for teleoperation mode are met. Representatively, user console <b>102</b> is shown including a chair <b>110</b> and display <b>115</b>, as previously discussed. In addition, user console <b>102</b> is shown having an optional display <b>202</b> mounted to chair <b>110</b> by a display arm <b>204</b>. For example, display <b>115</b> may be an open display, and display <b>202</b> may be an immersive display. The term \\\"open\\\" display is intended to refer to a display which is designed to allow the user to see outside of the display, for example with their peripheral vision, even when directly facing the display. In addition, in an open display, the user can be a distance from the display screen or not directly in front of the display, and still view a surgical procedure on the display. Therefore, in an open display such as display <b>115</b>, the fact that the user may not be close to the display or have their face directly in front of the display, would not necessarily mean the user is not looking at the display or focused on the surgical procedure. In the case of an open display, in which the user can turn their head and still see the display using their peripheral vision, it is therefore important that turning of the head slightly not be interpreted to mean the user is not focused on the display. Rather, in the case of an open display, there will be some tolerance to such actions and allow for teleoperation mode to continue. This is in contrast to an immersive display such as display <b>202</b>. For example, display <b>202</b> could be a completely immersive display (as in the case of a periscope) that prevents the user from seeing outside of the display screen when they are facing the display and requires the user to be relatively close to the display screen. For example, in the case of an immersive display, the user must have their face relatively close to, and facing, the display screen to use it to view the surgical procedure in progress. If the user pulls their head away from the display screen, or doesn't face the display screen, they can no longer see the display screen, therefore this would typically be interpreted to mean the user is not focused on the screen or not paying sufficient attention to continue in teleoperation mode.</p><p>[0033] To detect whether all the interlocks required for engagement of the teleoperation mode (or continuing in the teleoperation mode) are met, user console <b>102</b> may further include a number of interlock detecting devices, mechanisms, systems and/or components. The interlock detecting mechanisms may include, but are not limited to, an eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, an immersive display deployment sensor <b>214</b>, a UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, UID contact sensor <b>220</b>, and/or a transport sensor <b>222</b>. Each of the interlock detecting mechanisms are configured to detect at least one of a number of conditions (referred to herein as \\\"interlocks\\\") which are required to engage the teleoperation mode. As previously discussed, the interlocks may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner. Table 1 provides an exemplary listing of the specific interlocks which are required to engage a teleoperation mode, and to continue teleoperation mode, once engaged.</p><p>[0034] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\" pgwide=\\\"1\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Interlock</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze on the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated toward the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated toward the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is resting on the headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is inside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace not distorted and tracker signal is </TD></TR><TR><TD>accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is being held by the user</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is not in a transportation mode</TD></TR><TR></TR></Table></p><p>[0035] If any of interlocks 1-11 are not detected, a teleoperation mode cannot be engaged, or if already engaged, may be automatically disengaged. Said another way, if any one or more of the following conditions listed in Table 2 are determined (e.g., when any of interlocks 1-11 are not detected) teleoperation mode is either not engaged or disengaged.</p><p>[0036] <Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\" pgwide=\\\"1\\\"><TR><TD>TABLE 2</TD></TR><TR></TR><TR><TD>Condition</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze is outside of the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated away from the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated away from the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing away from the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is not seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is not resting on the headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a non-use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is outside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace distorted and tracker signal is not </TD></TR><TR><TD>accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is not being held by the user, or is in a stored configuration</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is in a transportation mode</TD></TR><TR></TR></Table></p><p>[0037] The interlock detecting mechanisms for detecting, for example, interlocks 1-7 listed in Table 1, which generally relate to whether the user is looking at or focused on the display, may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, chair swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, headrest pressure sensor <b>212</b>, and immersive display deployment sensor <b>214</b>. The interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether the user is holding the controllers in a usable manner, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b> and UID contact sensor <b>220</b>. The interlock detecting mechanisms for detecting interlock 11, which relates to whether the surgical environment is set up in a usable manner, may include transport sensor <b>222</b>, in addition to one or more of the UID sensors <b>216</b>, <b>218</b>, <b>220</b>. Once a condition or interlock is detected by the interlock detecting mechanism, the information is output to a processing component of the surgical robotic system, where it is received as an interlock input and monitored to determine whether teleoperation should continue, should be engaged or should be disengaged. For example, if interlock inputs corresponding to all the required interlocks for teleoperation mode are received, teleoperation mode is engaged. If any interlocks are missing, teleoperation mode is not engaged (if currently in a non-teleoperation mode), or disengaged (if currently in teleoperation mode).</p><p>[0038] Referring now in more detail to each of the interlock detecting mechanisms, eye and/or head tracker <b>206</b> may be a tracking device that is coupled to the display <b>115</b> to track a user's gaze and/or head with respect to the display <b>115</b>, and indicate whether or not the user is looking toward or away from the display. Eye and/or head tracker <b>206</b> may be attached to any portion of display <b>115</b> suitable for tracking a gaze of a user. For example, eye and/or head tracker <b>206</b> may be attached to a housing <b>224</b> of display <b>115</b>, for example, a top wall, a bottom wall, or a side wall of housing <b>224</b>, or integrated within a screen (not shown) mounted within the housing <b>224</b> of display <b>115</b>. Eye and/or head tracker <b>206</b> may include one or more projector(s) and camera(s) which face the user and can be used to track a gaze of a user. Representatively, the projector(s) may create a pattern of near-infrared light on the eyes of the user, and the camera(s) may take images of the user's eyes and the pattern. Eye and/or head tracker <b>206</b> may further be programmed to use this information (e.g., execute machine learning, image processing and/or mathematical algorithms) to determine where the user is looking based on a position of each of the user's eyes and/or gaze point or location relative to one another, and display <b>115</b>. For example, when the user is sitting on chair <b>110</b> in front of display <b>115</b>, eye and/or head tracker <b>206</b> may be configured to create a pattern of near-infrared light on the eyes of the user and take images of the user's eyes and the pattern. This information may, in turn, be input to the surgical robotic system processing component (e.g., an interlock input signal) to determine whether the gaze of the user is inside or outside of display <b>115</b>. If the gaze is determined to be inside the display, the user is determined to be looking toward the display. On the other hand, if the gaze is determined to be outside the display, the user is determined to be looking away from the display.</p><p>[0039] In addition, eye and/or head tracker <b>206</b> can also track a position, orientation and/or location of the user's head as an indication of whether the user is looking at the display. For example, tracker <b>206</b> may detect that the user's head is in front of the display and/or the user's face or nose is facing display <b>115</b>, which suggests the user is looking at the display, or that the user's head is not in front of the display and/or their face or nose is turned away from display <b>115</b>, which suggests the user is looking away from the display. In addition, the tracker <b>206</b> may be used to determine automatically if the open display <b>115</b> or the immersive display <b>202</b> should be used for teleoperation.</p><p>[0040] Still further, tracking markers <b>226</b> on the user's glasses and/or tracking markers <b>228</b> on the user's face mask may be used to determine whether the user is looking toward or away from the display <b>115</b>. The tracking markers <b>226</b> on the user's glasses (e.g., 3D glasses) and/or markers <b>228</b> on the face mask (if user is sterile) may be active or passive markers. The markers <b>226</b> and/or <b>228</b> may track if the user's head is rotated toward the screen, indicating the user is looking toward the display. The markers <b>226</b> and/or <b>228</b> may further track whether the head is rotated away from the screen, indicating the user is looking away from the display. In some cases where the markers <b>226</b> are attached to the user's glasses, they may be similar to the previously discussed eye and/or head tracker and include one or more projectors, cameras and/or processing algorithms that can analyze information received from the camera(s) and projector(s) to determine whether the user is looking at the display.</p><p>[0041] Additional interlock mechanisms for detecting whether or not the user is looking toward or away from the display may include a swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, and a headrest pressure sensor <b>212</b>. The swivel sensor <b>208</b> may be any type of sensing device or component suitable for detecting a rotation (or swivel) and/or orientation of the chair <b>110</b>, with respect to, for example, a stationary base <b>230</b>. For example, the swivel sensor <b>208</b> may include a position encoder, switch or the like. The swivel sensor <b>208</b> may then output a corresponding signal to a processing component of the surgical robotic system corresponding to the rotation, position and/or orientation of the chair <b>110</b>. The surgical robotic system can then use this information to determine whether the chair <b>110</b> is facing display <b>115</b>, and therefore a user seated in the chair is facing the display <b>115</b>. If, on the other hand, swivel sensor <b>208</b> detects the chair is facing away from the display, it is determined the user is looking away from the display. The chair pressure sensor <b>210</b> and the headrest pressure sensor <b>212</b> may be pressure sensors which are attached to the chair headrest and chair seat, respectively. The chair pressure sensor <b>210</b> can be used to detect when the user is seated in the chair <b>110</b>, and the headrest sensor <b>212</b> can be used to detect when the user's head is resting on the headrest. Sensors <b>210</b>, <b>212</b> may output a signal indicating that the user is seated in chair <b>110</b> and/or the head is resting on the headrest to the system processing component, which can be used as indicators that the user is looking at the display <b>115</b>. If the sensors <b>210</b>, <b>212</b> do not detect the user seated in the chair <b>110</b>, this is interpreted as an indication the user is not looking toward the display, or looking away from the display. Sensors <b>210</b> and <b>212</b> may be any type of sensor suitable for detecting the user applying pressure to the seat in the particular location in which the sensor is positioned, for example, they may be pressure plates that are mounted within portions of the headrest and seat which would be contacted by a user seated in the chair <b>110</b>. It should be understood, however, if chair <b>110</b> of console <b>102</b> does not swivel, swivel sensor <b>208</b> may be omitted, and interlock 4 of Table 1 (e.g., chair sensor indicates chair is facing the display) may not be required to engage teleoperation mode.</p><p>[0042] In addition, in embodiments where user console <b>102</b> includes immersive display <b>202</b>, an immersive display deployment sensor <b>214</b> to detect a location of the immersive display <b>202</b> may be provided. The immersive display deployment sensor <b>214</b> may be coupled to the immersive display <b>202</b> and include switches, encoders or the like that are operable to determine if display <b>202</b> is in a \\\"use\\\" position (e.g., position suitable for viewing by a user seated in the chair), \\\"non-use\\\" position (e.g., positioned not suitable for viewing by a user seated in the chair) and/or deployed position (e.g., positioned in front of the chair). This information can be used in conjunction with a sensor (e.g., pressure or light gate sensor) to determine if the user's head is in the immersive display <b>202</b>. A teleoperation mode will only be engaged if the immersive display <b>202</b> is in the \\\"use\\\" configuration and the sensor detects that the user's head is in the immersive display <b>202</b>. If display is determined to be in \\\"non-use\\\" position and/or the user's head is not detected in display <b>202</b>, this is interpreted as an indication the user is looking away from the display. It should be understood, however, if immersive display <b>202</b> is omitted, deployment sensor <b>214</b> may be omitted, and interlock 7 of Table 1 (e.g., immersive display sensor indicates immersive display is in a use position) is not required to engage teleoperation mode.</p><p>[0043] In addition, the interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether a user is holding the controllers (e.g., UID) in a usable or non-usable manner or configuration, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be, for example, an optical tracker, a position tracker and/or an encoder, that is coupled to the user console and/or the UID. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be used to track, for example, the position of the UID, whether the user is holding the UID in a usable or non-usable manner, and/or whether the UID is inside or outside of the trackable workspace. If the UID is outside of the workspace, the UID is not being held in a usable manner, and the system should be disengaged. In addition, if the tracking space is distorted and the signal received from the tracker does not give an accurate reading, the UID is considered not to be in a usable manner or in a non-usable configuration, and teleoperation should be disengaged. Still further, the sensor <b>216</b>, <b>218</b> and/or <b>220</b> can be used to track whether the UID is stored within the user console (e.g., within a holster or other holding mechanism) and/or the UID has been dropped. If the UID is stored within the console and/or has been dropped, such that the user is no longer holding the UID, the UID is considered to be in a non-usable configuration, and teleoperation should be disengaged. In some embodiments, one or more of sensors <b>216</b>, <b>218</b>, <b>220</b> may be an optical tracker and/or an absolute encoders. In some embodiments, a position sensor (optical tracker) is mounted on the user console (e.g., near display <b>115</b>) and/or the base of the UID. In some embodiments, the UID is tracked by a sensor coupled to the console or the user. The position sensor can provide the position of the UID relative to a user and/or a ground reference point (not shown). In some embodiments, more than one (e.g., a plurality, several) tracking technologies are used in conjunction.</p><p>[0044] In addition, the interlock detecting mechanisms for detecting, for example, interlock 11 listed in Table 1, which generally relates to whether the surgical environment is set up in a usable manner or non-usable manner, may include transport sensor <b>222</b>. Transport sensor <b>222</b> may be a sensor which detects when the surgical robotic system is transitioned to a transportation configuration or mode (e.g., console <b>102</b> can be moved), by sensing, for example, that the brakes are released. If a transition to a transportation configuration is detected, the transportation interlock is no longer met, and the surgical environment is considered to be a non-usable configuration and teleoperation mode should be disengaged. If, on the other hand, the transport sensor <b>222</b> determines the system is not in a transportation configuration (e.g., the brakes are engaged), the surgical environment is in a usable configuration, and teleoperation can be engaged.</p><p>[0045] Additional interlock detecting mechanisms may include the use of the eye tracking aspects previously discussed for determining interpupillary distance (IPD) for the immersive display <b>202</b>. For example, given the position of the center of the eyeballs from the eye tracker, the IPD can be calculated for the immersive display. This may remove a requirement for the user to manually adjust the immersive display IPD.</p><p>[0046] FIG. <b>3</b> is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment. Representatively, process <b>300</b> includes the operations of receiving a plurality of interlock inputs (block <b>302</b>). The plurality of interlock inputs may be one or more of the interlocks 1-11 received from the interlock detecting mechanisms previously discussed in reference to FIG. <b>2</b> and Table 1. Based on the plurality of interlock inputs received, the system then determines whether a user is looking toward or away from the display (block <b>304</b>). As previously discussed, the interlocks indicating that the user is looking toward the display may be interlocks 1-7 listed in Table 1. If any one of interlocks 1-7 are not met or satisfied, in other words at least one of conditions 1-7 listed in Table 2 is detected, it is determined that the user is not looking toward the display (e.g., looking away) and teleoperation mode is not engaged (block <b>306</b>). On the other hand, if all of interlocks 1-7 listed in Table 1 are met or satisfied, it is determined at operation <b>304</b> that the user is looking toward the display and process <b>300</b> continues to operation <b>308</b>.</p><p>[0047] At operation <b>308</b>, the system determines whether a user interface device is positioned in a usable or non-usable manner. A user interface device is considered positioned in a usable manner when it is in a position suitable for a user to control a surgical instrument in a teleoperation mode. The user interface device may be considered to be in a non-usable manner when it is not in a position suitable for the user to control the surgical instrument. Representatively, the user interface device is considered positioned in a usable manner if all of the interlocks 8-10 listed in Table 1 are met or satisfied, and the process <b>300</b> continues to operation <b>310</b>. For example, if UID <b>114</b> is inside a trackable workspace, the tracking workspace is not distorted and the UID <b>114</b> is being held by the user, the UID <b>114</b> is positioned in a usable manner. On the other hand, if it is determined at operation <b>308</b> that one of conditions 8-10 listed in Table 2 are detected, namely the UID is outside of the trackable workspace and the information received from the tracker is not accurate, the tracking space is distorted and the signal received from the tracker does not give an accurate reading, or the UID is not being held by a user (e.g., the UID is stored or dropped), the system determines the user interface device is not positioned in a usable manner, or is positioned in a non-usable manner, and does not engage teleoperation mode.</p><p>[0048] In operation <b>310</b>, the system determines whether the surgical workspace is configured in a usable manner or a non-usable manner. The surgical workspace (e.g., the user console, robotic arms, surgical table, etc.) may be considered to be positioned or configured in a usable manner when components of the surgical workspace are properly configured for a user to control a surgical instrument in a teleoperation mode. For example, the surgical workspace is considered to be configured in a usable manner if the interlock 11 listed in Table 1 is met or satisfied. For example, if the user console, chair, display or other associated component is in a fixed configuration (e.g., not in a transportation configuration in which the brakes are released), the surgical workspace is considered to be properly configured. If the surgical workspace is configured in a usable manner, the process continues to operation <b>312</b> and teleoperation mode is engaged. If, on the other hand, it is determined at operation <b>310</b> that the system is transitioned to a transportation configuration (e.g., the brakes are released), in other words condition 11 of Table 2 is detected, the surgical workspace is configured in a non-usable manner for teleoperation mode, and the process proceeds to operation <b>306</b> and teleoperation mode is not engaged.</p><p>[0049] FIG. <b>4</b> is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment. Representatively, similarly to process <b>300</b>, process <b>400</b> includes the operation of receiving a plurality of interlock inputs (block <b>402</b>), for example, interlocks 1-11 previously discussed in reference to Table 1. When all of the interlock inputs required for teleoperation mode are received (e.g., the user is looking toward or otherwise focused on the display), a teleoperation mode is engaged at operation <b>404</b>. Process <b>400</b> then continues to monitor whether all of the interlock inputs are still present at operation <b>406</b>. As long as the interlocks continue to be detected, teleoperation mode will continue (block <b>410</b>). If, on the other hand, it is determined at operation <b>406</b> that one or more of the interlock inputs is no longer detected, in other words one or more of conditions 1-11 listed in Table 2 are present, the system transitions out of the teleoperation mode at operation <b>408</b>, for example to a non-teleoperation mode in which the user interface device is prevented from controlling a surgical instrument.</p><p>[0050] FIG. <b>5</b> is a block diagram of a computer portion of a surgical robotic system, which is operable to implement the previously discussed operations, in accordance with an embodiment. The exemplary surgical robotic system <b>500</b> may include a user console <b>102</b>, a surgical robot <b>120</b>, and a control tower <b>103</b>. The surgical robotic system <b>500</b> may include other or additional hardware components; thus, the diagram is provided by way of example and not a limitation to the system architecture.</p><p>[0051] As described above, the user console <b>102</b> may include console computers <b>511</b>, one or more UIDs <b>512</b>, console actuators <b>513</b>, displays <b>514</b>, foot pedals <b>516</b>, console computers <b>511</b> and a network interface <b>518</b>. In addition, user console <b>102</b> may include a number of interlock detecting mechanisms, devices, or components, for example, a UID tracker(s) <b>515</b>, a display tracker(s) <b>517</b> and a console tracker(s) <b>519</b>, for detecting whether the previously discussed interlocks required for teleoperation are satisfied. For example, UID tracker(s) <b>515</b> may include the previously discussed UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. Display tracker(s) <b>517</b> may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, and/or an immersive display deployment sensor <b>214</b>. Console tracker(s) may include transport sensor <b>222</b>. The interlock information detected and/or tracked by any one or more of the trackers may be monitored and communicated to the console computer <b>511</b>, and dispatched to the control tower <b>103</b> via the network interface <b>518</b>, so that the system can determine whether to engage and/or disengage a teleoperation mode.</p><p>[0052] It should further be understood that a user or surgeon sitting at the user console <b>102</b> can adjust ergonomic settings of the user console <b>102</b> manually, or the settings can be automatically adjusted according to user profile or preference. The manual and automatic adjustments may be achieved through driving the console actuators <b>513</b> based on user input or stored configurations by the console computers <b>511</b>. The user may perform robot-assisted surgeries by controlling the surgical robot <b>120</b> using one or more master UIDs <b>512</b> and foot pedals <b>516</b>. Positions and orientations of the UIDs <b>512</b> are continuously tracked by the UID tracker <b>515</b>, and status changes are recorded by the console computers <b>511</b> as user input and dispatched to the control tower <b>103</b> via the network interface <b>518</b>. Real-time surgical video of patient anatomy, instrumentation, and relevant software apps can be presented to the user on the high resolution 3D displays <b>514</b> including open or immersive displays.</p><p>[0053] The user console <b>102</b> may be communicatively coupled to the control tower <b>103</b>. The user console also provides additional features for improved ergonomics. For example, the user console may be an open architecture system including an open display, although an immersive display, in some cases, may be provided. Furthermore, a highly-adjustable seat for surgeons and master UIDs tracked through electromagnetic or optical trackers are included at the user console <b>102</b> for improved ergonomics.</p><p>[0054] The control tower <b>103</b> can be a mobile point-of-care cart housing touchscreen displays, computers that control the surgeon's robotically-assisted manipulation of instruments, safety systems, graphical user interface (GUI), light source, and video and graphics computers. As shown in FIG. <b>5</b>, the control tower <b>103</b> may include central computers <b>531</b> including at least a visualization computer, a control computer, and an auxiliary computer, various displays <b>533</b> including a team display and a nurse display, and a network interface <b>518</b> coupling the control tower <b>103</b> to both the user console <b>102</b> and the surgical robot <b>120</b>. The control tower <b>103</b> may offer additional features for user convenience, such as the nurse display touchscreen, soft power and E-hold buttons, user-facing USB for video and still images, and electronic caster control interface. The auxiliary computer may also run a real-time Linux, providing logging/monitoring and interacting with cloud-based web services.</p><p>[0055] The surgical robot <b>120</b> may include an articulated operating table <b>524</b> with a plurality of integrated arms <b>522</b> that can be positioned over the target patient anatomy. A suite of compatible tools <b>523</b> can be attached to or detached from the distal ends of the arms <b>522</b>, enabling the surgeon to perform various surgical procedures. The surgical robot <b>120</b> may also comprise control interface <b>525</b> for manual control of the arms <b>522</b>, table <b>524</b>, and tools <b>523</b>. The control interface can include items such as, but not limited to, remote controls, buttons, panels, and touchscreens. Other accessories such as trocars (sleeves, seal cartridge, and obturators) and drapes may also be needed to perform procedures with the system. In some variations, the plurality of the arms <b>522</b> includes four arms mounted on both sides of the operating table <b>524</b>, with two arms on each side. For certain surgical procedures, an arm mounted on one side of the table can be positioned on the other side of the table by stretching out and crossing over under the table and arms mounted on the other side, resulting in a total of three arms positioned on the same side of the table <b>524</b>. The surgical tool can also comprise table computers <b>521</b> and a network interface <b>518</b>, which can place the surgical robot <b>120</b> in communication with the control tower <b>103</b>.</p><p>[0056] In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p>1. A method for disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system;<BR />determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate the following interlock conditions are satisfied: (1) at least one or more user interface devices of the surgical robotic system having a tracker that indicates the at least one or more user interface devices are outside a trackable workspace of the surgical robotic system or the at least one or more user interface devices are in a stored configuration, and (2) a user&apos;s gaze is outside a display of the surgical robotic system; and<BR />in response to determining the interlock conditions are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>2. The method of claim 1 wherein the at least one or more user interface devices comprise at least one of a handheld user input device and a foot pedal.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>3. The method of claim 1 wherein the display is an open display.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>4. The method of claim 1 wherein the interlock detection component for determining the user&apos;s gaze is outside the display comprises an eye tracker that indicates the user gaze is outside of the display.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>5. The method of claim 1 wherein the interlock detection component for determining the user&apos;s gaze is outside the display comprises a tracking marker on a pair of glasses or a face mask of the user, and the tracking marker indicates a head of the user is rotated away from the display.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>6. The method of claim 1 wherein the interlock detection component for determining the user&apos;s gaze is outside the display comprises a chair sensor coupled to a chair, and the chair sensor indicates at least one of the chair is facing away from the display, the user is not seated in the chair, or a head of the user is not resting on a headrest of the chair.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>7. The method of claim 1 wherein the tracker indicates the at least one of the user interface devices are outside of a trackable workspace.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p>8. The method of claim 1 wherein the interlock detection component comprises a transport sensor and at least one of the plurality of interlock inputs further indicates the surgical workspace of the surgical robotic system is not configured in a usable manner by <BR />detecting the surgical robotic system is not in a transportation configuration.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"yes\\\"><p>9. A method for disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one of a plurality of interlock inputs received from one or more interlock detection components comprising a headrest pressure sensor or a display deployment sensor of the surgical robotic system indicates a user&apos;s gaze is outside a display of the surgical robotic system; and<BR />in response to determining that the user&apos;s gaze is outside of the display, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>10. The method of claim 9 wherein the interlock detection component further comprises an eye tracker coupled to a display of the surgical robotic system, and the eye tracker comprises a projector that projects a pattern of near-infrared light on the eye of the user, and a camera that takes an image of the user&apos;s eye relative to the pattern to determine the gazes is outside the display.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>11. The method of claim 9 wherein the interlock detection component further comprises a tracking marker on a user&apos;s glasses or a user&apos;s face mask, and the tracking marker tracks a user&apos;s head rotated away from the display to determine the gaze is outside the display.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>12. The method of claim 9 further comprises determining the surgical robotic system is configured in a non-usable manner by detecting at least one of the following: <BR />at least one user interface device is outside a surgical workspace;<BR />a received user interface device location is inaccurate; and<BR />at least one user interface device is dropped by the user.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p>13. The method of claim 9 further comprising determining a surgical workspace of the surgical robotic system is configured in a non-usable manner by detecting the surgical robotic system is in a transportation configuration.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"yes\\\"><p>14. A surgical robotic system, comprising: <BR />a surgical instrument;<BR />a user console comprising a display, an interlock detecting component, and a user interface device; and<BR />one or more processors communicatively coupled to the interlock detecting component, the processors configured to: <BR />receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device,<BR />determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present and a user&apos;s gaze is outside of a display based on at least one interlock input received from a headrest pressure sensor or a display deployment sensor, and<BR />transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>15. The system of claim 14 wherein the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>16. The system of claim 14 wherein the interlock detecting component for detecting the user&apos;s gaze is outside of the display further comprises an eye and/or a head tracker.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>17. The system of claim 14 wherein the interlock detecting component for detecting the user&apos;s gaze is outside of the display further comprises a tracking marker on a user&apos;s glasses, or a tracking marker on a user&apos;s face mask.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>18. The system of claim 14 wherein the interlock detecting component for detecting the user&apos;s gaze is outside the display further comprises a chair swivel sensor, or a chair pressure sensor.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p>19. The system of claim 14 wherein the interlock detecting component further comprises an eye tracker for tracking the user&apos;s gaze, and further comprises a transport sensor for detecting an interlock indicating a surgical workspace is not configured in a usable manner.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"19\\\"><p>20. The system of claim 19 wherein the transport sensor indicates the surgical robotic system is not in a transportation mode.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/00\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/00\",\n",
      "\n",
      "\"B25J9/16\",\n",
      "\n",
      "\"G06F3/01\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2034/305\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/0009\",\n",
      "\n",
      "\"B25J9/1689\",\n",
      "\n",
      "\"G06F3/012\",\n",
      "\n",
      "\"G06F3/013\",\n",
      "\n",
      "\"G06F3/017\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"VERB SURGICAL INC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190517,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230470252\",\n",
      "\n",
      "\"ad\": 20230919,\n",
      "\n",
      "\"pn\": \"US12213756\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20250204,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"granted / extensions (spc, cpc, pte)\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20230470252\",\n",
      "\n",
      "\"ad\": 20230919,\n",
      "\n",
      "\"pn\": \"US12213756\",\n",
      "\n",
      "\"kd\": \"BB\",\n",
      "\n",
      "\"daterecorded\": 20430919,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"PEXP\",\n",
      "\n",
      "\"event\": \"Calculated Patent Term Expiry\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82037919\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20190415992\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"INTERLOCK MECHANISMS TO DISENGAGE AND ENGAGE A TELEOPERATION MODE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method for engaging and disengaging a surgical instrument of a surgical robotic system comprising: receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<shortsum><p>BACKGROUND</p><p>Field</p><p>[0001] Embodiments related surgical robotic systems, are disclosed. More particularly, embodiments related to interlock mechanisms that disengage and engage a teleoperation mode, are disclosed.</p><p>Background</p><p>[0002] Endoscopic surgery involves looking into a patient's body and performing surgery inside the body using endoscopes and other surgical tools. For example, laparoscopic surgery can use a laparoscope to access and view an abdominal cavity. Endoscopic surgery can be performed using manual tools and/or a surgical robotic system having robotically-assisted tools.</p><p>[0003] A surgical robotic system may be remotely operated by a surgeon to command a robotically-assisted tool located at an operating table. Such operation of a robotically-assisted tool remotely by a surgeon may be commonly referred to as teleoperation. For example, the surgeon may use a computer console located in the operating room, or it may be located in a different city, to command a robot to manipulate the surgical tool mounted on the operating table. The robotically-controlled surgical tool can be an endoscope mounted on a robotic arm. Accordingly, the surgical robotic system may be used by the remote surgeon to perform an endoscopic surgery.</p><p>[0004] The surgeon may provide input commands to the surgical robotic system, and one or more processors of the surgical robotic system can control system components in response to the input commands. For example, the surgeon may hold in her hand a user input device such as a joystick or a computer mouse that she manipulates to generate control signals to cause motion of the surgical robotic system components, e.g., an actuator, a robotic arm, and/or a surgical tool of the robotic system.</p><p>SUMMARY</p><p>[0005] During teleoperation with an open display in which the user can view their surroundings (as compared to a periscope type display) there is the possibility that the surgeon is looking away from the screen but still holding the user input devices that control the robotic tools. This introduces a risk since the surgeon could move the user input devices and unintentionally move the tools while not focusing on the screen. Therefore, in some aspects, the processes disclosed herein provide methods for determining whether a number of interlocks required for teleoperation are met, and therefore the user is looking at the open display (or an immersive display such as a periscope) and focused on teleoperation, such that teleoperation mode may be engaged or continue. In this aspect, the system detects a number of interlock parameters, conditions, inputs or the like, and then determines based on the detection of the interlock parameters, whether the system should disengage or engage teleoperation. In general, the interlock mechanisms or parameters are designed to determine the following (1) is the user looking at the screen, (2) is the user holding the user interface device in a usable manner, and (3) is the environment set up for teleoperation mode. If the answer to all of these conditions is yes, than teleoperation mode is engaged, if the answer to any of them is no, teleoperation mode may be disengaged.</p><p>[0006] Representatively, in one aspect, a method for engaging and disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument. The user interface devices may include at least one of a handheld user input device and a foot pedal. The display may be an open display. The one or more interlock detection components may include at least one of an eye and/or head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display may include detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner may include detecting at least one user interface device is inside the surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some cases, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner may include detecting the surgical robotic system is not in a transportation configuration.</p><p>[0007] In another aspect, a method for disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument. In some aspects, determining the user is looking away from the display may include at least one of the following: detecting a user gaze is outside of the display; detecting a user head is rotated away from the display; detecting a chair associated of the surgical robotic system is facing away from the display; detecting a user is not seated on the chair; and detecting the display is in a non-use position. Iin some aspects, determining the surgical robotic system is configured in a non-usable manner may include at least one of the following: detecting at least one user interface device is outside a surgical workspace; detecting a received user interface device location is inaccurate; and detecting at least one user interface device is dropped by the user. In some aspects, determining the surgical workspace of the surgical robotic system is configured in a non-usable manner may include detecting the surgical robotic system is in a transportation configuration.</p><p>[0008] In still further aspects, a surgical robotic system is disclosed. The system may include a surgical instrument; a user console comprising a display, an interlock detecting component, and a user interface device; and one or more processors communicatively coupled to the interlock detecting component, the processors configured to: receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device; determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument. In some aspects, the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner. The interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display may include: least one of an eye and/or a head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor. The interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner may include: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor. The interlock detecting component for detecting the interlock indicating the surgical workspace may be configured in a usable manner comprises a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display comprise: detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: detecting at least one user interface device is inside a surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some aspects, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner includes detecting the surgical robotic system is not in a transportation configuration.</p><p>[0009] The above summary does not include an exhaustive list of all aspects of the present invention. It is contemplated that the invention includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0010] The embodiments of the invention are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \\\"an\\\" or \\\"one\\\" embodiment of the invention in this disclosure are not necessarily to the same embodiment, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one embodiment of the invention, and not all elements in the figure may be required for a given embodiment.</p><p>[0011] FIG. 1 is a pictorial view of an example surgical robotic system in an operating arena, in accordance with an embodiment.</p><p>[0012] FIG. 2 is a pictorial view of a user console, in accordance with an embodiment.</p><p>[0013] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment.</p><p>[0014] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment.</p><p>[0015] FIG. 5 is a block diagram of a computer portion of a user console including interlock mechanisms, in accordance with an embodiment.</p><p>[0016] FIG. 6 is a block diagram of a computer portion of a surgical robotic system, in accordance with an embodiment.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0017] In various embodiments, description is made with reference to the figures. However, certain embodiments may be practiced without one or more of these specific details, or in combination with other known methods and configurations. In the following description, numerous specific details are set forth, such as specific configurations, dimensions, and processes, in order to provide a thorough understanding of the embodiments. In other instances, well-known processes and manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the description. Reference throughout this specification to \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, means that a particular feature, structure, configuration, or characteristic described is included in at least one embodiment. Thus, the appearance of the phrase \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p>[0018] In addition, the terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the invention. Spatially relative terms, such as \\\"beneath\\\", \\\"below\\\", \\\"lower\\\", \\\"above\\\", \\\"upper\\\", and the like may be used herein for ease of description to describe one element's or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as \\\"below\\\" or \\\"beneath\\\" other elements or features would then be oriented \\\"above\\\" the other elements or features. Thus, the exemplary term \\\"below\\\" can encompass both an orientation of above and below. The device may be otherwise oriented (e.g., rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.</p><p>[0019] As used herein, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" are intended to include the plural forms as well, unless the context indicates otherwise. It will be further understood that the terms \\\"comprises\\\" and/or \\\"comprising\\\" specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.</p><p>[0020] The terms \\\"or\\\" and \\\"and/or\\\" as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, \\\"A, B or C\\\" or \\\"A, B and/or C\\\" mean \\\"any of the following: A; B; C; A and B; A and C; B and C; A, B and C.\\\" An exception to this definition will occur only when a combination of elements, functions, steps or acts are in some way inherently mutually exclusive.</p><p>[0021] Moreover, the use of relative terms throughout the description may denote a relative position or direction. For example, \\\"distal\\\" may indicate a first direction away from a reference point, e.g., away from a user. Similarly, \\\"proximal\\\" may indicate a location in a second direction opposite to the first direction, e.g., toward the user. Such terms are provided to establish relative frames of reference, however, and are not intended to limit the use or orientation of any particular surgical robotic component to a specific configuration described in the various embodiments below.</p><p>[0022] Referring to FIG. 1, this is a pictorial view of an example surgical robotic system <b>100</b> in an operating arena. The surgical robotic system <b>100</b> includes a user console <b>102</b>, a control tower <b>103</b>, and one or more surgical robots <b>120</b>, including robotic arms <b>104</b> at a surgical robotic platform <b>105</b>, e.g., an operating table, a bed, etc. The system <b>100</b> can incorporate any number of devices, tools, or accessories used to perform surgery on a patient <b>106</b>. For example, the system <b>100</b> may include one or more surgical tools <b>107</b> used to perform surgery. A surgical tool <b>107</b> may be an end effector that is attached to a distal end of a surgical arm <b>104</b>, for executing a surgical procedure.</p><p>[0023] Each surgical tool <b>107</b> may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool <b>107</b> may be a tool used to enter, view, or manipulate an internal anatomy of the patient <b>106</b>. In an embodiment, the surgical tool <b>107</b> is a grasper that can grasp tissue of the patient. The surgical tool <b>107</b> may be controlled manually, by a bedside operator <b>108</b>; or it may be controlled robotically, via actuated movement of the surgical robotic arm <b>104</b> to which it is attached. The robotic arms <b>104</b> are shown as a table-mounted system, but in other configurations the arms <b>104</b> may be mounted in a cart, ceiling or sidewall, or in another suitable structural support.</p><p>[0024] Generally, a remote operator <b>109</b>, such as a surgeon or other operator, may use the user console <b>102</b> to remotely manipulate the arms <b>104</b> and/or the attached surgical tools <b>107</b>, e.g., teleoperation. The user console <b>102</b> may be located in the same operating room as the rest of the system <b>100</b>, as shown in FIG. 1. In other environments however, the user console <b>102</b> may be located in an adjacent or nearby room, or it may be at a remote location, e.g., in a different building, city, or country. The user console <b>102</b> may comprise a seat <b>110</b>, one or more user interface devices, for example, foot-operated controls <b>113</b> or handheld user input devices (UID) <b>114</b>, and at least one user display <b>115</b> that is configured to display, for example, a view of the surgical site inside the patient <b>106</b>. In the example user console <b>102</b>, the remote operator <b>109</b> is sitting in the seat <b>110</b> and viewing the user display <b>115</b> while manipulating a foot-operated control <b>113</b> and a handheld UID <b>114</b> in order to remotely control the arms <b>104</b> and the surgical tools <b>107</b> (that are mounted on the distal ends of the arms <b>104</b>).</p><p>[0025] In some variations, the bedside operator <b>108</b> may also operate the system <b>100</b> in an \\\"over the bed\\\" mode, in which the bedside operator <b>108</b> (user) is now at a side of the patient <b>106</b> and is simultaneously manipulating a robotically-driven tool (end effector as attached to the arm <b>104</b>), e.g., with a handheld UID <b>114</b> held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator <b>108</b> may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient <b>106</b>.</p><p>[0026] During an example procedure (surgery), the patient <b>106</b> is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system <b>100</b> are in a stowed configuration or withdrawn configuration (to facilitate access to the surgical site.) Once access is completed, initial positioning or preparation of the robotic system <b>100</b> including its arms <b>104</b> may be performed. Next, the surgery proceeds with the remote operator <b>109</b> at the user console <b>102</b> utilizing the foot-operated controls <b>113</b> and the UIDs <b>114</b> to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, e.g., the bedside operator <b>108</b> who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms <b>104</b>. Non-sterile personnel may also be present to assist the remote operator <b>109</b> at the user console <b>102</b>. When the procedure or surgery is completed, the system <b>100</b> and the user console <b>102</b> may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilization and healthcare record entry or printout via the user console <b>102</b>.</p><p>[0027] In one embodiment, the remote operator <b>109</b> holds and moves the UID <b>114</b> to provide an input command to move a robot arm actuator <b>117</b> in the robotic system <b>100</b>. The UID <b>114</b> may be communicatively coupled to the rest of the robotic system <b>100</b>, e.g., via a console computer system <b>116</b>. Representatively, in some embodiments, UID <b>114</b> may be a portable handheld user input device or controller that is ungrounded with respect to another component of the surgical robotic system. For example, UID <b>114</b> may be ungrounded while either tethered or untethered from the user console. The term \\\"ungrounded\\\" is intended to refer to implementations where, for example, both UIDs are neither mechanically nor kinematically constrained with respect to the user console. For example, a user may hold a UID <b>114</b> in a hand and move freely to any possible position and orientation within space only limited by, for example, a tracking mechanism of the user console. The UID <b>114</b> can generate spatial state signals corresponding to movement of the UID <b>114</b>, e.g. position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator <b>117</b>. The robotic system <b>100</b> may use control signals derived from the spatial state signals, to control proportional motion of the actuator <b>117</b>. In one embodiment, a console processor of the console computer system <b>116</b> receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator <b>117</b> is energized to move a segment or link of the arm <b>104</b>, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID <b>114</b>. Similarly, interaction between the remote operator <b>109</b> and the UID <b>114</b> can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool <b>107</b> to close and grip the tissue of patient <b>106</b>.</p><p>[0028] The surgical robotic system <b>100</b> may include several UIDs <b>114</b>, where respective control signals are generated for each UID that control the actuators and the surgical tool (end effector) of a respective arm <b>104</b>. For example, the remote operator <b>109</b> may move a first UID <b>114</b> to control the motion of an actuator <b>117</b> that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc., in that arm <b>104</b>. Similarly, movement of a second UID <b>114</b> by the remote operator <b>109</b> controls the motion of another actuator <b>117</b>, which in turn moves other linkages, gears, etc., of the robotic system <b>100</b>. The robotic system <b>100</b> may include a right arm <b>104</b> that is secured to the bed or table to the right side of the patient, and a left arm <b>104</b> that is at the left side of the patient. An actuator <b>117</b> may include one or more motors that are controlled so that they drive the rotation of a joint of the arm <b>104</b>, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool <b>107</b> that is attached to that arm. Motion of several actuators <b>117</b> in the same arm <b>104</b> can be controlled by the spatial state signals generated from a particular UID <b>114</b>. The UIDs <b>114</b> can also control motion of respective surgical tool graspers. For example, each UID <b>114</b> can generate a respective grip signal to control motion of an actuator, e.g., a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool <b>107</b> to grip tissue within patient <b>106</b>.</p><p>[0029] In some aspects, the communication between the platform <b>105</b> and the user console <b>102</b> may be through a control tower <b>103</b>, which may translate user commands that are received from the user console <b>102</b> (and more particularly from the console computer system <b>116</b>) into robotic control commands that are transmitted to the arms <b>104</b> on the robotic platform <b>105</b>. The control tower <b>103</b> may also transmit status and feedback from the platform <b>105</b> back to the user console <b>102</b>. The communication connections between the robotic platform <b>105</b>, the user console <b>102</b>, and the control tower <b>103</b> may be via wired and/or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor and/or walls or ceiling of the operating room. The robotic system <b>100</b> may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. It will be appreciated that the operating room scene in FIG. 1 is illustrative and may not accurately represent certain medical practices.</p><p>[0030] In addition, in some aspects, surgical robotic system <b>100</b> may further include one or more interlock detecting mechanisms, devices, components or systems (e.g., trackers, sensors, etc), that can be used to determine whether a teleoperation mode should be engaged or, once engaged, should be disengaged. The interlock detecting mechanisms may detect, for example, whether a user is looking toward or away from the display <b>115</b>, the UID is configured in a usable manner and/or the surgical environment is set up in a usable manner, and in turn, whether a teleoperation mode should be engaged or disengaged. For example, during a teleoperation mode of the surgical robotic system <b>100</b>, in which the user is controlling the surgical tool <b>107</b> using the UID <b>114</b>, the user should be viewing the tool movement on display <b>115</b>. In some cases, however, the user may look away from the display <b>115</b> (intentionally or unintentionally) while still holding the UID <b>114</b> and controlling the surgical tool <b>107</b>. This introduces a risk since the user could move the UID <b>114</b> and, in turn, unintentionally move the tool <b>107</b> while not focusing on the display <b>115</b>. Surgical robotic system <b>100</b> may therefore further include interlock detecting mechanisms that can be used to determine whether the user is focused on the display <b>115</b>, or otherwise interlocked or engaged with the system, in such a way that teleoperation mode is appropriate. The various interlock detecting mechanisms may be coupled to, in communication with, or otherwise associated with components of, the user console <b>102</b>, and will be described in more detail in reference to FIG. 2.</p><p>[0031] It should be understood that \\\"engaging\\\" the teleoperation mode is intended to refer to an operation in which, for example, a UID or foot pedal that is prevented from controlling the surgical instrument, is transitioned to a mode (e.g., a teleoperation mode) in which it can now control the surgical instrument. On the other hand, disengaging the teleoperation mode is intended to refer to an operation which occurs when the system is in a teleoperation mode, and then transitioned to a mode (non-teleoperation mode) in which the UID or foot pedal can no longer control the surgical instrument. For example, teleoperation mode may be disengaged when the system determines that one or more of a number of interlocks required for teleoperation mode is no longer present or met. The interlocks required for teleoperation mode may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner.</p><p>[0032] Referring now to FIG. 2, FIG. 2 illustrates a pictorial view of an exemplary user console including a number of devices, mechanisms, systems and/or components used to detect whether the interlocks required for teleoperation mode are met. Representatively, user console <b>102</b> is shown including a chair <b>110</b> and display <b>115</b>, as previously discussed. In addition, user console <b>102</b> is shown having an optional display <b>202</b> mounted to chair <b>110</b> by a display arm <b>204</b>. For example, display <b>115</b> may be an open display, and display <b>202</b> may be an immersive display. The term \\\"open\\\" display is intended to refer to a display which is designed to allow the user to see outside of the display, for example with their peripheral vision, even when directly facing the display. In addition, in an open display, the user can be a distance from the display screen or not directly in front of the display, and still view a surgical procedure on the display. Therefore, in an open display such as display <b>115</b>, the fact that the user may not be close to the display or have their face directly in front of the display, would not necessarily mean the user is not looking at the display or focused on the surgical procedure. In the case of an open display, in which the user can turn their head and still see the display using their peripheral vision, it is therefore important that turning of the head slightly not be interpreted to mean the user is not focused on the display. Rather, in the case of an open display, there will be some tolerance to such actions and allow for teleoperation mode to continue. This is in contrast to an immersive display such as display <b>202</b>. For example, display <b>202</b> could be a completely immersive display (as in the case of a periscope) that prevents the user from seeing outside of the display screen when they are facing the display and requires the user to be relatively close to the display screen. For example, in the case of an immersive display, the user must have their face relatively close to, and facing, the display screen to use it to view the surgical procedure in progress. If the user pulls their head away from the display screen, or doesn't face the display screen, they can no longer see the display screen, therefore this would typically be interpreted to mean the user is not focused on the screen or not paying sufficient attention to continue in teleoperation mode.</p><p>[0033] To detect whether all the interlocks required for engagement of the teleoperation mode (or continuing in the teleoperation mode) are met, user console <b>102</b> may further include a number of interlock detecting devices, mechanisms, systems and/or components. The interlock detecting mechanisms may include, but are not limited to, an eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, an immersive display deployment sensor <b>214</b>, a UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, UID contact sensor <b>220</b>, and/or a transport sensor <b>222</b>. Each of the interlock detecting mechanisms are configured to detect at least one of a number of conditions (referred to herein as \\\"interlocks\\\") which are required to engage the teleoperation mode. As previously discussed, the interlocks may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner. Table 1 provides an exemplary listing of the specific interlocks which are required to engage a teleoperation mode, and to continue teleoperation mode, once engaged.</p><p><Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Interlock</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze on the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated</TD></TR><TR><TD>toward the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is</TD></TR><TR><TD>rotated toward the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display</TD></TR><TR><TD>is in a use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is inside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace not distorted</TD></TR><TR><TD>and tracker signal is accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is being held by the user</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is not in a transportation</TD></TR><TR><TD>mode</TD></TR><TR></TR></Table></p><p>[0034] If any of interlocks 1-11 are not detected, a teleoperation mode cannot be engaged, or if already engaged, may be automatically disengaged. Said another way, if any one or more of the following conditions listed in Table 2 are determined (e.g., when any of interlocks 1-11 are not detected) teleoperation mode is either not engaged or disengaged.</p><p><Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 2</TD></TR><TR></TR><TR><TD>Condition</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze is outside of the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated</TD></TR><TR><TD>away from the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is</TD></TR><TR><TD>rotated away from the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing away from the</TD></TR><TR><TD>display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is not seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is not resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display</TD></TR><TR><TD>is in a non-use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is outside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace distorted</TD></TR><TR><TD>and tracker signal is not accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is not being held by the user,</TD></TR><TR><TD>or is in a stored configuration</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is in a transportation</TD></TR><TR><TD>mode</TD></TR><TR></TR></Table></p><p>[0035] The interlock detecting mechanisms for detecting, for example, interlocks 1-7 listed in Table 1, which generally relate to whether the user is looking at or focused on the display, may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, chair swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, headrest pressure sensor <b>212</b>, and immersive display deployment sensor <b>214</b>. The interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether the user is holding the controllers in a usable manner, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b> and UID contact sensor <b>220</b>. The interlock detecting mechanisms for detecting interlock 11, which relates to whether the surgical environment is set up in a usable manner, may include transport sensor <b>222</b>, in addition to one or more of the UID sensors <b>216</b>, <b>218</b>, <b>220</b>. Once a condition or interlock is detected by the interlock detecting mechanism, the information is output to a processing component of the surgical robotic system, where it is received as an interlock input and monitored to determine whether teleoperation should continue, should be engaged or should be disengaged. For example, if interlock inputs corresponding to all the required interlocks for teleoperation mode are received, teleoperation mode is engaged. If any interlocks are missing, teleoperation mode is not engaged (if currently in a non-teleoperation mode), or disengaged (if currently in teleoperation mode).</p><p>[0036] Referring now in more detail to each of the interlock detecting mechanisms, eye and/or head tracker <b>206</b> may be a tracking device that is coupled to the display <b>115</b> to track a user's gaze and/or head with respect to the display <b>115</b>, and indicate whether or not the user is looking toward or away from the display. Eye and/or head tracker <b>206</b> may be attached to any portion of display <b>115</b> suitable for tracking a gaze of a user. For example, eye and/or head tracker <b>206</b> may be attached to a housing <b>224</b> of display <b>115</b>, for example, a top wall, a bottom wall, or a side wall of housing <b>224</b>, or integrated within a screen (not shown) mounted within the housing <b>224</b> of display <b>115</b>. Eye and/or head tracker <b>206</b> may include one or more proj ector(s) and camera(s) which face the user and can be used to track a gaze of a user. Representatively, the projector(s) may create a pattern of near-infrared light on the eyes of the user, and the camera(s) may take images of the user's eyes and the pattern. Eye and/or head tracker <b>206</b> may further be programmed to use this information (e.g., execute machine learning, image processing and/or mathematical algorithms) to determine where the user is looking based on a position of each of the user's eyes and/or gaze point or location relative to one another, and display <b>115</b>. For example, when the user is sitting on chair <b>110</b> in front of display <b>115</b>, eye and/or head tracker <b>206</b> may be configured to create a pattern of near-infrared light on the eyes of the user and take images of the user's eyes and the pattern. This information may, in turn, be input to the surgical robotic system processing component (e.g., an interlock input signal) to determine whether the gaze of the user is inside or outside of display <b>115</b>. If the gaze is determined to be inside the display, the user is determined to be looking toward the display. On the other hand, if the gaze is determined to be outside the display, the user is determined to be looking away from the display.</p><p>[0037] In addition, eye and/or head tracker <b>206</b> can also track a position, orientation and/or location of the user's head as an indication of whether the user is looking at the display. For example, tracker <b>206</b> may detect that the user's head is in front of the display and/or the user's face or nose is facing display <b>115</b>, which suggests the user is looking at the display, or that the user's head is not in front of the display and/or their face or nose is turned away from display <b>115</b>, which suggests the user is looking away from the display. In addition, the tracker <b>206</b> may be used to determine automatically if the open display <b>115</b> or the immersive display <b>202</b> should be used for teleoperation.</p><p>[0038] Still further, tracking markers <b>226</b> on the user's glasses and/or tracking markers <b>228</b> on the user's face mask may be used to determine whether the user is looking toward or away from the display <b>115</b>. The tracking markers <b>226</b> on the user's glasses (e.g., 3D glasses) and/or markers <b>228</b> on the face mask (if user is sterile) may be active or passive markers. The markers <b>226</b> and/or <b>228</b> may track if the user's head is rotated toward the screen, indicating the user is looking toward the display. The markers <b>226</b> and/or <b>228</b> may further track whether the head is rotated away from the screen, indicating the user is looking away from the display. In some cases where the markers <b>226</b> are attached to the user's glasses, they may be similar to the previously discussed eye and/or head tracker and include one or more projectors, cameras and/or processing algorithms that can analyze information received from the camera(s) and projector(s) to determine whether the user is looking at the display.</p><p>[0039] Additional interlock mechanisms for detecting whether or not the user is looking toward or away from the display may include a swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, and a headrest pressure sensor <b>212</b>. The swivel sensor <b>208</b> may be any type of sensing device or component suitable for detecting a rotation (or swivel) and/or orientation of the chair <b>110</b>, with respect to, for example, a stationary base <b>230</b>. For example, the swivel sensor <b>208</b> may include a position encoder, switch or the like. The swivel sensor <b>208</b> may then output a corresponding signal to a processing component of the surgical robotic system corresponding to the rotation, position and/or orientation of the chair <b>110</b>. The surgical robotic system can then use this information to determine whether the chair <b>110</b> is facing display <b>115</b>, and therefore a user seated in the chair is facing the display <b>115</b>. If, on the other hand, swivel sensor <b>208</b> detects the chair is facing away from the display, it is determined the user is looking away from the display. The chair pressure sensor <b>210</b> and the headrest pressure sensor <b>212</b> may be pressure sensors which are attached to the chair headrest and chair seat, respectively. The chair pressure sensor <b>210</b> can be used to detect when the user is seated in the chair <b>110</b>, and the headrest sensor <b>212</b> can be used to detect when the user's head is resting on the headrest. Sensors <b>210</b>, <b>212</b> may output a signal indicating that the user is seated in chair <b>110</b> and/or the head is resting on the headrest to the system processing component, which can be used as indicators that the user is looking at the display <b>115</b>. If the sensors <b>210</b>, <b>212</b> do not detect the user seated in the chair <b>110</b>, this is interpreted as an indication the user is not looking toward the display, or looking away from the display. Sensors <b>210</b> and <b>212</b> may be any type of sensor suitable for detecting the user applying pressure to the seat in the particular location in which the sensor is positioned, for example, they may be pressure plates that are mounted within portions of the headrest and seat which would be contacted by a user seated in the chair <b>110</b>. It should be understood, however, if chair <b>110</b> of console <b>102</b> does not swivel, swivel sensor <b>208</b> may be omitted, and interlock 4 of Table 1 (e.g., chair sensor indicates chair is facing the display) may not be required to engage teleoperation mode.</p><p>[0040] In addition, in embodiments where user console <b>102</b> includes immersive display <b>202</b>, an immersive display deployment sensor <b>214</b> to detect a location of the immersive display <b>202</b> may be provided. The immersive display deployment sensor <b>214</b> may be coupled to the immersive display <b>202</b> and include switches, encoders or the like that are operable to determine if display <b>202</b> is in a \\\"use\\\" position (e.g., position suitable for viewing by a user seated in the chair), \\\"non-use\\\" position (e.g., positioned not suitable for viewing by a user seated in the chair) and/or deployed position (e.g., positioned in front of the chair). This information can be used in conjunction with a sensor (e.g., pressure or light gate sensor) to determine if the user's head is in the immersive display <b>202</b>. A teleoperation mode will only be engaged if the immersive display <b>202</b> is in the \\\"use\\\" configuration and the sensor detects that the user's head is in the immersive display <b>202</b>. If display is determined to be in \\\"non-use\\\" position and/or the user's head is not detected in display <b>202</b>, this is interpreted as an indication the user is looking away from the display. It should be understood, however, if immersive display <b>202</b> is omitted, deployment sensor <b>214</b> may be omitted, and interlock 7 of Table 1 (e.g., immersive display sensor indicates immersive display is in a use position) is not required to engage teleoperation mode.</p><p>[0041] In addition, the interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether a user is holding the controllers (e.g., UID) in a usable or non-usable manner or configuration, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be, for example, an optical tracker, a position tracker and/or an encoder, that is coupled to the user console and/or the UID. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be used to track, for example, the position of the UID, whether the user is holding the UID in a usable or non-usable manner, and/or whether the UID is inside or outside of the trackable workspace. If the UID is outside of the workspace, the UID is not being held in a usable manner, and the system should be disengaged. In addition, if the tracking space is distorted and the signal received from the tracker does not give an accurate reading, the UID is considered not to be in a usable manner or in a non-usable configuration, and teleoperation should be disengaged. Still further, the sensor <b>216</b>, <b>218</b> and/or <b>220</b> can be used to track whether the UID is stored within the user console (e.g., within a holster or other holding mechanism) and/or the UID has been dropped. If the UID is stored within the console and/or has been dropped, such that the user is no longer holding the UID, the UID is considered to be in a non-usable configuration, and teleoperation should be disengaged. In some embodiments, one or more of sensors <b>216</b>, <b>218</b>, <b>220</b> may be an optical tracker and/or an absolute encoders. In some embodiments, a position sensor (optical tracker) is mounted on the user console (e.g., near display <b>115</b>) and/or the base of the UID. In some embodiments, the UID is tracked by a sensor coupled to the console or the user. The position sensor can provide the position of the UID relative to a user and/or a ground reference point (not shown). In some embodiments, more than one (e.g., a plurality, several) tracking technologies are used in conjunction.</p><p>[0042] In addition, the interlock detecting mechanisms for detecting, for example, interlock 11 listed in Table 1, which generally relates to whether the surgical environment is set up in a usable manner or non-usable manner, may include transport sensor <b>222</b>. Transport sensor <b>222</b> may be a sensor which detects when the surgical robotic system is transitioned to a transportation configuration or mode (e.g., console <b>102</b> can be moved), by sensing, for example, that the brakes are released. If a transition to a transportation configuration is detected, the transportation interlock is no longer met, and the surgical environment is considered to be a non-usable configuration and teleoperation mode should be disengaged. If, on the other hand, the transport sensor <b>222</b> determines the system is not in a transportation configuration (e.g., the brakes are engaged), the surgical environment is in a usable configuration, and teleoperation can be engaged.</p><p>[0043] Additional interlock detecting mechanisms may include the use of the eye tracking aspects previously discussed for determining interpupillary distance (IPD) for the immersive display <b>202</b>. For example, given the position of the center of the eyeballs from the eye tracker, the IPD can be calculated for the immersive display. This may remove a requirement for the user to manually adjust the immersive display IPD.</p><p>[0044] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment. Representatively, process <b>300</b> includes the operations of receiving a plurality of interlock inputs (block <b>302</b>). The plurality of interlock inputs may be one or more of the interlocks 1-11 received from the interlock detecting mechanisms previously discussed in reference to FIG. 2 and Table 1. Based on the plurality of interlock inputs received, the system then determines whether a user is looking toward or away from the display (block <b>304</b>). As previously discussed, the interlocks indicating that the user is looking toward the display may be interlocks 1-7 listed in Table 1. If any one of interlocks 1-7 are not met or satisfied, in other words at least one of conditions 1-7 listed in Table 2 is detected, it is determined that the user is not looking toward the display (e.g., looking away) and teleoperation mode is not engaged (block <b>306</b>). On the other hand, if all of interlocks 1-7 listed in Table <b>1</b> are met or satisfied, it is determined at operation <b>304</b> that the user is looking toward the display and process <b>300</b> continues to operation <b>308</b>.</p><p>[0045] At operation <b>308</b>, the system determines whether a user interface device is positioned in a usable or non-usable manner. A user interface device is considered positioned in a usable manner when it is in a position suitable for a user to control a surgical instrument in a teleoperation mode. The user interface device may be considered to be in a non-usable manner when it is not in a position suitable for the user to control the surgical instrument. Representatively, the user interface device is considered positioned in a usable manner if all of the interlocks 8-10 listed in Table 1 are met or satisfied, and the process <b>300</b> continues to operation <b>310</b>. For example, if UID <b>114</b> is inside a trackable workspace, the tracking workspace is not distorted and the UID <b>114</b> is being held by the user, the UID <b>114</b> is positioned in a usable manner. On the other hand, if it is determined at operation <b>308</b> that one of conditions 8-10 listed in Table 2 are detected, namely the UID is outside of the trackable workspace and the information received from the tracker is not accurate, the tracking space is distorted and the signal received from the tracker does not give an accurate reading, or the UID is not being held by a user (e.g., the UID is stored or dropped), the system determines the user interface device is not positioned in a usable manner, or is positioned in a non-usable manner, and does not engage teleoperation mode.</p><p>[0046] In operation <b>310</b>, the system determines whether the surgical workspace is configured in a usable manner or a non-usable manner. The surgical workspace (e.g., the user console, robotic arms, surgical table, etc) may be considered to be positioned or configured in a usable manner when components of the surgical workspace are properly configured for a user to control a surgical instrument in a teleoperation mode. For example, the surgical workspace is considered to be configured in a usable manner if the interlock 11 listed in Table 1 is met or satisfied. For example, if the user console, chair, display or other associated component is in a fixed configuration (e.g., not in a transportation configuration in which the brakes are released), the surgical workspace is considered to be properly configured. If the surgical workspace is configured in a usable manner, the process continues to operation <b>312</b> and teleoperation mode is engaged. If, on the other hand, it is determined at operation <b>310</b> that the system is transitioned to a transportation configuration (e.g., the brakes are released), in other words condition 11 of Table 2 is detected, the surgical workspace is not configured in a non-usable manner for teleoperation mode, and the process proceeds to operation <b>306</b> and teleoperation mode is not engaged.</p><p>[0047] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment. Representatively, similarly to process <b>300</b>, process <b>400</b> includes the operation of receiving a plurality of interlock inputs (block <b>402</b>), for example, interlocks 1-11 previously discussed in reference to Table 1. When all of the interlock inputs required for teleoperation mode are received (e.g., the user is looking toward or otherwise focused on the display), a teleoperation mode is engaged at operation <b>404</b>. Process <b>400</b> then continues to monitor whether all of the interlock inputs are still present at operation <b>406</b>. As long as the interlocks continue to be detected, teleoperation mode will continue (block <b>410</b>). If, on the other hand, it is determined at operation <b>406</b> that one or more of the interlock inputs is no longer detected, in other words one or more of conditions 1-11 listed in Table 2 are present, the system transitions out of the teleoperation mode at operation <b>408</b>, for example to a non-teleoperation mode in which the user interface device is prevented from controlling a surgical instrument.</p><p>[0048] FIG. 5 is a block diagram of a computer portion of a surgical robotic system, which is operable to implement the previously discussed operations, in accordance with an embodiment. The exemplary surgical robotic system <b>500</b> may include a user console <b>102</b>, a surgical robot <b>120</b>, and a control tower <b>103</b>. The surgical robotic system <b>500</b> may include other or additional hardware components; thus, the diagram is provided by way of example and not a limitation to the system architecture.</p><p>[0049] As described above, the user console <b>102</b> may include console computers <b>511</b>, one or more UIDs <b>512</b>, console actuators <b>513</b>, displays <b>514</b>, foot pedals <b>516</b>, console computers <b>511</b> and a network interface <b>518</b>. In addition, user console <b>102</b> may include a number of interlock detecting mechanisms, devices, or components, for example, a UID tracker(s) <b>515</b>, a display tracker(s) <b>517</b> and a console tracker(s) <b>519</b>, for detecting whether the previously discussed interlocks required for teleoperation are satisfied. For example, UID tracker(s) <b>515</b> may include the previously discussed UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. Display tracker(s) <b>517</b> may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, and/or an immersive display deployment sensor <b>214</b>. Console tracker(s) may include transport sensor <b>222</b>. The interlock information detected and/or tracked by any one or more of the trackers may be monitored and communicated to the console computer <b>511</b>, and dispatched to the control tower <b>103</b> via the network interface <b>518</b>, so that the system can determine whether to engage and/or disengage a teleoperation mode.</p><p>[0050] It should further be understood that a user or surgeon sitting at the user console <b>102</b> can adjust ergonomic settings of the user console <b>102</b> manually, or the settings can be automatically adjusted according to user profile or preference. The manual and automatic adjustments may be achieved through driving the console actuators <b>513</b> based on user input or stored configurations by the console computers <b>511</b>. The user may perform robot-assisted surgeries by controlling the surgical robot <b>120</b> using one or more master UIDs <b>512</b> and foot pedals <b>516</b>. Positions and orientations of the UIDs <b>512</b> are continuously tracked by the UID tracker <b>515</b>, and status changes are recorded by the console computers <b>511</b> as user input and dispatched to the control tower <b>103</b> via the network interface <b>518</b>. Real-time surgical video of patient anatomy, instrumentation, and relevant software apps can be presented to the user on the high resolution 3D displays <b>514</b> including open or immersive displays.</p><p>[0051] The user console <b>102</b> may be communicatively coupled to the control tower <b>103</b>. The user console also provides additional features for improved ergonomics. For example, the user console may be an open architecture system including an open display, although an immersive display, in some cases, may be provided. Furthermore, a highly-adjustable seat for surgeons and master UIDs tracked through electromagnetic or optical trackers are included at the user console <b>102</b> for improved ergonomics.</p><p>[0052] The control tower <b>103</b> can be a mobile point-of-care cart housing touchscreen displays, computers that control the surgeon's robotically-assisted manipulation of instruments, safety systems, graphical user interface (GUI), light source, and video and graphics computers. As shown in FIG. 5, the control tower <b>103</b> may include central computers <b>531</b> including at least a visualization computer, a control computer, and an auxiliary computer, various displays <b>533</b> including a team display and a nurse display, and a network interface <b>518</b> coupling the control tower <b>103</b> to both the user console <b>102</b> and the surgical robot <b>120</b>. The control tower <b>103</b> may offer additional features for user convenience, such as the nurse display touchscreen, soft power and E-hold buttons, user-facing USB for video and still images, and electronic caster control interface. The auxiliary computer may also run a real-time Linux, providing logging/monitoring and interacting with cloud-based web services.</p><p>[0053] The surgical robot <b>120</b> may include an articulated operating table <b>524</b> with a plurality of integrated arms <b>522</b> that can be positioned over the target patient anatomy. A suite of compatible tools <b>523</b> can be attached to or detached from the distal ends of the arms <b>522</b>, enabling the surgeon to perform various surgical procedures. The surgical robot <b>120</b> may also comprise control interface <b>525</b> for manual control of the arms <b>522</b>, table <b>524</b>, and tools <b>523</b>. The control interface can include items such as, but not limited to, remote controls, buttons, panels, and touchscreens. Other accessories such as trocars (sleeves, seal cartridge, and obturators) and drapes may also be needed to perform procedures with the system. In some variations, the plurality of the arms <b>522</b> includes four arms mounted on both sides of the operating table <b>524</b>, with two arms on each side. For certain surgical procedures, an arm mounted on one side of the table can be positioned on the other side of the table by stretching out and crossing over under the table and arms mounted on the other side, resulting in a total of three arms positioned on the same side of the table <b>524</b>. The surgical tool can also comprise table computers <b>521</b> and a network interface <b>518</b>, which can place the surgical robot <b>120</b> in communication with the control tower <b>103</b>.</p><p>[0054] In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. A method for engaging and disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system;<BR />determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner;<BR />in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and<BR />in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The method of claim 1, wherein the user interface devices comprise at least one of a handheld user input device and a foot pedal.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>3</b>. The method of claim 1 wherein the display is an open display.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>4</b>. The method of claim 1 wherein the one or more interlock detection components comprise at least one of an eye and/or head tracker, a tracking marker on a user&apos;s glasses, a tracking marker on a user&apos;s face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>5</b>. The method of claim 1 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward the display;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair associated of the surgical robotic system is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>6</b>. The method of claim 1 wherein the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside the surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>7</b>. The method of claim 1 wherein at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner comprises: <BR />detecting the surgical robotic system is not in a transportation configuration.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"yes\\\"><p><b>8</b>. A method for disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and<BR />in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"8\\\"><p><b>9</b>. The method of claim 8 wherein determining the user is looking away from the display comprises at least one of the following: <BR />detecting a user gaze is outside of the display;<BR />detecting a user head is rotated away from the display;<BR />detecting a chair associated of the surgical robotic system is facing away from the display;<BR />detecting a user is not seated on the chair; and<BR />detecting the display is in a non-use position.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>10</b>. The method of claim 9 wherein determining the surgical robotic system is configured in a non-usable manner comprises at least one of the following: <BR />detecting at least one user interface device is outside a surgical workspace;<BR />detecting a received user interface device location is inaccurate; and<BR />detecting at least one user interface device is dropped by the user.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"9\\\"><p><b>11</b>. The method of claim 9 wherein determining the surgical workspace of the surgical robotic system is configured in a non-usable manner comprises: <BR />detecting the surgical robotic system is in a transportation configuration.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"yes\\\"><p><b>12</b>. A surgical robotic system, comprising: <BR />a surgical instrument;<BR />a user console comprising a display, an interlock detecting component, and a user interface device; and<BR />one or more processors communicatively coupled to the interlock detecting component, the processors configured to:<BR />receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device;<BR />determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and<BR />transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>13</b>. The system of claim 12 wherein the display is an open display.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>14</b>. The system of claim 12 wherein the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>15</b>. The system of claim 14 wherein the interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display comprise: least one of an eye and/or a head tracker, a tracking marker on a user&apos;s glasses, a tracking marker on a user&apos;s face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>16</b>. The system of claim 14 wherein the interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner comprise: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>17</b>. The system of claim 14 wherein the interlock detecting component for detecting the interlock indicating the surgical workspace is configured in a usable manner comprises a transport sensor.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>18</b>. The system of claim 14 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward the display;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair associated of the surgical robotic system is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>19</b>. The system of claim 14 wherein the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside a surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"14\\\"><p><b>20</b>. The system of claim 14 wherein at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner comprises: <BR />detecting the surgical robotic system is not in a transportation configuration.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/00\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/00\",\n",
      "\n",
      "\"B25J9/16\",\n",
      "\n",
      "\"G06F3/01\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2017/00212\",\n",
      "\n",
      "\"A61B2017/00216\",\n",
      "\n",
      "\"A61B2034/305\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"A61B34/37\",\n",
      "\n",
      "\"A61B34/74\",\n",
      "\n",
      "\"A61B90/03\",\n",
      "\n",
      "\"B25J9/0009\",\n",
      "\n",
      "\"B25J9/1689\",\n",
      "\n",
      "\"G06F3/012\",\n",
      "\n",
      "\"G06F3/013\",\n",
      "\n",
      "\"G06F3/017\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"VERB SURGICAL INC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190517,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20190517,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FEPP\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"FEE PAYMENT PROCEDURE\",\n",
      "\n",
      "\"details\": \"Text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20190614,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20190612, OldParty: SECURITY INTEREST;ASSIGNOR:VERB SURGICAL INC.;REEL/FRAME:049474/0591, Owner: JOHNSON & JOHNSON INNOVATION - JJDC, INC., NEW JERSEY, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20190612\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"SECURITY INTEREST;ASSIGNOR:VERB SURGICAL INC.;REEL/FRAME:049474/0591\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"JOHNSON & JOHNSON INNOVATION - JJDC, INC., NEW JERSEY\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20190614,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20190612, OldParty: SECURITY INTEREST;ASSIGNOR:VERB SURGICAL INC.;REEL/FRAME:049474/0591, Owner: VERILY LIFE SCIENCES LLC, CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20190612\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"SECURITY INTEREST;ASSIGNOR:VERB SURGICAL INC.;REEL/FRAME:049474/0591\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"VERILY LIFE SCIENCES LLC, CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20190620,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"OldParty: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SAVALL, JOAN;MILLER, DENISE ANN;FREIIN VON KAPRI, ANETTE LIA;AND OTHERS;SIGNING DATES FROM 20190401 TO 20190612;REEL/FRAME:049536/0198, Owner: VERB SURGICAL INC., CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SAVALL, JOAN;MILLER, DENISE ANN;FREIIN VON KAPRI, ANETTE LIA;AND OTHERS;SIGNING DATES FROM 20190401 TO 20190612;REEL/FRAME:049536/0198\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"VERB SURGICAL INC., CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20200102,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20200103,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20200103,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20200220,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20200218, OldParty: RELEASE BY SECURED PARTY;ASSIGNOR:VERILY LIFE SCIENCES LLC;REEL/FRAME:051986/0252, Owner: VERB SURGICAL INC., CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20200218\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RELEASE BY SECURED PARTY;ASSIGNOR:VERILY LIFE SCIENCES LLC;REEL/FRAME:051986/0252\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"VERB SURGICAL INC., CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20200220,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AS\",\n",
      "\n",
      "\"eventgroup\": \"LSAS\",\n",
      "\n",
      "\"event\": \"ASSIGNMENT\",\n",
      "\n",
      "\"details\": \"Effective date: 20200218, OldParty: RELEASE BY SECURED PARTY;ASSIGNOR:JOHNSON & JOHNSON INNOVATION - JJDC, INC.;REEL/FRAME:051983/0028, Owner: VERB SURGICAL INC., CALIFORNIA, PartyType: owner\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"20200218\",\n",
      "\n",
      "\"Name\": \"Effective date\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RELEASE BY SECURED PARTY;ASSIGNOR:JOHNSON & JOHNSON INNOVATION - JJDC, INC.;REEL/FRAME:051983/0028\",\n",
      "\n",
      "\"Name\": \"OldParty\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"VERB SURGICAL INC., CALIFORNIA\",\n",
      "\n",
      "\"Name\": \"Owner\"\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"owner\",\n",
      "\n",
      "\"Name\": \"PartyType\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20201119,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20211004,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20211004,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NON FINAL ACTION MAILED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220107,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RSP4\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Response to Non-Final Office Action Entered and Forwarded to Examiner\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220107,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220124,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220303,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"AWA4\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Awaiting TC Resp., Issue Fee Not Paid\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220303,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: AWAITING TC RESP., ISSUE FEE NOT PAID\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"AWAITING TC RESP., ISSUE FEE NOT PAID\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220308,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NAM1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Notice of Allowance Mailed -- Application Received in Office of Publications\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220308,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220420,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPR1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Received\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220421,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PPV1\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"Publications -- Issue Fee Payment Verified\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220421,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220504,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"PAT1\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"Patented Case\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20190415992\",\n",
      "\n",
      "\"ad\": 20190517,\n",
      "\n",
      "\"pn\": \"US2020360096\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220504,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STCF\",\n",
      "\n",
      "\"eventgroup\": \"LSGT\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT GRANT\",\n",
      "\n",
      "\"details\": \"Text: PATENTED CASE\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"PATENTED CASE\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "],\n",
      "\n",
      "\"Timeline\": \"https://www.patbase.com/lstimeline/?family=82037919\"\n",
      "\n",
      "}\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"CountryCode\": \"US\",\n",
      "\n",
      "\"ApplicationNumber\": \"US20220725351\",\n",
      "\n",
      "\"Titles\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"INTERLOCK MECHANISMS TO DISENGAGE AND ENGAGE A TELEOPERATION MODE\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Abstracts\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"A method for engaging and disengaging a surgical instrument of a surgical robotic system comprising: receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode.\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Descriptions\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<relapp><p>CROSS-REFERENCE TO RELATED APPLICATIONS</p><p>[0001] This application is a continuation of co-pending U.S. application Ser. No. 16/415,992 filed May 17, 2019, which is incorporated by reference herein.</p></relapp><shortsum><p>FIELD</p><p>[0002] Embodiments related surgical robotic systems, are disclosed. More particularly, embodiments related to interlock mechanisms that disengage and engage a teleoperation mode, are disclosed.</p><p>BACKGROUND</p><p>[0003] Endoscopic surgery involves looking into a patient's body and performing surgery inside the body using endoscopes and other surgical tools. For example, laparoscopic surgery can use a laparoscope to access and view an abdominal cavity. Endoscopic surgery can be performed using manual tools and/or a surgical robotic system having robotically-assisted tools.</p><p>[0004] A surgical robotic system may be remotely operated by a surgeon to command a robotically-assisted tool located at an operating table. Such operation of a robotically-assisted tool remotely by a surgeon may be commonly referred to as teleoperation. For example, the surgeon may use a computer console located in the operating room, or it may be located in a different city, to command a robot to manipulate the surgical tool mounted on the operating table. The robotically-controlled surgical tool can be an endoscope mounted on a robotic arm. Accordingly, the surgical robotic system may be used by the remote surgeon to perform an endoscopic surgery.</p><p>[0005] The surgeon may provide input commands to the surgical robotic system, and one or more processors of the surgical robotic system can control system components in response to the input commands. For example, the surgeon may hold in her hand a user input device such as a joystick or a computer mouse that she manipulates to generate control signals to cause motion of the surgical robotic system components, e.g., an actuator, a robotic arm, and/or a surgical tool of the robotic system.</p><p>SUMMARY</p><p>[0006] During teleoperation with an open display in which the user can view their surroundings (as compared to a periscope type display) there is the possibility that the surgeon is looking away from the screen but still holding the user input devices that control the robotic tools. This introduces a risk since the surgeon could move the user input devices and unintentionally move the tools while not focusing on the screen. Therefore, in some aspects, the processes disclosed herein provide methods for determining whether a number of interlocks required for teleoperation are met, and therefore the user is looking at the open display (or an immersive display such as a periscope) and focused on teleoperation, such that teleoperation mode may be engaged or continue. In this aspect, the system detects a number of interlock parameters, conditions, inputs or the like, and then determines based on the detection of the interlock parameters, whether the system should disengage or engage teleoperation. In general, the interlock mechanisms or parameters are designed to determine the following (1) is the user looking at the screen, (2) is the user holding the user interface device in a usable manner, and (3) is the environment set up for teleoperation mode. If the answer to all of these conditions is yes, than teleoperation mode is engaged, if the answer to any of them is no, teleoperation mode may be disengaged.</p><p>[0007] Representatively, in one aspect, a method for engaging and disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system; determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner; in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices; and in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument. The user interface devices may include at least one of a handheld user input device and a foot pedal. The display may be an open display. The one or more interlock detection components may include at least one of an eye and/or head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor, an immersive display deployment sensor, a user interface device tracking sensor, a user interface device storage sensor, a user interface device contact sensor, or a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display may include detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner may include detecting at least one user interface device is inside the surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some cases, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner may include detecting the surgical robotic system is not in a transportation configuration.</p><p>[0008] In another aspect, a method for disengaging a surgical instrument of a surgical robotic system is disclosed. The method may include determining, by one or more processors communicatively coupled to the surgical robotic system, that at least one interlock input previously received from one or more interlock detection components of the surgical robotic system is no longer present and therefore (1) a user is looking away from a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a non-usable manner, or (3) a surgical workspace of the surgical robotic system is configured in a non-usable manner; and in response to determining that the interlock input is no longer present, transition the surgical robotic system out of a teleoperation mode such that a user interface device of the surgical robotic system is prevented from controlling a surgical instrument. In some aspects, determining the user is looking away from the display may include at least one of the following: detecting a user gaze is outside of the display; detecting a user head is rotated away from the display; detecting a chair associated of the surgical robotic system is facing away from the display; detecting a user is not seated on the chair; and detecting the display is in a non-use position. In some aspects, determining the surgical robotic system is configured in a non-usable manner may include at least one of the following: detecting at least one user interface device is outside a surgical workspace; detecting a received user interface device location is inaccurate; and detecting at least one user interface device is dropped by the user. In some aspects, determining the surgical workspace of the surgical robotic system is configured in a non-usable manner may include detecting the surgical robotic system is in a transportation configuration.</p><p>[0009] In still further aspects, a surgical robotic system is disclosed. The system may include a surgical instrument; a user console comprising a display, an interlock detecting component, and a user interface device; and one or more processors communicatively coupled to the interlock detecting component, the processors configured to: receive a plurality of interlock inputs from the interlock detecting component, wherein all of the plurality of interlock inputs are required for engagement of a teleoperation mode in which the surgical instrument is controlled by the user interface device; determine at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present; and transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument. In some aspects, the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner. The interlock detecting component for detecting the plurality of interlocks indicating a user is looking toward a display may include: least one of an eye and/or a head tracker, a tracking marker on a user's glasses, a tracking marker on a user's face mask, a chair swivel sensor, a chair pressure sensor, a headrest pressure sensor or an immersive display deployment sensor. The interlock detecting components for detecting the interlocks indicating at least one or more user interface devices are configured in a usable manner may include: a user interface device tracking sensor, a user interface device storage sensor, or a user interface device contact sensor. The interlock detecting component for detecting the interlock indicating the surgical workspace may be configured in a usable manner comprises a transport sensor. In some aspects, the plurality of interlock inputs indicating a user is looking toward the display comprise: detecting a user gaze is toward the display; detecting a user head is rotated toward the display; detecting a chair associated of the surgical robotic system is facing the display; detecting a user is seated on the chair; and detecting the display is in a use position. In still further aspects, the plurality of interlock inputs indicating at least one or more user interfaces device of the surgical robotic system is configured in a usable manner comprise: detecting at least one user interface device is inside a surgical workspace; detecting an accurate user interface device location is received; and detecting at least one user interface device is being held by the user. In some aspects, at least one of the plurality of interlock inputs indicating the surgical workspace of the surgical robotic system is configured in a usable manner includes detecting the surgical robotic system is not in a transportation configuration.</p><p>[0010] The above summary does not include an exhaustive list of all aspects of the present invention. It is contemplated that the invention includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary.</p></shortsum><shortdesdrw><p>BRIEF DESCRIPTION OF THE DRAWINGS</p><p>[0011] The embodiments of the invention are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \\\"an\\\" or \\\"one\\\" embodiment of the invention in this disclosure are not necessarily to the same embodiment, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one embodiment of the invention, and not all elements in the figure may be required for a given embodiment.</p><p>[0012] FIG. 1 is a pictorial view of an example surgical robotic system in an operating arena, in accordance with an embodiment.</p><p>[0013] FIG. 2 is a pictorial view of a user console, in accordance with an embodiment.</p><p>[0014] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment.</p><p>[0015] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment.</p><p>[0016] FIG. 5 is a block diagram of a computer portion of a surgical robotic system, in accordance with an embodiment.</p></shortdesdrw><p>DETAILED DESCRIPTION</p><p>[0017] In various embodiments, description is made with reference to the figures. However, certain embodiments may be practiced without one or more of these specific details, or in combination with other known methods and configurations. In the following description, numerous specific details are set forth, such as specific configurations, dimensions, and processes, in order to provide a thorough understanding of the embodiments. In other instances, well-known processes and manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the description. Reference throughout this specification to \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, means that a particular feature, structure, configuration, or characteristic described is included in at least one embodiment. Thus, the appearance of the phrase \\\"one embodiment,\\\" \\\"an embodiment,\\\" or the like, in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p>[0018] In addition, the terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the invention. Spatially relative terms, such as \\\"beneath\\\", \\\"below\\\", \\\"lower\\\", \\\"above\\\", \\\"upper\\\", and the like may be used herein for ease of description to describe one element's or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as \\\"below\\\" or \\\"beneath\\\" other elements or features would then be oriented \\\"above\\\" the other elements or features. Thus, the exemplary term \\\"below\\\" can encompass both an orientation of above and below. The device may be otherwise oriented (e.g., rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.</p><p>[0019] As used herein, the singular forms \\\"a\\\", \\\"an\\\", and \\\"the\\\" are intended to include the plural forms as well, unless the context indicates otherwise. It will be further understood that the terms \\\"comprises\\\" and/or \\\"comprising\\\" specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.</p><p>[0020] The terms \\\"or\\\" and \\\"and/or\\\" as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, \\\"A, B or C\\\" or \\\"A, B and/or C\\\" mean \\\"any of the following: A; B; C; A and B; A and C; B and C; A, B and C.\\\" An exception to this definition will occur only when a combination of elements, functions, steps or acts are in some way inherently mutually exclusive.</p><p>[0021] Moreover, the use of relative terms throughout the description may denote a relative position or direction. For example, \\\"distal\\\" may indicate a first direction away from a reference point, e.g., away from a user. Similarly, \\\"proximal\\\" may indicate a location in a second direction opposite to the first direction, e.g., toward the user. Such terms are provided to establish relative frames of reference, however, and are not intended to limit the use or orientation of any particular surgical robotic component to a specific configuration described in the various embodiments below.</p><p>[0022] Referring to FIG. 1, this is a pictorial view of an example surgical robotic system <b>100</b> in an operating arena. The surgical robotic system <b>100</b> includes a user console <b>102</b>, a control tower <b>103</b>, and one or more surgical robots <b>120</b>, including robotic arms <b>104</b> at a surgical robotic platform <b>105</b>, e.g., an operating table, a bed, etc. The system <b>100</b> can incorporate any number of devices, tools, or accessories used to perform surgery on a patient <b>106</b>. For example, the system <b>100</b> may include one or more surgical tools <b>107</b> used to perform surgery. A surgical tool <b>107</b> may be an end effector that is attached to a distal end of a surgical arm <b>104</b>, for executing a surgical procedure.</p><p>[0023] Each surgical tool <b>107</b> may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool <b>107</b> may be a tool used to enter, view, or manipulate an internal anatomy of the patient <b>106</b>. In an embodiment, the surgical tool <b>107</b> is a grasper that can grasp tissue of the patient. The surgical tool <b>107</b> may be controlled manually, by a bedside operator <b>108</b>; or it may be controlled robotically, via actuated movement of the surgical robotic arm <b>104</b> to which it is attached. The robotic arms <b>104</b> are shown as a table-mounted system, but in other configurations the arms <b>104</b> may be mounted in a cart, ceiling or sidewall, or in another suitable structural support.</p><p>[0024] Generally, a remote operator <b>109</b>, such as a surgeon or other operator, may use the user console <b>102</b> to remotely manipulate the arms <b>104</b> and/or the attached surgical tools <b>107</b>, e.g., teleoperation. The user console <b>102</b> may be located in the same operating room as the rest of the system <b>100</b>, as shown in FIG. 1. In other environments however, the user console <b>102</b> may be located in an adjacent or nearby room, or it may be at a remote location, e.g., in a different building, city, or country. The user console <b>102</b> may comprise a seat <b>110</b>, one or more user interface devices, for example, foot-operated controls <b>113</b> or handheld user input devices (UID) <b>114</b>, and at least one user display <b>115</b> that is configured to display, for example, a view of the surgical site inside the patient <b>106</b>. In the example user console <b>102</b>, the remote operator <b>109</b> is sitting in the seat <b>110</b> and viewing the user display <b>115</b> while manipulating a foot-operated control <b>113</b> and a handheld UID <b>114</b> in order to remotely control the arms <b>104</b> and the surgical tools <b>107</b> (that are mounted on the distal ends of the arms <b>104</b>).</p><p>[0025] In some variations, the bedside operator <b>108</b> may also operate the system <b>100</b> in an \\\"over the bed\\\" mode, in which the bedside operator <b>108</b> (user) is now at a side of the patient <b>106</b> and is simultaneously manipulating a robotically-driven tool (end effector as attached to the arm <b>104</b>), e.g., with a handheld UID <b>114</b> held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator <b>108</b> may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient <b>106</b>.</p><p>[0026] During an example procedure (surgery), the patient <b>106</b> is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system <b>100</b> are in a stowed configuration or withdrawn configuration (to facilitate access to the surgical site.) Once access is completed, initial positioning or preparation of the robotic system <b>100</b> including its arms <b>104</b> may be performed. Next, the surgery proceeds with the remote operator <b>109</b> at the user console <b>102</b> utilizing the foot-operated controls <b>113</b> and the UIDs <b>114</b> to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, e.g., the bedside operator <b>108</b> who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms <b>104</b>. Non-sterile personnel may also be present to assist the remote operator <b>109</b> at the user console <b>102</b>. When the procedure or surgery is completed, the system <b>100</b> and the user console <b>102</b> may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilisation and healthcare record entry or printout via the user console <b>102</b>.</p><p>[0027] In one embodiment, the remote operator <b>109</b> holds and moves the UID <b>114</b> to provide an input command to move a robot arm actuator <b>117</b> in the robotic system <b>100</b>. The UID <b>114</b> may be communicatively coupled to the rest of the robotic system <b>100</b>, e.g., via a console computer system <b>116</b>. Representatively, in some embodiments, UID <b>114</b> may be a portable handheld user input device or controller that is ungrounded with respect to another component of the surgical robotic system. For example, UID <b>114</b> may be ungrounded while either tethered or untethered from the user console. The term \\\"ungrounded\\\" is intended to refer to implementations where, for example, both UIDs are neither mechanically nor kinematically constrained with respect to the user console. For example, a user may hold a UID <b>114</b> in a hand and move freely to any possible position and orientation within space only limited by, for example, a tracking mechanism of the user console. The UID <b>114</b> can generate spatial state signals corresponding to movement of the UID <b>114</b>, e.g. position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator <b>117</b>. The robotic system <b>100</b> may use control signals derived from the spatial state signals, to control proportional motion of the actuator <b>117</b>. In one embodiment, a console processor of the console computer system <b>116</b> receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator <b>117</b> is energized to move a segment or link of the arm <b>104</b>, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID <b>114</b>. Similarly, interaction between the remote operator <b>109</b> and the UID <b>114</b> can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool <b>107</b> to close and grip the tissue of patient <b>106</b>.</p><p>[0028] The surgical robotic system <b>100</b> may include several UIDs <b>114</b>, where respective control signals are generated for each UID that control the actuators and the surgical tool (end effector) of a respective arm <b>104</b>. For example, the remote operator <b>109</b> may move a first UID <b>114</b> to control the motion of an actuator <b>117</b> that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc., in that arm <b>104</b>. Similarly, movement of a second UID <b>114</b> by the remote operator <b>109</b> controls the motion of another actuator <b>117</b>, which in turn moves other linkages, gears, etc., of the robotic system <b>100</b>. The robotic system <b>100</b> may include a right arm <b>104</b> that is secured to the bed or table to the right side of the patient, and a left arm <b>104</b> that is at the left side of the patient. An actuator <b>117</b> may include one or more motors that are controlled so that they drive the rotation of a joint of the arm <b>104</b>, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool <b>107</b> that is attached to that arm. Motion of several actuators <b>117</b> in the same arm <b>104</b> can be controlled by the spatial state signals generated from a particular UID <b>114</b>. The UIDs <b>114</b> can also control motion of respective surgical tool graspers. For example, each UID <b>114</b> can generate a respective grip signal to control motion of an actuator, e.g., a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool <b>107</b> to grip tissue within patient <b>106</b>.</p><p>[0029] In some aspects, the communication between the platform <b>105</b> and the user console <b>102</b> may be through a control tower <b>103</b>, which may translate user commands that are received from the user console <b>102</b> (and more particularly from the console computer system <b>116</b>) into robotic control commands that are transmitted to the arms <b>104</b> on the robotic platform <b>105</b>. The control tower <b>103</b> may also transmit status and feedback from the platform <b>105</b> back to the user console <b>102</b>. The communication connections between the robotic platform <b>105</b>, the user console <b>102</b>, and the control tower <b>103</b> may be via wired and/or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor and/or walls or ceiling of the operating room. The robotic system <b>100</b> may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. It will be appreciated that the operating room scene in FIG. 1 is illustrative and may not accurately represent certain medical practices.</p><p>[0030] In addition, in some aspects, surgical robotic system <b>100</b> may further include one or more interlock detecting mechanisms, devices, components or systems (e.g., trackers, sensors, etc), that can be used to determine whether a teleoperation mode should be engaged or, once engaged, should be disengaged. The interlock detecting mechanisms may detect, for example, whether a user is looking toward or away from the display <b>115</b>, the UID is configured in a usable manner and/or the surgical environment is set up in a usable manner, and in turn, whether a teleoperation mode should be engaged or disengaged. For example, during a teleoperation mode of the surgical robotic system <b>100</b>, in which the user is controlling the surgical tool <b>107</b> using the UID <b>114</b>, the user should be viewing the tool movement on display <b>115</b>. In some cases, however, the user may look away from the display <b>115</b> (intentionally or unintentionally) while still holding the UID <b>114</b> and controlling the surgical tool <b>107</b>. This introduces a risk since the user could move the UID <b>114</b> and, in turn, unintentionally move the tool <b>107</b> while not focusing on the display <b>115</b>. Surgical robotic system <b>100</b> may therefore further include interlock detecting mechanisms that can be used to determine whether the user is focused on the display <b>115</b>, or otherwise interlocked or engaged with the system, in such a way that teleoperation mode is appropriate. The various interlock detecting mechanisms may be coupled to, in communication with, or otherwise associated with components of, the user console <b>102</b>, and will be described in more detail in reference to FIG. 2.</p><p>[0031] It should be understood that \\\"engaging\\\" the teleoperation mode is intended to refer to an operation in which, for example, a UID or foot pedal that is prevented from controlling the surgical instrument, is transitioned to a mode (e.g., a teleoperation mode) in which it can now control the surgical instrument. On the other hand, disengaging the teleoperation mode is intended to refer to an operation which occurs when the system is in a teleoperation mode, and then transitioned to a mode (non-teleoperation mode) in which the UID or foot pedal can no longer control the surgical instrument. For example, teleoperation mode may be disengaged when the system determines that one or more of a number of interlocks required for teleoperation mode is no longer present or met. The interlocks required for teleoperation mode may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner.</p><p>[0032] Referring now to FIG. 2, FIG. 2 illustrates a pictorial view of an exemplary user console including a number of devices, mechanisms, systems and/or components used to detect whether the interlocks required for teleoperation mode are met. Representatively, user console <b>102</b> is shown including a chair <b>110</b> and display <b>115</b>, as previously discussed. In addition, user console <b>102</b> is shown having an optional display <b>202</b> mounted to chair <b>110</b> by a display arm <b>204</b>. For example, display <b>115</b> may be an open display, and display <b>202</b> may be an immersive display. The term \\\"open\\\" display is intended to refer to a display which is designed to allow the user to see outside of the display, for example with their peripheral vision, even when directly facing the display. In addition, in an open display, the user can be a distance from the display screen or not directly in front of the display, and still view a surgical procedure on the display. Therefore, in an open display such as display <b>115</b>, the fact that the user may not be close to the display or have their face directly in front of the display, would not necessarily mean the user is not looking at the display or focused on the surgical procedure. In the case of an open display, in which the user can turn their head and still see the display using their peripheral vision, it is therefore important that turning of the head slightly not be interpreted to mean the user is not focused on the display. Rather, in the case of an open display, there will be some tolerance to such actions and allow for teleoperation mode to continue. This is in contrast to an immersive display such as display <b>202</b>. For example, display <b>202</b> could be a completely immersive display (as in the case of a periscope) that prevents the user from seeing outside of the display screen when they are facing the display and requires the user to be relatively close to the display screen. For example, in the case of an immersive display, the user must have their face relatively close to, and facing, the display screen to use it to view the surgical procedure in progress. If the user pulls their head away from the display screen, or doesn't face the display screen, they can no longer see the display screen, therefore this would typically be interpreted to mean the user is not focused on the screen or not paying sufficient attention to continue in teleoperation mode.</p><p>[0033] To detect whether all the interlocks required for engagement of the teleoperation mode (or continuing in the teleoperation mode) are met, user console <b>102</b> may further include a number of interlock detecting devices, mechanisms, systems and/or components. The interlock detecting mechanisms may include, but are not limited to, an eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, an immersive display deployment sensor <b>214</b>, a UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, UID contact sensor <b>220</b>, and/or a transport sensor <b>222</b>. Each of the interlock detecting mechanisms are configured to detect at least one of a number of conditions (referred to herein as \\\"interlocks\\\") which are required to engage the teleoperation mode. As previously discussed, the interlocks may correspond to user interactions with the user console that indicate the user is looking at, or otherwise focused on, the display, holding the controllers (e.g., UID) in a usable manner and/or the surgical environment is set up in a usable manner. Table 1 provides an exemplary listing of the specific interlocks which are required to engage a teleoperation mode, and to continue teleoperation mode, once engaged.</p><p><Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 1</TD></TR><TR></TR><TR><TD>Interlock</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze on the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated toward</TD></TR><TR><TD>the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated</TD></TR><TR><TD>toward the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is resting on the headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a</TD></TR><TR><TD>use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is inside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace not distorted and</TD></TR><TR><TD>tracker signal is accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is being held by the user</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is not in a transportation mode</TD></TR><TR></TR></Table></p><p>[0034] If any of interlocks 1-11 are not detected, a teleoperation mode cannot be engaged, or if already engaged, may be automatically disengaged. Said another way, if any one or more of the following conditions listed in Table 2 are determined (e.g., when any of interlocks 1-11 are not detected) teleoperation mode is either not engaged or disengaged.</p><p><Table frame=\\\"none\\\" colsep=\\\"0\\\" rowsep=\\\"0\\\"><TR><TD>TABLE 2</TD></TR><TR></TR><TR><TD>Condition</TD><TD>Description</TD></TR><TR></TR><TR></TR><TR><TD>1</TD><TD>Eye tracker indicates user gaze is outside of the display</TD></TR><TR><TD>2</TD><TD>Tracking marker on glasses indicates user head rotated away</TD></TR><TR><TD>from the display</TD></TR><TR><TD>3</TD><TD>Tracking marker on a face mask indicates user head is rotated</TD></TR><TR><TD>away from the display</TD></TR><TR><TD>4</TD><TD>Chair sensor indicates chair is facing away from the display</TD></TR><TR><TD>5</TD><TD>Chair sensor indicates user is not seated in the chair</TD></TR><TR><TD>6</TD><TD>Chair sensor indicates user's head is not resting on the</TD></TR><TR><TD>headrest</TD></TR><TR><TD>7</TD><TD>Immersive display sensor indicates immersive display is in a</TD></TR><TR><TD>non-use position</TD></TR><TR><TD>8</TD><TD>Tracker indicates UID is outside the trackable workspace</TD></TR><TR><TD>9</TD><TD>Tracker indicates UID trackable workspace distorted and</TD></TR><TR><TD>tracker signal is not accurate</TD></TR><TR><TD>10</TD><TD>Tracker indicates a UID is not being held by the user, or is in</TD></TR><TR><TD>a stored configuration</TD></TR><TR><TD>11</TD><TD>Tracker indicates the system is in a transportation mode</TD></TR><TR></TR></Table></p><p>[0035] The interlock detecting mechanisms for detecting, for example, interlocks 1-7 listed in Table 1, which generally relate to whether the user is looking at or focused on the display, may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, chair swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, headrest pressure sensor <b>212</b>, and immersive display deployment sensor <b>214</b>. The interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether the user is holding the controllers in a usable manner, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b> and UID contact sensor <b>220</b>. The interlock detecting mechanisms for detecting interlock 11, which relates to whether the surgical environment is set up in a usable manner, may include transport sensor <b>222</b>, in addition to one or more of the UID sensors <b>216</b>, <b>218</b>, <b>220</b>. Once a condition or interlock is detected by the interlock detecting mechanism, the information is output to a processing component of the surgical robotic system, where it is received as an interlock input and monitored to determine whether teleoperation should continue, should be engaged or should be disengaged. For example, if interlock inputs corresponding to all the required interlocks for teleoperation mode are received, teleoperation mode is engaged. If any interlocks are missing, teleoperation mode is not engaged (if currently in a non-teleoperation mode), or disengaged (if currently in teleoperation mode).</p><p>[0036] Referring now in more detail to each of the interlock detecting mechanisms, eye and/or head tracker <b>206</b> may be a tracking device that is coupled to the display <b>115</b> to track a user's gaze and/or head with respect to the display <b>115</b>, and indicate whether or not the user is looking toward or away from the display. Eye and/or head tracker <b>206</b> may be attached to any portion of display <b>115</b> suitable for tracking a gaze of a user. For example, eye and/or head tracker <b>206</b> may be attached to a housing <b>224</b> of display <b>115</b>, for example, a top wall, a bottom wall, or a side wall of housing <b>224</b>, or integrated within a screen (not shown) mounted within the housing <b>224</b> of display <b>115</b>. Eye and/or head tracker <b>206</b> may include one or more projector(s) and camera(s) which face the user and can be used to track a gaze of a user. Representatively, the projector(s) may create a pattern of near-infrared light on the eyes of the user, and the camera(s) may take images of the user's eyes and the pattern. Eye and/or head tracker <b>206</b> may further be programmed to use this information (e.g., execute machine learning, image processing and/or mathematical algorithms) to determine where the user is looking based on a position of each of the user's eyes and/or gaze point or location relative to one another, and display <b>115</b>. For example, when the user is sitting on chair <b>110</b> in front of display <b>115</b>, eye and/or head tracker <b>206</b> may be configured to create a pattern of near-infrared light on the eyes of the user and take images of the user's eyes and the pattern. This information may, in turn, be input to the surgical robotic system processing component (e.g., an interlock input signal) to determine whether the gaze of the user is inside or outside of display <b>115</b>. If the gaze is determined to be inside the display, the user is determined to be looking toward the display. On the other hand, if the gaze is determined to be outside the display, the user is determined to be looking away from the display.</p><p>[0037] In addition, eye and/or head tracker <b>206</b> can also track a position, orientation and/or location of the user's head as an indication of whether the user is looking at the display. For example, tracker <b>206</b> may detect that the user's head is in front of the display and/or the user's face or nose is facing display <b>115</b>, which suggests the user is looking at the display, or that the user's head is not in front of the display and/or their face or nose is turned away from display <b>115</b>, which suggests the user is looking away from the display. In addition, the tracker <b>206</b> may be used to determine automatically if the open display <b>115</b> or the immersive display <b>202</b> should be used for teleoperation.</p><p>[0038] Still further, tracking markers <b>226</b> on the user's glasses and/or tracking markers <b>228</b> on the user's face mask may be used to determine whether the user is looking toward or away from the display <b>115</b>. The tracking markers <b>226</b> on the user's glasses (e.g., 3D glasses) and/or markers <b>228</b> on the face mask (if user is sterile) may be active or passive markers. The markers <b>226</b> and/or <b>228</b> may track if the user's head is rotated toward the screen, indicating the user is looking toward the display. The markers <b>226</b> and/or <b>228</b> may further track whether the head is rotated away from the screen, indicating the user is looking away from the display. In some cases where the markers <b>226</b> are attached to the user's glasses, they may be similar to the previously discussed eye and/or head tracker and include one or more projectors, cameras and/or processing algorithms that can analyze information received from the camera(s) and projector(s) to determine whether the user is looking at the display.</p><p>[0039] Additional interlock mechanisms for detecting whether or not the user is looking toward or away from the display may include a swivel sensor <b>208</b>, chair pressure sensor <b>210</b>, and a headrest pressure sensor <b>212</b>. The swivel sensor <b>208</b> may be any type of sensing device or component suitable for detecting a rotation (or swivel) and/or orientation of the chair <b>110</b>, with respect to, for example, a stationary base <b>230</b>. For example, the swivel sensor <b>208</b> may include a position encoder, switch or the like. The swivel sensor <b>208</b> may then output a corresponding signal to a processing component of the surgical robotic system corresponding to the rotation, position and/or orientation of the chair <b>110</b>. The surgical robotic system can then use this information to determine whether the chair <b>110</b> is facing display <b>115</b>, and therefore a user seated in the chair is facing the display <b>115</b>. If, on the other hand, swivel sensor <b>208</b> detects the chair is facing away from the display, it is determined the user is looking away from the display. The chair pressure sensor <b>210</b> and the headrest pressure sensor <b>212</b> may be pressure sensors which are attached to the chair headrest and chair seat, respectively. The chair pressure sensor <b>210</b> can be used to detect when the user is seated in the chair <b>110</b>, and the headrest sensor <b>212</b> can be used to detect when the user's head is resting on the headrest. Sensors <b>210</b>, <b>212</b> may output a signal indicating that the user is seated in chair <b>110</b> and/or the head is resting on the headrest to the system processing component, which can be used as indicators that the user is looking at the display <b>115</b>. If the sensors <b>210</b>, <b>212</b> do not detect the user seated in the chair <b>110</b>, this is interpreted as an indication the user is not looking toward the display, or looking away from the display. Sensors <b>210</b> and <b>212</b> may be any type of sensor suitable for detecting the user applying pressure to the seat in the particular location in which the sensor is positioned, for example, they may be pressure plates that are mounted within portions of the headrest and seat which would be contacted by a user seated in the chair <b>110</b>. It should be understood, however, if chair <b>110</b> of console <b>102</b> does not swivel, swivel sensor <b>208</b> may be omitted, and interlock 4 of Table 1 (e.g., chair sensor indicates chair is facing the display) may not be required to engage teleoperation mode.</p><p>[0040] In addition, in embodiments where user console <b>102</b> includes immersive display <b>202</b>, an immersive display deployment sensor <b>214</b> to detect a location of the immersive display <b>202</b> may be provided. The immersive display deployment sensor <b>214</b> may be coupled to the immersive display <b>202</b> and include switches, encoders or the like that are operable to determine if display <b>202</b> is in a \\\"use\\\" position (e.g., position suitable for viewing by a user seated in the chair), \\\"non-use\\\" position (e.g., positioned not suitable for viewing by a user seated in the chair) and/or deployed position (e.g., positioned in front of the chair). This information can be used in conjunction with a sensor (e.g., pressure or light gate sensor) to determine if the user's head is in the immersive display <b>202</b>. A teleoperation mode will only be engaged if the immersive display <b>202</b> is in the \\\"use\\\" configuration and the sensor detects that the user's head is in the immersive display <b>202</b>. If display is determined to be in \\\"non-use\\\" position and/or the user's head is not detected in display <b>202</b>, this is interpreted as an indication the user is looking away from the display. It should be understood, however, if immersive display <b>202</b> is omitted, deployment sensor <b>214</b> may be omitted, and interlock 7 of Table 1 (e.g., immersive display sensor indicates immersive display is in a use position) is not required to engage teleoperation mode.</p><p>[0041] In addition, the interlock detecting mechanisms for detecting, for example, interlocks 8-10 listed in Table 1, which generally relate to whether a user is holding the controllers (e.g., UID) in a usable or non-usable manner or configuration, may include UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be, for example, an optical tracker, a position tracker and/or an encoder, that is coupled to the user console and/or the UID. The sensor <b>216</b>, <b>218</b> and/or <b>220</b> may be used to track, for example, the position of the UID, whether the user is holding the UID in a usable or non-usable manner, and/or whether the UID is inside or outside of the trackable workspace. If the UID is outside of the workspace, the UID is not being held in a usable manner, and the system should be disengaged. In addition, if the tracking space is distorted and the signal received from the tracker does not give an accurate reading, the UID is considered not to be in a usable manner or in a non-usable configuration, and teleoperation should be disengaged. Still further, the sensor <b>216</b>, <b>218</b> and/or <b>220</b> can be used to track whether the UID is stored within the user console (e.g., within a holster or other holding mechanism) and/or the UID has been dropped. If the UID is stored within the console and/or has been dropped, such that the user is no longer holding the UID, the UID is considered to be in a non-usable configuration, and teleoperation should be disengaged. In some embodiments, one or more of sensors <b>216</b>, <b>218</b>, <b>220</b> may be an optical tracker and/or an absolute encoders. In some embodiments, a position sensor (optical tracker) is mounted on the user console (e.g., near display <b>115</b>) and/or the base of the UID. In some embodiments, the UID is tracked by a sensor coupled to the console or the user. The position sensor can provide the position of the UID relative to a user and/or a ground reference point (not shown). In some embodiments, more than one (e.g., a plurality, several) tracking technologies are used in conjunction.</p><p>[0042] In addition, the interlock detecting mechanisms for detecting, for example, interlock 11 listed in Table 1, which generally relates to whether the surgical environment is set up in a usable manner or non-usable manner, may include transport sensor <b>222</b>. Transport sensor <b>222</b> may be a sensor which detects when the surgical robotic system is transitioned to a transportation configuration or mode (e.g., console <b>102</b> can be moved), by sensing, for example, that the brakes are released. If a transition to a transportation configuration is detected, the transportation interlock is no longer met, and the surgical environment is considered to be a non-usable configuration and teleoperation mode should be disengaged. If, on the other hand, the transport sensor <b>222</b> determines the system is not in a transportation configuration (e.g., the brakes are engaged), the surgical environment is in a usable configuration, and teleoperation can be engaged.</p><p>[0043] Additional interlock detecting mechanisms may include the use of the eye tracking aspects previously discussed for determining interpupillary distance (IPD) for the immersive display <b>202</b>. For example, given the position of the center of the eyeballs from the eye tracker, the IPD can be calculated for the immersive display. This may remove a requirement for the user to manually adjust the immersive display IPD.</p><p>[0044] FIG. 3 is a block diagram of an exemplary operation for disengaging or engaging a teleoperation mode, in accordance with an embodiment. Representatively, process <b>300</b> includes the operations of receiving a plurality of interlock inputs (block <b>302</b>). The plurality of interlock inputs may be one or more of the interlocks 1-11 received from the interlock detecting mechanisms previously discussed in reference to FIG. 2 and Table 1. Based on the plurality of interlock inputs received, the system then determines whether a user is looking toward or away from the display (block <b>304</b>). As previously discussed, the interlocks indicating that the user is looking toward the display may be interlocks 1-7 listed in Table 1. If any one of interlocks 1-7 are not met or satisfied, in other words at least one of conditions 1-7 listed in Table 2 is detected, it is determined that the user is not looking toward the display (e.g., looking away) and teleoperation mode is not engaged (block <b>306</b>). On the other hand, if all of interlocks 1-7 listed in Table 1 are met or satisfied, it is determined at operation <b>304</b> that the user is looking toward the display and process <b>300</b> continues to operation <b>308</b>.</p><p>[0045] At operation <b>308</b>, the system determines whether a user interface device is positioned in a usable or non-usable manner. A user interface device is considered positioned in a usable manner when it is in a position suitable for a user to control a surgical instrument in a teleoperation mode. The user interface device may be considered to be in a non-usable manner when it is not in a position suitable for the user to control the surgical instrument. Representatively, the user interface device is considered positioned in a usable manner if all of the interlocks 8-10 listed in Table 1 are met or satisfied, and the process <b>300</b> continues to operation <b>310</b>. For example, if UID <b>114</b> is inside a trackable workspace, the tracking workspace is not distorted and the UID <b>114</b> is being held by the user, the UID <b>114</b> is positioned in a usable manner. On the other hand, if it is determined at operation <b>308</b> that one of conditions 8-10 listed in Table 2 are detected, namely the UID is outside of the trackable workspace and the information received from the tracker is not accurate, the tracking space is distorted and the signal received from the tracker does not give an accurate reading, or the UID is not being held by a user (e.g., the UID is stored or dropped), the system determines the user interface device is not positioned in a usable manner, or is positioned in a non-usable manner, and does not engage teleoperation mode.</p><p>[0046] In operation <b>310</b>, the system determines whether the surgical workspace is configured in a usable manner or a non-usable manner. The surgical workspace (e.g., the user console, robotic arms, surgical table, etc) may be considered to be positioned or configured in a usable manner when components of the surgical workspace are properly configured for a user to control a surgical instrument in a teleoperation mode. For example, the surgical workspace is considered to be configured in a usable manner if the interlock 11 listed in Table 1 is met or satisfied. For example, if the user console, chair, display or other associated component is in a fixed configuration (e.g., not in a transportation configuration in which the brakes are released), the surgical workspace is considered to be properly configured. If the surgical workspace is configured in a usable manner, the process continues to operation <b>312</b> and teleoperation mode is engaged. If, on the other hand, it is determined at operation <b>310</b> that the system is transitioned to a transportation configuration (e.g., the brakes are released), in other words condition 11 of Table 2 is detected, the surgical workspace is configured in a non-usable manner for teleoperation mode, and the process proceeds to operation <b>306</b> and teleoperation mode is not engaged.</p><p>[0047] FIG. 4 is a block diagram of an exemplary operation for disengaging a teleoperation mode, in accordance with an embodiment. Representatively, similarly to process <b>300</b>, process <b>400</b> includes the operation of receiving a plurality of interlock inputs (block <b>402</b>), for example, interlocks 1-11 previously discussed in reference to Table 1. When all of the interlock inputs required for teleoperation mode are received (e.g., the user is looking toward or otherwise focused on the display), a teleoperation mode is engaged at operation <b>404</b>. Process <b>400</b> then continues to monitor whether all of the interlock inputs are still present at operation <b>406</b>. As long as the interlocks continue to be detected, teleoperation mode will continue (block <b>410</b>). If, on the other hand, it is determined at operation <b>406</b> that one or more of the interlock inputs is no longer detected, in other words one or more of conditions 1-11 listed in Table 2 are present, the system transitions out of the teleoperation mode at operation <b>408</b>, for example to a non-teleoperation mode in which the user interface device is prevented from controlling a surgical instrument.</p><p>[0048] FIG. 5 is a block diagram of a computer portion of a surgical robotic system, which is operable to implement the previously discussed operations, in accordance with an embodiment. The exemplary surgical robotic system <b>500</b> may include a user console <b>102</b>, a surgical robot <b>120</b>, and a control tower <b>103</b>. The surgical robotic system <b>500</b> may include other or additional hardware components; thus, the diagram is provided by way of example and not a limitation to the system architecture.</p><p>[0049] As described above, the user console <b>102</b> may include console computers <b>511</b>, one or more UIDs <b>512</b>, console actuators <b>513</b>, displays <b>514</b>, foot pedals <b>516</b>, console computers <b>511</b> and a network interface <b>518</b>. In addition, user console <b>102</b> may include a number of interlock detecting mechanisms, devices, or components, for example, a UID tracker(s) <b>515</b>, a display tracker(s) <b>517</b> and a console tracker(s) <b>519</b>, for detecting whether the previously discussed interlocks required for teleoperation are satisfied. For example, UID tracker(s) <b>515</b> may include the previously discussed UID tracking sensor <b>216</b>, UID storage sensor <b>218</b>, and/or UID contact sensor <b>220</b>. Display tracker(s) <b>517</b> may include eye and/or head tracker <b>206</b>, tracking markers <b>226</b> on the user's glasses, tracking markers <b>228</b> on the user's face mask, a chair swivel sensor <b>208</b>, a chair pressure sensor <b>210</b>, a headrest pressure sensor <b>212</b>, and/or an immersive display deployment sensor <b>214</b>. Console tracker(s) may include transport sensor <b>222</b>. The interlock information detected and/or tracked by any one or more of the trackers may be monitored and communicated to the console computer <b>511</b>, and dispatched to the control tower <b>103</b> via the network interface <b>518</b>, so that the system can determine whether to engage and/or disengage a teleoperation mode.</p><p>[0050] It should further be understood that a user or surgeon sitting at the user console <b>102</b> can adjust ergonomic settings of the user console <b>102</b> manually, or the settings can be automatically adjusted according to user profile or preference. The manual and automatic adjustments may be achieved through driving the console actuators <b>513</b> based on user input or stored configurations by the console computers <b>511</b>. The user may perform robot-assisted surgeries by controlling the surgical robot <b>120</b> using one or more master UIDs <b>512</b> and foot pedals <b>516</b>. Positions and orientations of the UIDs <b>512</b> are continuously tracked by the UID tracker <b>515</b>, and status changes are recorded by the console computers <b>511</b> as user input and dispatched to the control tower <b>103</b> via the network interface <b>518</b>. Real-time surgical video of patient anatomy, instrumentation, and relevant software apps can be presented to the user on the high resolution 3D displays <b>514</b> including open or immersive displays.</p><p>[0051] The user console <b>102</b> may be communicatively coupled to the control tower <b>103</b>. The user console also provides additional features for improved ergonomics. For example, the user console may be an open architecture system including an open display, although an immersive display, in some cases, may be provided. Furthermore, a highly-adjustable seat for surgeons and master UIDs tracked through electromagnetic or optical trackers are included at the user console <b>102</b> for improved ergonomics.</p><p>[0052] The control tower <b>103</b> can be a mobile point-of-care cart housing touchscreen displays, computers that control the surgeon's robotically-assisted manipulation of instruments, safety systems, graphical user interface (GUI), light source, and video and graphics computers. As shown in FIG. 5, the control tower <b>103</b> may include central computers <b>531</b> including at least a visualization computer, a control computer, and an auxiliary computer, various displays <b>533</b> including a team display and a nurse display, and a network interface <b>518</b> coupling the control tower <b>103</b> to both the user console <b>102</b> and the surgical robot <b>120</b>. The control tower <b>103</b> may offer additional features for user convenience, such as the nurse display touchscreen, soft power and E-hold buttons, user-facing USB for video and still images, and electronic caster control interface. The auxiliary computer may also run a real-time Linux, providing logging/monitoring and interacting with cloud-based web services.</p><p>[0053] The surgical robot <b>120</b> may include an articulated operating table <b>524</b> with a plurality of integrated arms <b>522</b> that can be positioned over the target patient anatomy. A suite of compatible tools <b>523</b> can be attached to or detached from the distal ends of the arms <b>522</b>, enabling the surgeon to perform various surgical procedures. The surgical robot <b>120</b> may also comprise control interface <b>525</b> for manual control of the arms <b>522</b>, table <b>524</b>, and tools <b>523</b>. The control interface can include items such as, but not limited to, remote controls, buttons, panels, and touchscreens. Other accessories such as trocars (sleeves, seal cartridge, and obturators) and drapes may also be needed to perform procedures with the system. In some variations, the plurality of the arms <b>522</b> includes four arms mounted on both sides of the operating table <b>524</b>, with two arms on each side. For certain surgical procedures, an arm mounted on one side of the table can be positioned on the other side of the table by stretching out and crossing over under the table and arms mounted on the other side, resulting in a total of three arms positioned on the same side of the table <b>524</b>. The surgical tool can also comprise table computers <b>521</b> and a network interface <b>518</b>, which can place the surgical robot <b>120</b> in communication with the control tower <b>103</b>.</p><p>[0054] In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"Claims\": {\n",
      "\n",
      "\"Texts\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"lang\": \"en\",\n",
      "\n",
      "\"text\": \"<Claim claimid=\\\"1\\\" independent=\\\"yes\\\" first=\\\"first\\\"><p><b>1</b>. A method for engaging and disengaging a surgical instrument of a surgical robotic system, the method comprising: <BR />receiving a plurality of interlock inputs from one or more interlock detection components of the surgical robotic system, and wherein at least one of the one or more interlock detection components comprises a transport sensor for detecting whether the surgical robotic system is in a transportation mode as one of the plurality of interlock inputs;<BR />determining, by one or more processors communicatively coupled to the interlock detection components, whether the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interface devices of the surgical robotic system are configured in a usable manner, and (3) the transport sensor detects the surgical robotic system is not in the transportation mode; and<BR />in response to determining each of the interlock requirements are satisfied, transition the surgical robotic system into a teleoperation mode in which the surgical instrument is controlled by at least one or more of the user interface devices.</p></Claim><Claim claimid=\\\"2\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>2</b>. The method of claim 1 wherein the transport sensor is coupled to a user console of the surgical robotic system and the transport sensor detects the surgical robotic system is not in the transportation mode when a brake associated with the user console is engaged.</p></Claim><Claim claimid=\\\"3\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>3</b>. The method of claim 1 wherein in response to determining less than all of the interlock requirements are satisfied, transition the surgical robotic system out of a teleoperation mode such that the at least one or more user interface devices is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"4\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>4</b>. The method of claim 1, wherein the one or more user interface devices comprise at least one of a handheld user input device and a foot pedal.</p></Claim><Claim claimid=\\\"5\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>5</b>. The method of claim 1 wherein the display is an open display.</p></Claim><Claim claimid=\\\"6\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>6</b>. The method of claim 1 wherein the plurality of interlock inputs indicating a user is looking toward the display comprise: <BR />detecting a user gaze is toward a display of the console;<BR />detecting a user head is rotated toward the display;<BR />detecting a chair of the console is facing the display;<BR />detecting a user is seated on the chair; and<BR />detecting the display is in a use position.</p></Claim><Claim claimid=\\\"7\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>7</b>. The method of claim 1 wherein the plurality of interlock inputs indicating at least one or more user interface devices of the surgical robotic system is configured in a usable manner comprise: <BR />detecting at least one user interface device is inside the surgical workspace;<BR />detecting an accurate user interface device location is received; and<BR />detecting at least one user interface device is being held by the user.</p></Claim><Claim claimid=\\\"8\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>8</b>. The method of claim 1 wherein the one or more interlock detection components further comprise an eye tracker, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the eye tracker detecting a user gaze is toward the display.</p></Claim><Claim claimid=\\\"9\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>9</b>. The method of claim 1 wherein the one or more interlock detection components further comprise a tracking marker on a user&apos;s glasses or a tracking marker on a user&apos;s face mask, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the tracking marker indicating a user&apos;s head is rotated toward the display.</p></Claim><Claim claimid=\\\"10\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>10</b>. The method of claim 1 wherein the one or more interlock detection components further comprise a chair sensor, and wherein the plurality of interlock inputs indicating a user is looking toward the display comprise the chair sensor indicating a chair is facing the display, a user is seated in the chair or a user&apos;s head is contacting a headrest of the chair.</p></Claim><Claim claimid=\\\"11\\\" independent=\\\"no\\\" parents=\\\"1\\\"><p><b>11</b>. The method of claim 1 wherein the one or more interlock detection components further comprise a user interface device sensor, and at least one of the interlock requirements comprises the user interface device sensor indicating the user interface device is inside the surgical workspace, the user interface device is being held by a user or the user interface device is not in a stored position.</p></Claim><Claim claimid=\\\"12\\\" independent=\\\"yes\\\"><p><b>12</b>. A surgical robotic system, comprising: <BR />a surgical instrument;<BR />a user console comprising a display, an interlock detecting component, and a user interface device; and<BR />one or more processors communicatively coupled to the interlock detecting component, the processors configured to:<BR />receive a plurality of interlock inputs from the interlock detecting component and transition the surgical robotic system to a teleoperation mode in which the surgical instrument is controlled by the user interface device; and<BR />in response to determining at least one of the plurality of interlock inputs received from the interlock detecting component is no longer present, transition the surgical robotic system out of a teleoperation mode such that the user interface device is prevented from controlling the surgical instrument.</p></Claim><Claim claimid=\\\"13\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>13</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises an eye tracker indicating a user gaze on the display.</p></Claim><Claim claimid=\\\"14\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>14</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a tracking marker on a user&apos;s glasses or face mask indicating the user&apos;s head is rotated toward the display.</p></Claim><Claim claimid=\\\"15\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>15</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a chair sensor indicating a chair is facing the display, a user is seated in the chair or a user&apos;s head is resting on a headrest of the chair.</p></Claim><Claim claimid=\\\"16\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>16</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a display sensor indicating the display is in a use position.</p></Claim><Claim claimid=\\\"17\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>17</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a tracker indicating the user interface device is inside a surgical workspace or the user interface device is being held by a user.</p></Claim><Claim claimid=\\\"18\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>18</b>. The system of claim 12 wherein the at least one of the plurality of interlock inputs no longer present comprises a transport sensor indicating engagement of a brake associated with the user console.</p></Claim><Claim claimid=\\\"19\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>19</b>. The system of claim 12 wherein the plurality of interlock inputs indicate each of the following interlock requirements are satisfied: (1) a user is looking toward a display of the surgical robotic system, (2) at least one or more user interfaces device of the surgical robotic system is configured in a usable manner, and (3) a surgical workspace of the surgical robotic system is configured in a usable manner.</p></Claim><Claim claimid=\\\"20\\\" independent=\\\"no\\\" parents=\\\"12\\\"><p><b>20</b>. The system of claim 12 wherein the one or more user interface devices comprise at least one of a handheld user input device and a foot pedal, and the display is an open display.</p></Claim>\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "\"IPCs\": [\n",
      "\n",
      "\"A61B34/00\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"B25J9/00\",\n",
      "\n",
      "\"B25J9/16\",\n",
      "\n",
      "\"G06F3/01\"\n",
      "\n",
      "],\n",
      "\n",
      "\"CPCs\": [\n",
      "\n",
      "\"A61B2017/00212\",\n",
      "\n",
      "\"A61B2017/00216\",\n",
      "\n",
      "\"A61B2034/305\",\n",
      "\n",
      "\"A61B34/25\",\n",
      "\n",
      "\"A61B34/30\",\n",
      "\n",
      "\"A61B34/35\",\n",
      "\n",
      "\"A61B34/37\",\n",
      "\n",
      "\"A61B34/74\",\n",
      "\n",
      "\"A61B90/03\",\n",
      "\n",
      "\"B25J9/0009\",\n",
      "\n",
      "\"B25J9/1689\",\n",
      "\n",
      "\"G06F3/012\",\n",
      "\n",
      "\"G06F3/013\",\n",
      "\n",
      "\"G06F3/017\"\n",
      "\n",
      "],\n",
      "\n",
      "\"ProbablePatentAssignee\": \"VERB SURGICAL INC\",\n",
      "\n",
      "\"EarliestPriorityDate\": 20190517,\n",
      "\n",
      "\"LegalStatus\": {\n",
      "\n",
      "\"Events\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US2022323168\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220420,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"FEPP\",\n",
      "\n",
      "\"eventgroup\": \"LSFE\",\n",
      "\n",
      "\"event\": \"FEE PAYMENT PROCEDURE\",\n",
      "\n",
      "\"details\": \"Text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US2022323168\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220628,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"RFE1\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"Docketed New Case - Ready for Examination\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US2022323168\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20220628,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"STPP\",\n",
      "\n",
      "\"eventgroup\": \"\",\n",
      "\n",
      "\"event\": \"INFORMATION ON STATUS: PATENT APPLICATION AND GRANTING PROCEDURE IN GENERAL\",\n",
      "\n",
      "\"details\": \"Text: DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"attributes\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"Value\": \"DOCKETED NEW CASE - READY FOR EXAMINATION\",\n",
      "\n",
      "\"Name\": \"Text\"\n",
      "\n",
      "}\n",
      "\n",
      "]\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US2022323168\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20221013,\n",
      "\n",
      "\"eventbenefit\": \"+\",\n",
      "\n",
      "\"eventcode\": \"\",\n",
      "\n",
      "\"eventgroup\": \"LSPB\",\n",
      "\n",
      "\"event\": \"published / reissued\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n",
      "{\n",
      "\n",
      "\"an\": \"US20220725351\",\n",
      "\n",
      "\"ad\": 20220420,\n",
      "\n",
      "\"pn\": \"US2022323168\",\n",
      "\n",
      "\"kd\": \"AA\",\n",
      "\n",
      "\"daterecorded\": 20230317,\n",
      "\n",
      "\"eventbenefit\": \"\",\n",
      "\n",
      "\"eventcode\": \"NFA1\",\n",
      "\n",
      "\"eventgroup\": \"LSES\",\n",
      "\n",
      "\"event\": \"Non Final Action Mailed\",\n",
      "\n",
      "\"details\": \"\",\n",
      "\n",
      "\"attributes\": []\n",
      "\n",
      "},\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_id, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        print(line)  \n",
    "        if i == 3000: \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf85e324-e027-4bc0-81d1-16f2a0fd5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: patent_chunk_0.csv\n",
      "Saved: patent_chunk_1.csv\n",
      "Saved: patent_chunk_2.csv\n",
      "Saved: patent_chunk_3.csv\n",
      "Saved: patent_chunk_4.csv\n",
      "Saved: patent_chunk_5.csv\n",
      "Saved: patent_chunk_6.csv\n",
      "Saved: patent_chunk_7.csv\n",
      "Saved: patent_chunk_8.csv\n",
      "Saved: patent_chunk_9.csv\n",
      "Saved: patent_chunk_10.csv\n",
      "Saved: patent_chunk_11.csv\n",
      "Saved: patent_chunk_12.csv\n",
      "Saved: patent_chunk_13.csv\n",
      "Saved: patent_chunk_14.csv\n",
      "Saved: patent_chunk_15.csv\n"
     ]
    }
   ],
   "source": [
    "# Open JSON file in streaming mode\n",
    "with open(file_id, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract records\n",
    "patents = data.get(\"Records\", [])\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 5000  # Adjust this based on your needs\n",
    "\n",
    "for i in range(0, len(patents), chunk_size):\n",
    "    chunk = patents[i : i + chunk_size]  # Get a small subset\n",
    "    df_chunk = pd.DataFrame(chunk)  # Convert to DataFrame\n",
    "    \n",
    "    # Save each chunk separately (optional)\n",
    "    chunk_file = f\"patent_chunk_{i//chunk_size}.csv\"\n",
    "    df_chunk.to_csv(chunk_file, index=False)\n",
    "    \n",
    "    print(f\"Saved: {chunk_file}\")  # Track progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c3f00-3130-4928-9ea6-1fb5481ce832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "with open(file_id, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract patent records\n",
    "patents = data[\"Records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a3e8e-fe5f-43e5-9caf-1df1eadd889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "for patent in patents:\n",
    "    record = {\n",
    "        \"CountryCode\": patent.get(\"CountryCode\", None),\n",
    "        \"ApplicationNumber\": patent.get(\"ApplicationNumber\", None),\n",
    "        \"Title\": (patent.get(\"Titles\", {}).get(\"Texts\", [{}])[0].get(\"text\", None)),\n",
    "        \"Abstract\": (patent.get(\"Abstracts\", {}).get(\"Texts\", [{}])[0].get(\"text\", None)),\n",
    "        \"Description\": (patent.get(\"Descriptions\", {}).get(\"Texts\", [{}])[0].get(\"text\", None)),\n",
    "        \"Claim\": (patent.get(\"Claims\", {}).get(\"Texts\", [{}])[0].get(\"text\", None)),\n",
    "        \"IPCs\": \", \".join(patent.get(\"IPCs\", [])),  # Handle missing IPCs\n",
    "        \"CPCs\": \", \".join(patent.get(\"CPCs\", [])),  # Handle missing CPCs\n",
    "        \"Assignee\": patent.get(\"ProbablePatentAssignee\", None),\n",
    "        \"EarliestPriorityDate\": patent.get(\"EarliestPriorityDate\", None),\n",
    "    }\n",
    "    flattened_data.append(record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd14fd-a97e-468c-ae58-912c585135a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Display first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1c50d-19ea-4e7f-a230-1fbdbf180904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for further analysis\n",
    "df.to_csv(\"patents_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
